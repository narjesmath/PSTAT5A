---
title: "Hypothesis Testing: Making Decisions with Data"
subtitle: "From Questions to Statistical Evidence"
author: "Narjes Mathlouthi"
format: 
  revealjs:
    logo: /img/logo.png
    theme: default
    css: /files/lecture_notes/lecture9/new-style.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Understanding Data ‚Äì Hypothesis Testing Lecture ¬© 2025"
    incremental: true
    transition: slide
    background-transition: fade
    smaller: true
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import scipy.stats as stats
from scipy.stats import norm, t
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Custom color palette
colors = {
    'primary': '#3b82f6',
    'secondary': '#f59e0b', 
    'success': '#10b981',
    'danger': '#ef4444',
    'info': '#8b5cf6',
    'light': '#f8fafc',
    'dark': '#1f2937'
}
```

## üì¢ Announcements

### üìù Quiz 2
**When:**  
- üìÖ **Date:** Friday, July 25  
- ‚è∞ **Window:** Available 7 AM ‚Äì 12 AM  
- ‚è≥ **Duration:** 1 hour (once you begin)

**Where:**  
- üíª Online on Canvas

**Covers:**  
- üìö Material from Weeks 3-4

**Note:**  
- üì∏ Upload a photo of your written work for some questions


## Learning Objectives üéØ

By the end of this lecture, you will be able to:

- **Formulate null and alternative hypotheses** from research questions
- **Understand the logic** of hypothesis testing
- **Calculate and interpret p-values** correctly
- **Make decisions** using significance levels
- **Recognize Type I and Type II errors** and their consequences
- **Perform common hypothesis tests** in Python

## What is Hypothesis Testing?

::: {.incremental}
- **Hypothesis testing** is a statistical method for making decisions about population parameters based on sample data
- It helps us answer questions like:
  - "Is this new drug more effective than the current treatment?"
  - "Has customer satisfaction improved after our changes?"
  - "Are students' test scores significantly different from the national average?"
:::

::: {.fragment}
**Key Idea:** We use sample data to make inferences about populations, acknowledging that our conclusions might be wrong due to random variation.
:::

## The Courtroom Analogy

::: columns
::: {.column width="50%"}
**Criminal Trial**

- **Presumption:** Innocent until proven guilty
  
- **Burden of proof:** Prosecution must prove guilt
  
- **Standard:** "Beyond reasonable doubt"
  
- **Verdict:** Guilty or Not Guilty
:::

::: {.column width="50%"}
**Hypothesis Testing**

- **Presumption:** Null hypothesis is true
  
- **Burden of proof:** Data must provide evidence against null
  
- **Standard:** Significance level ($\alpha = 0.05$)
  
- **Decision:** Reject or Fail to Reject H‚ÇÄ
:::
:::

::: {.fragment}
**Just like in court, we never "prove" innocence or "accept" the null hypothesis, we only determine if there's sufficient evidence to reject it.**
:::

## The Six Steps of Hypothesis Testing

## Step 1: State the Hypotheses

::: {.incremental}
- **Null Hypothesis ($H_0$):** The "status quo" or "no effect" statement
  - Usually includes "=", "‚â§", or "‚â•"
  - What we assume to be true until proven otherwise

- **Alternative Hypothesis ($H_1$ or $H_a$):** The research claim we want to test
  - Usually includes "‚â†", "<", or ">"
  - What we're trying to find evidence for
:::

::: {.fragment}
**Example:** Testing if a new teaching method improves test scores

- $H_0: \mu = 75$ (no improvement, scores stay the same)
  
- $H_1: \mu > 75$ (scores improve with new method)
:::

## Types of Alternative Hypotheses

```{python}
#| fig-align: center
#| fig-width: 20
#| fig-height: 5

import matplotlib.pyplot as plt
import numpy as np
from scipy import stats

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
x = np.linspace(-4, 4, 1000)
y = stats.norm.pdf(x, 0, 1)

# Two-tailed test
axes[0].plot(x, y, 'b-', linewidth=2, label='Standard Normal')
axes[0].fill_between(x[x <= -1.96], y[x <= -1.96], alpha=0.3, color='red', label='Rejection Region')
axes[0].fill_between(x[x >= 1.96], y[x >= 1.96], alpha=0.3, color='red')
axes[0].set_title('Two-tailed Test\nH‚ÇÅ: Œº ‚â† Œº‚ÇÄ', fontweight='bold')
axes[0].set_ylim(0, 0.45)
axes[0].axvline(0, color='black', linestyle='--', alpha=0.7)
axes[0].text(0, 0.35, 'H‚ÇÄ: Œº = Œº‚ÇÄ', ha='center', fontsize=10, 
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

# Left-tailed test
axes[1].plot(x, y, 'b-', linewidth=2)
axes[1].fill_between(x[x <= -1.645], y[x <= -1.645], alpha=0.3, color='red')
axes[1].set_title('Left-tailed Test\nH‚ÇÅ: Œº < Œº‚ÇÄ', fontweight='bold')
axes[1].set_ylim(0, 0.45)
axes[1].axvline(0, color='black', linestyle='--', alpha=0.7)
axes[1].text(0, 0.35, 'H‚ÇÄ: Œº ‚â• Œº‚ÇÄ', ha='center', fontsize=10,
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

# Right-tailed test
axes[2].plot(x, y, 'b-', linewidth=2)
axes[2].fill_between(x[x >= 1.645], y[x >= 1.645], alpha=0.3, color='red')
axes[2].set_title('Right-tailed Test\nH‚ÇÅ: Œº > Œº‚ÇÄ', fontweight='bold')
axes[2].set_ylim(0, 0.45)
axes[2].axvline(0, color='black', linestyle='--', alpha=0.7)
axes[2].text(0, 0.35, 'H‚ÇÄ: Œº ‚â§ Œº‚ÇÄ', ha='center', fontsize=10,
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

for ax in axes:
    ax.set_xlabel('Test Statistic')
    ax.set_ylabel('Probability Density')
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Step 2: Choose Significance Level (Œ±)

::: {.incremental}
- **Significance level (Œ±):** The probability of rejecting H‚ÇÄ when it's actually true
- **Common choices:** Œ± = 0.05, 0.01, or 0.10
- **Interpretation:** "We're willing to be wrong 5% of the time"
:::

::: {.fragment}
**How to choose Œ±:**
- **Œ± = 0.05:** Standard for most research
- **Œ± = 0.01:** More conservative, when Type I errors are costly
- **Œ± = 0.10:** Less conservative, when Type II errors are costly
:::

::: {.fragment}
**Important:** Choose Œ± **before** collecting data to avoid bias!
:::

## Step 3: Check Assumptions and Conditions

Common assumptions for many tests:

::: {.incremental}
- **Independence:** Observations don't influence each other
- **Normality:** Data comes from a normal distribution (or n ‚â• 30)
- **Equal variances:** When comparing groups
- **Random sampling:** Sample represents the population
:::

::: {.fragment}
**What if assumptions are violated?**
- Use non-parametric tests
- Transform the data
- Use robust methods
- Increase sample size
:::

## Step 4: Calculate the Test Statistic

::: {.incremental}
- **Test statistic:** A standardized measure of how far our sample result is from what we'd expect if H‚ÇÄ were true
- **Common test statistics:**
  - **z-statistic:** For means when œÉ is known
  - **t-statistic:** For means when œÉ is unknown
  - **œá¬≤ statistic:** For categorical data
  - **F-statistic:** For comparing variances
:::

::: {.fragment}
**Formula for one-sample t-test:**
$$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$$

Where: $\bar{x}$ = sample mean, $\mu_0$ = hypothesized mean, $s$ = sample standard deviation, $n$ = sample size
:::

## Step 5: Find the P-value

```{python}
#| echo: false
#| eval: true
#| fig-align: center

fig, ax = plt.subplots(figsize=(10, 6))

# Generate t-distribution
x = np.linspace(-4, 4, 1000)
df = 9  # degrees of freedom
y = stats.t.pdf(x, df)

# Plot the distribution
ax.plot(x, y, 'b-', linewidth=2, label=f't-distribution (df={df})')

# Mark the observed test statistic
t_obs = 2.3
ax.axvline(t_obs, color='red', linewidth=3, label=f'Observed t = {t_obs}')

# Shade the p-value area (right-tailed)
x_pval = x[x >= t_obs]
y_pval = y[x >= t_obs]
ax.fill_between(x_pval, y_pval, alpha=0.3, color='red', label='P-value')

# Calculate and display p-value
p_value = 1 - stats.t.cdf(t_obs, df)
ax.text(t_obs + 0.3, 0.15, f'P-value = {p_value:.4f}', fontsize=14, 
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

ax.set_xlabel('t-statistic', fontsize=12)
ax.set_ylabel('Probability Density', fontsize=12)
ax.set_title('P-value: Probability of Observing This Result (or More Extreme)\nif the Null Hypothesis is True', 
             fontsize=14, fontweight='bold')
ax.legend(fontsize=12)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

::: {.fragment}
**P-value interpretation:** "If H‚ÇÄ were true, what's the probability of getting a test statistic at least as extreme as what we observed?"
:::

## Common P-value Misconceptions

::: {.callout-warning}
## What P-values DON'T tell us

‚ùå **WRONG:** "P-value is the probability that H‚ÇÄ is true"

‚ùå **WRONG:** "P-value is the probability of making an error"

‚ùå **WRONG:** "1 - p-value is the probability that H‚ÇÅ is true"

‚ùå **WRONG:** "Smaller p-values mean larger effects"
:::

::: {.fragment}
‚úÖ **CORRECT:** "P-value is the probability of observing this result (or more extreme) **assuming H‚ÇÄ is true**"
:::

## Step 6: Make a Decision and Interpret

::: {.incremental}
**Decision Rule:**
- If p-value ‚â§ Œ±: **Reject H‚ÇÄ** (statistically significant)
- If p-value > Œ±: **Fail to reject H‚ÇÄ** (not statistically significant)
:::

::: {.fragment}
**Language matters:**
- ‚úÖ "Reject H‚ÇÄ" or "Fail to reject H‚ÇÄ"
- ‚ùå "Accept H‚ÇÄ" or "Prove H‚ÇÅ"
- ‚úÖ "Evidence suggests..." or "Data supports..."
- ‚ùå "H‚ÇÅ is true" or "H‚ÇÄ is false"
:::

## Types of Errors

```{python}
#| echo: false
#| fig-align: center

import pandas as pd

# Create error types table
error_data = {
    '': ['Decision: Reject H‚ÇÄ', 'Decision: Fail to Reject H‚ÇÄ'],
    'H‚ÇÄ is True (Reality)': ['Type I Error\n(Œ± = P(Type I))', 'Correct Decision\n(1 - Œ±)'],
    'H‚ÇÄ is False (Reality)': ['Correct Decision\n(1 - Œ≤ = Power)', 'Type II Error\n(Œ≤ = P(Type II))']
}

fig, ax = plt.subplots(figsize=(12, 6))
ax.axis('tight')
ax.axis('off')

# Create table
table_data = [
    ['', 'H‚ÇÄ is True (Reality)', 'H‚ÇÄ is False (Reality)'],
    ['Decision: Reject H‚ÇÄ', 'Type I Error\n(Œ± = P(Type I))\n"False Positive"', 'Correct Decision\n(1 - Œ≤ = Power)\n"True Positive"'],
    ['Decision: Fail to Reject H‚ÇÄ', 'Correct Decision\n(1 - Œ±)\n"True Negative"', 'Type II Error\n(Œ≤ = P(Type II))\n"False Negative"']
]

table = ax.table(cellText=table_data, cellLoc='center', loc='center',
                colWidths=[0.25, 0.35, 0.35])
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1, 3)

# Color code the cells
for i in range(3):
    for j in range(3):
        cell = table[(i, j)]
        if i == 0:  # Header row
            cell.set_facecolor('#4472C4')
            cell.set_text_props(weight='bold', color='white')
        elif j == 0:  # Header column
            cell.set_facecolor('#4472C4')
            cell.set_text_props(weight='bold', color='white')
        elif (i == 1 and j == 1) or (i == 2 and j == 2):  # Error cells
            cell.set_facecolor('#FFE6E6')
        else:  # Correct decision cells
            cell.set_facecolor('#E6F3E6')

ax.set_title('Types of Errors in Hypothesis Testing', fontsize=16, fontweight='bold', pad=20)
plt.show()
```

## Real-World Error Consequences

::: columns
::: {.column width="50%"}
**Type I Error Examples:**
- **Medical:** Saying a drug works when it doesn't
- **Legal:** Convicting an innocent person
- **Quality Control:** Rejecting good products
- **Marketing:** Launching ineffective campaigns
:::

::: {.column width="50%"}
**Type II Error Examples:**
- **Medical:** Missing a disease diagnosis
- **Legal:** Acquitting a guilty person
- **Quality Control:** Accepting defective products
- **Security:** Missing a threat
:::
:::

::: {.fragment}
**The Trade-off:** Reducing one type of error usually increases the other. We must balance based on the consequences of each error type.
:::

## Statistical Power

::: {.incremental}
- **Power (1 - Œ≤):** The probability of correctly rejecting a false null hypothesis
- **What affects power?**
  - **Effect size:** Larger effects are easier to detect
  - **Sample size:** More data increases power
  - **Significance level:** Higher Œ± increases power
  - **Variability:** Less noise increases power
:::

```{python}
#| echo: false
#| fig-align: center

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# Power vs Sample Size
sample_sizes = np.arange(5, 101, 5)
effect_size = 0.5
alpha = 0.05

power_values = []
for n in sample_sizes:
    # Power calculation for one-sample t-test
    delta = effect_size * np.sqrt(n)
    t_crit = stats.t.ppf(1 - alpha, n - 1)
    power = 1 - stats.t.cdf(t_crit - delta, n - 1)
    power_values.append(power)

axes[0].plot(sample_sizes, power_values, 'b-', linewidth=3, marker='o')
axes[0].axhline(0.8, color='red', linestyle='--', linewidth=2, label='Desired Power = 0.8')
axes[0].set_xlabel('Sample Size', fontsize=12)
axes[0].set_ylabel('Statistical Power', fontsize=12)
axes[0].set_title('Power vs Sample Size\n(Effect Size = 0.5, Œ± = 0.05)', fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].legend()
axes[0].set_ylim(0, 1)

# Power vs Effect Size
effect_sizes = np.linspace(0.1, 1.5, 50)
n = 30
power_values_effect = []

for es in effect_sizes:
    delta = es * np.sqrt(n)
    t_crit = stats.t.ppf(1 - alpha, n - 1)
    power = 1 - stats.t.cdf(t_crit - delta, n - 1)
    power_values_effect.append(power)

axes[1].plot(effect_sizes, power_values_effect, 'g-', linewidth=3, marker='o')
axes[1].axhline(0.8, color='red', linestyle='--', linewidth=2, label='Desired Power = 0.8')
axes[1].set_xlabel('Effect Size', fontsize=12)
axes[1].set_ylabel('Statistical Power', fontsize=12)
axes[1].set_title('Power vs Effect Size\n(n = 30, Œ± = 0.05)', fontweight='bold')
axes[1].grid(True, alpha=0.3)
axes[1].legend()
axes[1].set_ylim(0, 1)

plt.tight_layout()
plt.show()
```

## Example 1: One-Sample t-test

**Research Question:** Does a new study technique improve test scores compared to the school average of 75?

```{python}
import numpy as np
from scipy import stats

# Sample data from students using new technique
np.random.seed(42)
new_method_scores = np.random.normal(78, 8, 25)  # n=25, mean=78, sd=8

print("Sample Statistics:")
print(f"Sample size: {len(new_method_scores)}")
print(f"Sample mean: {np.mean(new_method_scores):.2f}")
print(f"Sample std: {np.std(new_method_scores, ddof=1):.2f}")
```

**Step 1: State Hypotheses**
- H‚ÇÄ: Œº = 75 (new method doesn't improve scores)
- H‚ÇÅ: Œº > 75 (new method improves scores)

**Steps 2-3:** Œ± = 0.05, assume normality (n=25 is borderline, but we'll proceed)

## Example 1: Calculations

```{python}
# Step 4: Calculate test statistic
mu_0 = 75  # hypothesized mean
x_bar = np.mean(new_method_scores)
s = np.std(new_method_scores, ddof=1)
n = len(new_method_scores)

t_stat = (x_bar - mu_0) / (s / np.sqrt(n))

print(f"Test statistic: t = {t_stat:.3f}")

# Step 5: Calculate p-value (right-tailed test)
df = n - 1
p_value = 1 - stats.t.cdf(t_stat, df)

print(f"Degrees of freedom: {df}")
print(f"P-value: {p_value:.4f}")

# Step 6: Make decision
alpha = 0.05
print(f"\nDecision:")
print(f"Œ± = {alpha}")
if p_value <= alpha:
    print(f"P-value ({p_value:.4f}) ‚â§ Œ± ({alpha}): Reject H‚ÇÄ")
    print("Conclusion: There is sufficient evidence that the new study technique improves test scores.")
else:
    print(f"P-value ({p_value:.4f}) > Œ± ({alpha}): Fail to reject H‚ÇÄ")
    print("Conclusion: There is insufficient evidence that the new study technique improves test scores.")
```

## Example 1: Visualization

```{python}
#| echo: false
#| fig-align: center

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Left plot: Sample data
ax1.hist(new_method_scores, bins=8, alpha=0.7, color='skyblue', edgecolor='black')
ax1.axvline(x_bar, color='red', linewidth=3, label=f'Sample Mean = {x_bar:.2f}')
ax1.axvline(mu_0, color='orange', linewidth=3, linestyle='--', label=f'H‚ÇÄ: Œº = {mu_0}')
ax1.set_xlabel('Test Scores', fontsize=12)
ax1.set_ylabel('Frequency', fontsize=12)
ax1.set_title('Sample Data: New Study Method Scores', fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Right plot: t-distribution with test statistic and p-value
x = np.linspace(-4, 4, 1000)
y = stats.t.pdf(x, df)

ax2.plot(x, y, 'b-', linewidth=2, label=f't-distribution (df={df})')
ax2.axvline(t_stat, color='red', linewidth=3, label=f'Observed t = {t_stat:.3f}')

# Shade p-value area
x_pval = x[x >= t_stat]
y_pval = stats.t.pdf(x_pval, df)
ax2.fill_between(x_pval, y_pval, alpha=0.3, color='red', label=f'P-value = {p_value:.4f}')

# Critical value
t_crit = stats.t.ppf(1 - alpha, df)
ax2.axvline(t_crit, color='green', linewidth=2, linestyle=':', label=f'Critical value = {t_crit:.3f}')

ax2.set_xlabel('t-statistic', fontsize=12)
ax2.set_ylabel('Probability Density', fontsize=12)
ax2.set_title('Hypothesis Test Results', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Using Python for Hypothesis Testing

```{python}
# Method 1: Using scipy.stats.ttest_1samp
from scipy.stats import ttest_1samp

# One-sample t-test
t_statistic, p_value = ttest_1samp(new_method_scores, mu_0)

print("Using scipy.stats.ttest_1samp:")
print(f"t-statistic: {t_statistic:.3f}")
print(f"p-value (two-tailed): {p_value:.4f}")
print(f"p-value (one-tailed): {p_value/2:.4f}")  # For right-tailed test

# Method 2: Using statsmodels for more detailed output
import statsmodels.stats.weightstats as sm

# Create DescrStatsW object
desc_stats = sm.DescrStatsW(new_method_scores)

# Perform one-sample t-test
t_stat_sm, p_val_sm, df_sm = desc_stats.ttest_mean(mu_0, alternative='larger')

print(f"\nUsing statsmodels:")
print(f"t-statistic: {t_stat_sm:.3f}")
print(f"p-value (one-tailed): {p_val_sm:.4f}")
print(f"degrees of freedom: {df_sm}")
```

## Example 2: Two-Sample t-test

**Research Question:** Is there a difference in test scores between two teaching methods?

```{python}
# Generate sample data for two groups
np.random.seed(123)
method_A = np.random.normal(75, 10, 30)  # Traditional method
method_B = np.random.normal(80, 12, 28)  # New method

print("Method A (Traditional):")
print(f"  n = {len(method_A)}, mean = {np.mean(method_A):.2f}, std = {np.std(method_A, ddof=1):.2f}")

print("Method B (New):")
print(f"  n = {len(method_B)}, mean = {np.mean(method_B):.2f}, std = {np.std(method_B, ddof=1):.2f}")

# Hypotheses
print("\nHypotheses:")
print("H‚ÇÄ: Œº_A = Œº_B (no difference between methods)")
print("H‚ÇÅ: Œº_A ‚â† Œº_B (there is a difference)")

# Perform two-sample t-test
from scipy.stats import ttest_ind

# Assuming equal variances (we can test this separately)
t_stat_2samp, p_val_2samp = ttest_ind(method_A, method_B, equal_var=True)

print(f"\nTwo-sample t-test results:")
print(f"t-statistic: {t_stat_2samp:.3f}")
print(f"p-value: {p_val_2samp:.4f}")

# Decision
alpha = 0.05
if p_val_2samp <= alpha:
    print(f"\nDecision: Reject H‚ÇÄ (p = {p_val_2samp:.4f} ‚â§ {alpha})")
    print("Conclusion: There is a significant difference between the two teaching methods.")
else:
    print(f"\nDecision: Fail to reject H‚ÇÄ (p = {p_val_2samp:.4f} > {alpha})")
    print("Conclusion: No significant difference between teaching methods.")
```

## Two-Sample Test Visualization

```{python}
#| echo: false
#| fig-align: center

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Box plots comparing the two methods
data_to_plot = [method_A, method_B]
labels = ['Method A\n(Traditional)', 'Method B\n(New)']

box_plot = ax1.boxplot(data_to_plot, labels=labels, patch_artist=True)
box_plot['boxes'][0].set_facecolor('lightcoral')
box_plot['boxes'][1].set_facecolor('lightblue')

ax1.set_ylabel('Test Scores', fontsize=12)
ax1.set_title('Comparison of Teaching Methods', fontweight='bold')
ax1.grid(True, alpha=0.3)

# Add means
means = [np.mean(method_A), np.mean(method_B)]
ax1.plot([1, 2], means, 'ro-', linewidth=2, markersize=8, label='Group Means')
ax1.legend()

# t-distribution for two-sample test
df_2samp = len(method_A) + len(method_B) - 2
x = np.linspace(-4, 4, 1000)
y = stats.t.pdf(x, df_2samp)

ax2.plot(x, y, 'b-', linewidth=2, label=f't-distribution (df={df_2samp})')
ax2.axvline(t_stat_2samp, color='red', linewidth=3, label=f'Observed t = {t_stat_2samp:.3f}')
ax2.axvline(-t_stat_2samp, color='red', linewidth=3, alpha=0.7)

# Shade p-value areas (two-tailed)
t_abs = abs(t_stat_2samp)
x_pval_right = x[x >= t_abs]
x_pval_left = x[x <= -t_abs]
y_pval_right = stats.t.pdf(x_pval_right, df_2samp)
y_pval_left = stats.t.pdf(x_pval_left, df_2samp)

ax2.fill_between(x_pval_right, y_pval_right, alpha=0.3, color='red')
ax2.fill_between(x_pval_left, y_pval_left, alpha=0.3, color='red', 
                label=f'P-value = {p_val_2samp:.4f}')

ax2.set_xlabel('t-statistic', fontsize=12)
ax2.set_ylabel('Probability Density', fontsize=12)
ax2.set_title('Two-Sample t-test Results', fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## Effect Size: Cohen's d

```{python}
# Calculate Cohen's d for effect size
def cohens_d(group1, group2):
    """Calculate Cohen's d for two independent groups"""
    n1, n2 = len(group1), len(group2)
    mean1, mean2 = np.mean(group1), np.mean(group2)
    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
    
    # Pooled standard deviation
    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
    
    # Cohen's d
    d = (mean1 - mean2) / pooled_std
    return d

d = cohens_d(method_A, method_B)
print(f"Cohen's d: {d:.3f}")

# Interpret effect size
if abs(d) < 0.2:
    interpretation = "negligible"
elif abs(d) < 0.5:
    interpretation = "small"
elif abs(d) < 0.8:
    interpretation = "medium"
else:
    interpretation = "large"

print(f"Effect size interpretation: {interpretation}")

# Cohen's d interpretation guide
print("\nCohen's d interpretation:")
print("  |d| < 0.2: negligible effect")
print("  0.2 ‚â§ |d| < 0.5: small effect")
print("  0.5 ‚â§ |d| < 0.8: medium effect")
print("  |d| ‚â• 0.8: large effect")
```

## Common Hypothesis Tests Summary

```{python}
#| echo: false
#| fig-align: center

import pandas as pd

# Create a summary table of common tests
test_data = {
    'Test': ['One-sample t-test', 'Two-sample t-test', 'Paired t-test', 
             'One-sample z-test', 'Chi-square goodness of fit', 'Chi-square independence'],
    'Purpose': ['Compare sample mean to known value', 'Compare means of two groups', 
                'Compare paired observations', 'Compare sample mean (known œÉ)',
                'Test if data fits distribution', 'Test independence of variables'],
    'Data Type': ['Continuous', 'Continuous', 'Continuous', 'Continuous', 
                  'Categorical', 'Categorical'],
    'Python Function': ['stats.ttest_1samp()', 'stats.ttest_ind()', 'stats.ttest_rel()',
                       'stats.normaltest()', 'stats.chisquare()', 'stats.chi2_contingency()']
}

df = pd.DataFrame(test_data)
print("Common Hypothesis Tests:")
print("=" * 80)
for i, row in df.iterrows():
    print(f"\n{row['Test']}:")
    print(f"  Purpose: {row['Purpose']}")
    print(f"  Data Type: {row['Data Type']}")
    print(f"  Python: {row['Python Function']}")
```

## Activity: Practice Problem

::: {.callout-tip}
## Your Turn!

A coffee shop claims their average wait time is 5 minutes. You collect data on 20 customers and find:
- Sample mean: 5.8 minutes
- Sample standard deviation: 2.1 minutes

**Questions:**
1. Set up appropriate hypotheses (use Œ± = 0.05)
2. What type of test should you use?
3. Calculate the test statistic and p-value
4. Make a decision and interpret the results
5. What are the practical implications?
:::

::: {.fragment}
**Think about:** Is this a one-tailed or two-tailed test? What assumptions do you need to check?
:::

## Practice Problem Solution

```{python}
# Given data
sample_mean = 5.8
claimed_mean = 5.0
sample_std = 2.1
n = 20
alpha = 0.05

print("Practice Problem Solution:")
print("=" * 40)

# 1. Hypotheses
print("1. Hypotheses:")
print("   H‚ÇÄ: Œº = 5 (average wait time is 5 minutes)")
print("   H‚ÇÅ: Œº ‚â† 5 (average wait time is different from 5 minutes)")
print("   (Two-tailed test - we're testing if it's different, not specifically longer)")

# 2. Test type
print("\n2. Test type: One-sample t-test")
print("   (Population standard deviation unknown, small sample)")

# 3. Calculate test statistic
t_stat = (sample_mean - claimed_mean) / (sample_std / np.sqrt(n))
df = n - 1
p_value = 2 * (1 - stats.t.cdf(abs(t_stat), df))  # Two-tailed

print(f"\n3. Calculations:")
print(f"   t = ({sample_mean} - {claimed_mean}) / ({sample_std} / ‚àö{n}) = {t_stat:.3f}")
print(f"   df = {n} - 1 = {df}")
print(f"   p-value = {p_value:.4f}")

# 4. Decision
print(f"\n4. Decision:")
if p_value <= alpha:
    decision = "Reject H‚ÇÄ"
    conclusion = "sufficient evidence that wait time differs from 5 minutes"
else:
    decision = "Fail to reject H‚ÇÄ"
    conclusion = "insufficient evidence that wait time differs from 5 minutes"

print(f"   {decision} (p = {p_value:.4f}, Œ± = {alpha})")
print(f"   Conclusion: There is {conclusion}.")

# 5. Practical implications
print(f"\n5. Practical implications:")
print(f"   The actual average wait time appears to be about {sample_mean} minutes,")
print(f"   which is {sample_mean - claimed_mean} minutes longer than claimed.")
print(f"   Management should investigate why wait times exceed the 5-minute target.")
```

## Common Mistakes and Pitfalls

::: {.callout-warning}
## Avoid These Common Errors

1. **Confusing practical vs. statistical significance**
   - Large samples can detect tiny, meaningless differences
   - Always consider effect size and practical importance

2. **P-hacking / Data dredging**
   - Testing multiple hypotheses until finding significance
   - Solution: Adjust Œ±, pre-specify analyses

3. **Misinterpreting p-values**
   - P-value ‚â† probability that H‚ÇÄ is true
   - P-value ‚â† probability of making an error

4. **Ignoring assumptions**
   - Check normality, independence, equal variances
   - Use appropriate alternatives when violated

5. **Choosing Œ± after seeing results**
   - Always set significance level before analysis
   - Avoid changing criteria to get desired results
:::

## Statistical Significance vs. Practical Significance

```{python}
#| echo: false
#| fig-align: center

# Demonstrate difference between statistical and practical significance
np.random.seed(42)

# Scenario 1: Large sample, small effect
large_n = 10000
small_effect = np.random.normal(100.1, 15, large_n)  # Tiny effect (0.1 points)
t_stat_large, p_val_large = ttest_1samp(small_effect, 100)

# Scenario 2: Small sample, large effect  
small_n = 10
large_effect = np.random.normal(110, 15, small_n)  # Large effect (10 points)
t_stat_small, p_val_small = ttest_1samp(large_effect, 100)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Scenario 1 visualization
ax1.hist(small_effect, bins=50, alpha=0.7, color='lightcoral', density=True)
ax1.axvline(np.mean(small_effect), color='red', linewidth=3, 
           label=f'Sample mean = {np.mean(small_effect):.1f}')
ax1.axvline(100, color='blue', linewidth=3, linestyle='--', label='H‚ÇÄ: Œº = 100')
ax1.set_title(f'Large Sample (n={large_n}), Small Effect\nt = {t_stat_large:.2f}, p = {p_val_large:.2e}\nStatistically Significant!', 
              fontweight='bold')
ax1.set_xlabel('Values')
ax1.set_ylabel('Density')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Scenario 2 visualization
ax2.hist(large_effect, bins=8, alpha=0.7, color='lightblue', density=True)
ax2.axvline(np.mean(large_effect), color='red', linewidth=3, 
           label=f'Sample mean = {np.mean(large_effect):.1f}')
ax2.axvline(100, color='blue', linewidth=3, linestyle='--', label='H‚ÇÄ: Œº = 100')
ax2.set_title(f'Small Sample (n={small_n}), Large Effect\nt = {t_stat_small:.2f}, p = {p_val_small:.3f}\nNot Statistically Significant', 
              fontweight='bold')
ax2.set_xlabel('Values')
ax2.set_ylabel('Density')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("Key Lesson: Statistical significance ‚â† Practical importance")
print(f"Left: Tiny effect ({np.mean(small_effect) - 100:.1f}) but significant due to large n")
print(f"Right: Large effect ({np.mean(large_effect) - 100:.1f}) but not significant due to small n")
```

## Best Practices for Hypothesis Testing

::: {.incremental}
1. **Plan before you collect data**
   - Pre-specify hypotheses, Œ± level, and analysis plan
   - Calculate required sample size (power analysis)

2. **Check your assumptions**
   - Use diagnostic plots and tests
   - Consider robust alternatives if violated

3. **Report effect sizes**
   - P-values don't tell the whole story
   - Include confidence intervals for estimates

4. **Consider practical significance**
   - Is the difference meaningful in context?
   - What are the costs/benefits of different decisions?

5. **Be honest about multiple testing**
   - Adjust for multiple comparisons when appropriate
   - Report all tests performed, not just significant ones
:::

## Summary: The Logic of Hypothesis Testing

::: {.incremental}
1. **Start with skepticism** (assume H‚ÇÄ is true)
2. **Collect evidence** (sample data)
3. **Quantify surprise** (how unusual is this result if H‚ÇÄ were true?)
4. **Make a decision** (is the evidence strong enough to reject H‚ÇÄ?)
5. **Acknowledge uncertainty** (we might be wrong!)
:::

::: {.fragment}
**Remember:** Hypothesis testing doesn't prove anything definitively. It provides a framework for making decisions under uncertainty using probabilistic reasoning.
:::

## Key Takeaways

- **Hypothesis testing** helps us make decisions about populations using sample data
- **P-values** tell us how surprising our data would be if H‚ÇÄ were true
- **Statistical significance ‚â† practical importance**
- **Always check assumptions** and consider effect sizes
- **Plan your analysis before collecting data**
- **Be aware of Type I and Type II errors**

::: {.fragment}
**Next steps:** Practice with different types of tests, learn about confidence intervals, and explore more advanced topics like multiple testing corrections and non-parametric alternatives.
:::

---

## Appendix: Python Code Templates

```{python}
#| eval: false
#| echo: true

# Template for one-sample t-test
import numpy as np
from scipy import stats

# Your data
data = [...]  # Replace with your data
null_value = 0  # Replace with your null hypothesis value

# Perform test
t_stat, p_value = stats.ttest_1samp(data, null_value)

# For one-tailed test, divide p-value by 2
p_value_one_tailed = p_value / 2

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value (two-tailed): {p_value:.4f}")
print(f"p-value (one-tailed): {p_value_one_tailed:.4f}")

# Template for two-sample t-test
group1 = [...]  # Replace with your first group
group2 = [...]  # Replace with your second group

# Perform test
t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=True)

print(f"t-statistic: {t_stat:.3f}")
print(f"p-value: {p_value:.4f}")

# Effect size (Cohen's d)
def cohens_d(x, y):
    nx, ny = len(x), len(y)
    dof = nx + ny - 2
    pooled_std = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / dof)
    return (np.mean(x) - np.mean(y)) / pooled_std

d = cohens_d(group1, group2)
print(f"Cohen's d: {d:.3f}")
```