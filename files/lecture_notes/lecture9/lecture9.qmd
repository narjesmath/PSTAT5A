---
title: "PSTAT 5A: Discrete Random Variables"
subtitle: "Lecture 7"
author: "Narjes Mathlouthi"
date: today
format: 
  revealjs:
    logo: /img/logo.png
    theme: default
    css: /files/lecture_notes/theme/lecture-styles.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Understanding Data - Discrete Random Variables © 2025 Narjes Mathlouthi"
    transition: slide
    background-transition: fade
jupyter: python3
---

## Welcome to Lecture 7 {.center}

**Discrete Random Variables**

*From outcomes to numbers: quantifying randomness*

![](img/random_variable_mapping.png){width=350px}
*Figure: A random variable maps outcomes (e.g., die rolls) to numbers.*

---

## Today's Learning Objectives

By the end of this lecture, you will be able to:

- Define random variables and distinguish discrete from continuous
- Work with probability mass functions (PMFs) and cumulative distribution functions (CDFs)
- Calculate expected values and variances for discrete random variables
- Apply properties of expectation and variance
- Work with common discrete distributions (Bernoulli, Binomial, Geometric, Poisson)
- Solve real-world problems using discrete random variables
- Use technology to compute probabilities and parameters

---

## What is a Random Variable?

A **random variable** is a function that assigns a numerical value to each outcome of a random experiment

**Notation**: Usually denoted by capital letters $X$, $Y$, $Z$

**Key insight**: Random variables transform outcomes into numbers, making mathematical analysis possible

![](img/random_variable_die.png){width=320px}
*Figure: Mapping die roll outcomes to values of $X$.*

:::{.fragment}
**Example**: Rolling a die
- Outcomes: $\{1, 2, 3, 4, 5, 6\}$
- Random variable $X$: the number shown
- $X$ can take values $\{1, 2, 3, 4, 5, 6\}$
:::

---

## Why Use Random Variables?

Random variables allow us to:

- **Quantify** outcomes numerically
- **Calculate** means, variances, and other statistics
- **Model** real-world phenomena mathematically
- **Make predictions** and decisions
- **Compare** different random processes

![](img/real_world_examples.png){width=340px}
*Figure: Examples of random variables in real life (height, test scores, etc.).*

:::{.fragment}
**Examples**: Height, test scores, number of defects, wait times, stock prices
:::

---

## Types of Random Variables

**Discrete Random Variable**: 
- Takes on a countable number of values
- Can list all possible values
- Examples: dice rolls, number of emails, quiz scores

**Continuous Random Variable**:
- Takes on uncountably many values (intervals)
- Cannot list all possible values  
- Examples: height, weight, time, temperature

```{mermaid}
graph TD
  A[Discrete: 0, 1, 2, ...] --|Gaps|--> D[Examples: Dice, Emails]
  B[Continuous: Any real value] --|No gaps|--> C[Examples: Height, Time]
```

:::{.fragment}
*Today we focus on discrete random variables*
:::

---

## Examples of Discrete Random Variables

**$X$ = Number of heads in 3 coin flips**
- Possible values: $\{0, 1, 2, 3\}$

**$Y$ = Number of customers in an hour**
- Possible values: $\{0, 1, 2, 3, \ldots\}$

**$Z$ = Score on a 10-question quiz**
- Possible values: $\{0, 1, 2, \ldots, 10\}$

![](img/coin_flips_heads.png){width=320px}
*Figure: Example of a discrete random variable: number of heads in coin flips.*

:::{.fragment}
Notice: All values are integers with gaps between them
:::

---

## Probability Mass Function (PMF)

The **probability mass function** of a discrete random variable $X$ is:

$$P(X = x) = \text{probability that } X \text{ takes the value } x$$

**Properties of PMF**:
1. $P(X = x) \geq 0$ for all $x$
2. $\sum_{\text{all } x} P(X = x) = 1$

```{mermaid}
%% Bar chart for PMF of a fair die
%% (Use Quarto's mermaid bar chart for illustration)
bar
    title PMF of a Fair Die
    x-axis 1 2 3 4 5 6
    y-axis Probability
    1: 0.1667
    2: 0.1667
    3: 0.1667
    4: 0.1667
    5: 0.1667
    6: 0.1667
```

:::{.fragment}
**Notation**: Sometimes written as $p(x)$ or $f(x)$
:::

---

## PMF Example: Fair Die

Let $X$ = outcome of rolling a fair six-sided die

$$P(X = x) = \begin{cases}
\frac{1}{6} & \text{if } x \in \{1, 2, 3, 4, 5, 6\} \\
0 & \text{otherwise}
\end{cases}$$

![](img/pmf_fair_die.png){width=340px}
*Figure: Bar chart of PMF for a fair die.*

:::{.fragment}
**Verification**: 
- All probabilities ≥ 0 ✓
- Sum = $6 \times \frac{1}{6} = 1$ ✓
:::

---

## PMF Example: Two Coin Flips

Let $X$ = number of heads in two coin flips

Sample space: $\{HH, HT, TH, TT\}$

```{mermaid}
bar
    title PMF: Number of Heads in 2 Coin Flips
    x-axis 0 1 2
    y-axis Probability
    0: 0.25
    1: 0.5
    2: 0.25
```

:::{.fragment}
| $x$ | Outcomes | $P(X = x)$ |
|-----|----------|------------|
| 0   | TT       | 1/4        |
| 1   | HT, TH   | 2/4 = 1/2  |
| 2   | HH       | 1/4        |

**Verification**: $\frac{1}{4} + \frac{1}{2} + \frac{1}{4} = 1$ ✓
:::

---

## Cumulative Distribution Function (CDF)

The **cumulative distribution function** of a random variable $X$ is:

$$F(x) = P(X \leq x)$$

**Properties of CDF**:
1. $F(x)$ is non-decreasing
2. $\lim_{x \to -\infty} F(x) = 0$
3. $\lim_{x \to \infty} F(x) = 1$
4. $F(x)$ is right-continuous

```{mermaid}
%% Step function for CDF of two coin flips
%% (Mermaid doesn't support step plots, so use a line chart for illustration)
line
    title CDF: Number of Heads in 2 Coin Flips
    x-axis -1 0 1 2 3
    y-axis F(x)
    -1: 0
    0: 0.25
    1: 0.75
    2: 1
    3: 1
```

---

## Expected Value (Mean)

The **expected value** of a discrete random variable $X$ is:

$$E[X] = \mu = \sum_{\text{all } x} x \cdot P(X = x)$$

![](img/expected_value_coins.png){width=320px}
*Figure: Expected value as the long-run average of repeated coin flips.*

---

## Variance

The **variance** of a random variable $X$ measures spread around the mean:

$$\text{Var}(X) = \sigma^2 = E[(X - \mu)^2]$$

![](img/variance_spread.png){width=320px}
*Figure: Variance measures the spread of possible values around the mean.*

---

## Bernoulli Distribution

A **Bernoulli random variable** models a single trial with two outcomes:

- Success (1) with probability $p$
- Failure (0) with probability $1-p$

```{mermaid}
bar
    title Bernoulli PMF (p=0.7)
    x-axis 0 1
    y-axis Probability
    0: 0.3
    1: 0.7
```

---

## Binomial Distribution

A **binomial random variable** counts successes in $n$ independent Bernoulli trials, each with success probability $p$

![](img/binomial_trials.png){width=340px}
*Figure: Binomial experiment: sequence of independent trials.*

```{mermaid}
bar
    title Binomial PMF (n=5, p=0.5)
    x-axis 0 1 2 3 4 5
    y-axis Probability
    0: 0.031
    1: 0.156
    2: 0.312
    3: 0.312
    4: 0.156
    5: 0.031
```

---

## Geometric Distribution

A **geometric random variable** counts the number of trials until the first success

![](img/geometric_trials.png){width=320px}
*Figure: Geometric experiment: waiting for the first success.*

```{mermaid}
bar
    title Geometric PMF (p=0.3)
    x-axis 1 2 3 4 5
    y-axis Probability
    1: 0.3
    2: 0.21
    3: 0.147
    4: 0.1029
    5: 0.072
```

---

## Poisson Distribution

A **Poisson random variable** counts events occurring in a fixed interval when events happen at a constant average rate

![](img/poisson_events.png){width=320px}
*Figure: Poisson process: random events in time.*

```{mermaid}
bar
    title Poisson PMF (λ=3)
    x-axis 0 1 2 3 4 5 6
    y-axis Probability
    0: 0.0498
    1: 0.1494
    2: 0.2240
    3: 0.2240
    4: 0.1680
    5: 0.1008
    6: 0.0504
```

---

## Hypergeometric Distribution

A **hypergeometric random variable** counts successes when sampling **without replacement** from a finite population

![](img/hypergeometric_sampling.png){width=340px}
*Figure: Hypergeometric sampling: drawing balls from a box without replacement.*

---

## Law of Large Numbers

As $n$ increases, the sample mean approaches the expected value:

$$\bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n} \to E[X]$$

```{mermaid}
line
    title Law of Large Numbers Simulation
    x-axis 0 100 200 300 400 500 600 700 800 900 1000
    y-axis Sample Mean
    0: 0.0
    100: 0.48
    200: 0.51
    300: 0.49
    400: 0.50
    500: 0.51
    600: 0.50
    700: 0.50
    800: 0.49
    900: 0.50
    1000: 0.50
```
*Figure: As the number of trials increases, the sample mean converges to the expected value.*

---

## Simulation/Monte Carlo

**Monte Carlo Method**: Use computer simulation to estimate probabilities

![](img/monte_carlo_simulation.png){width=340px}
*Figure: Monte Carlo simulation: repeated random experiments to estimate probabilities.*

---

## Practice Problem 1

A box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let $X$ = number of red balls drawn.

Find the PMF of $X$.

:::{.fragment}
**Solution**: $X$ can be 0, 1, or 2

$P(X = 0) = \frac{\binom{3}{0}\binom{2}{2}}{\binom{5}{2}} = \frac{1 \times 1}{10} = \frac{1}{10}$

$P(X = 1) = \frac{\binom{3}{1}\binom{2}{1}}{\binom{5}{2}} = \frac{3 \times 2}{10} = \frac{6}{10}$

$P(X = 2) = \frac{\binom{3}{2}\binom{2}{0}}{\binom{5}{2}} = \frac{3 \times 1}{10} = \frac{3}{10}$
:::

---

## Practice Problem 2

Using the red balls example from Problem 1, find:

a) $F(0.5)$
b) $F(1)$  
c) $P(X > 1)$
d) $P(0.5 < X \leq 1.5)$

:::{.fragment}
**Solutions**:
a) $F(0.5) = P(X \leq 0.5) = P(X = 0) = \frac{1}{10}$
b) $F(1) = P(X \leq 1) = P(X = 0) + P(X = 1) = \frac{1}{10} + \frac{6}{10} = \frac{7}{10}$
c) $P(X > 1) = 1 - F(1) = 1 - \frac{7}{10} = \frac{3}{10}$
d) $P(0.5 < X \leq 1.5) = F(1.5) - F(0.5) = \frac{7}{10} - \frac{1}{10} = \frac{6}{10}$
:::

---

## Practice Problem 3

Find the expected value for the red balls example (Problem 1).

:::{.fragment}
**Solution**:
$$E[X] = 0 \cdot \frac{1}{10} + 1 \cdot \frac{6}{10} + 2 \cdot \frac{3}{10}$$

$$= 0 + \frac{6}{10} + \frac{6}{10} = \frac{12}{10} = 1.2$$

On average, we expect to draw 1.2 red balls
:::

---

## Properties of Expected Value

**Linearity of Expectation**:
1. $E[c] = c$ (constant)
2. $E[cX] = c \cdot E[X]$ (scaling)
3. $E[X + Y] = E[X] + E[Y]$ (additivity)
4. $E[aX + bY + c] = aE[X] + bE[Y] + c$

:::{.fragment}
**Important**: Property 3 holds even if $X$ and $Y$ are dependent!
:::

---

## Expected Value of Functions

For a function $g(X)$ of a random variable $X$:

$$E[g(X)] = \sum_{\text{all } x} g(x) \cdot P(X = x)$$

:::{.fragment}
**Common example**: $g(X) = X^2$

$$E[X^2] = \sum_{\text{all } x} x^2 \cdot P(X = x)$$

*Note*: Generally $E[g(X)] \neq g(E[X])$
:::

---

## Practice Problem 4

For the red balls example, find the variance and standard deviation.

We found $E[X] = 1.2$. First find $E[X^2]$:

:::{.fragment}
$$E[X^2] = 0^2 \cdot \frac{1}{10} + 1^2 \cdot \frac{6}{10} + 2^2 \cdot \frac{3}{10}$$
$$= 0 + \frac{6}{10} + \frac{12}{10} = \frac{18}{10} = 1.8$$

$$\text{Var}(X) = 1.8 - (1.2)^2 = 1.8 - 1.44 = 0.36$$

$$\sigma = \sqrt{0.36} = 0.6$$
:::

---

## Practice Problem 5

For the quiz example:

a) What's the expected number of correct answers?
b) What's the standard deviation?
c) What's the probability of getting at least 4 correct?

:::{.fragment}
**Solutions**:
a) $E[X] = np = 10 \times 0.25 = 2.5$
b) $\sigma = \sqrt{np(1-p)} = \sqrt{10 \times 0.25 \times 0.75} = \sqrt{1.875} \approx 1.37$
c) $P(X \geq 4) = 1 - P(X \leq 3) = 1 - [P(X=0) + P(X=1) + P(X=2) + P(X=3)]$
   Use binomial table or calculator: $P(X \geq 4) \approx 0.224$
:::

---

## Practice Problem 6

A quality control inspector tests items until finding the first defective one. If 5% of items are defective:

a) What's the probability the first defective item is the 10th one tested?
b) What's the expected number of items tested?
c) What's the probability of testing more than 20 items?

:::{.fragment}
**Solutions**:
a) $P(X = 10) = (0.95)^9 \times 0.05 \approx 0.0315$
b) $E[X] = \frac{1}{0.05} = 20$ items
c) $P(X > 20) = (0.95)^{20} \approx 0.358$
:::

---

## Practice Problem 7

For the call center example:

a) What's the probability of no calls in a minute?
b) What's the probability of at most 2 calls?
c) In a 2-minute period, what's the expected number of calls?

:::{.fragment}
**Solutions**:
a) $P(X = 0) = \frac{3^0 e^{-3}}{0!} = e^{-3} \approx 0.0498$
b) $P(X \leq 2) = P(X=0) + P(X=1) + P(X=2) \approx 0.0498 + 0.1494 + 0.2240 = 0.423$
c) For 2 minutes: $Y \sim \text{Poisson}(6)$, so $E[Y] = 6$ calls
:::

---

## Practice Problem 8

For the light bulb example:

a) What's the expected number of defective bulbs in the sample?
b) What's the probability of no defective bulbs?
c) What's the probability of at least 2 defective bulbs?

:::{.fragment}
**Solutions**:
a) $E[X] = 5 \times \frac{3}{20} = 0.75$ bulbs
b) $P(X = 0) = \frac{\binom{3}{0}\binom{17}{5}}{\binom{20}{5}} = \frac{6188}{15504} \approx 0.399$
c) $P(X \geq 2) = 1 - P(X = 0) - P(X = 1) \approx 1 - 0.399 - 0.461 = 0.140$
:::

---

## Practice Problem 9

A website receives an average of 2 orders per minute. Assuming orders follow a Poisson process:

a) What's the probability of exactly 3 orders in a minute?
b) What's the probability of no orders in 30 seconds?
c) What's the probability of more than 5 orders in 2 minutes?

:::{.fragment}
**Solutions**:
a) $X \sim \text{Poisson}(2)$: $P(X = 3) = \frac{2^3 e^{-2}}{3!} \approx 0.180$
b) $Y \sim \text{Poisson}(1)$: $P(Y = 0) = e^{-1} \approx 0.368$
c) $Z \sim \text{Poisson}(4)$: $P(Z > 5) = 1 - P(Z \leq 5) \approx 0.215$
:::

---

## Central Limit Theorem Preview

For large $n$, many discrete distributions approach normal:

**Binomial**: $X \sim \text{Binomial}(n, p)$ → $N(np, np(1-p))$ when $np > 5$ and $n(1-p) > 5$

**Poisson**: $X \sim \text{Poisson}(\lambda)$ → $N(\lambda, \lambda)$ when $\lambda > 10$

:::{.fragment}
*This connection will be crucial for hypothesis testing and confidence intervals*
:::

---

## Common Mistakes

1. **Wrong distribution choice**: Check assumptions carefully
2. **Parameter confusion**: Is it $n$, $p$, or $\lambda$?
3. **Inclusion errors**: "At least 3" vs "More than 3"
4. **Independence assumption**: Sampling with/without replacement
5. **Technology errors**: pdf vs cdf functions

---

## Practice Problem 10

A multiple choice test has 20 questions with 5 options each. A student knows the answers to 12 questions and guesses on the rest.

a) What's the expected score?
b) What's the probability of scoring at least 15?
c) What's the standard deviation of the score?

:::{.fragment}
**Solution**: Let $X$ = known correct, $Y$ = guessed correct
- $X = 12$ (deterministic)
- $Y \sim \text{Binomial}(8, 0.2)$
- Total score $S = X + Y = 12 + Y$

a) $E[S] = 12 + E[Y] = 12 + 8(0.2) = 13.6$
b) $P(S \geq 15) = P(Y \geq 3) \approx 0.203$  
c) $\sigma_S = \sigma_Y = \sqrt{8(0.2)(0.8)} = \sqrt{1.28} \approx 1.13$
:::

---

## Extensions and Advanced Topics

**Negative Binomial**: Number of trials to get $r$ successes

**Multinomial**: Extension of binomial to more than 2 categories

**Compound Distributions**: Sums of random variables

**Generating Functions**: Advanced technique for deriving properties

:::{.fragment}
*These topics appear in advanced probability courses*
:::

---

## Historical Context

**Jacob Bernoulli** (1654-1705): Bernoulli trials and law of large numbers

**Siméon Denis Poisson** (1781-1840): Poisson distribution and approximation

**Abraham de Moivre** (1667-1754): Normal approximation to binomial

**Modern Applications**: Computer science, machine learning, bioinformatics

---

## Common Student Questions

**Q**: "How do I choose between binomial and hypergeometric?"
**A**: Use binomial for sampling with replacement, hypergeometric without

**Q**: "When can I use Poisson approximation?"
**A**: When $n$ is large, $p$ is small, and $np$ is moderate

**Q**: "Why does Poisson have mean = variance?"
**A**: Mathematical property arising from its derivation as a limit

**Q**: "How do I know if trials are independent?"
**A**: Check if outcome of one trial affects others

---

## Key Formulas Summary

- **Expected Value**: $E[X] = \sum x \cdot P(X = x)$
- **Variance**: $\text{Var}(X) = E[X^2] - (E[X])^2$
- **Binomial**: $P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$
- **Geometric**: $P(X = k) = (1-p)^{k-1} p$
- **Poisson**: $P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}$

---

## Looking Ahead

**Next lecture**: Continuous Random Variables
- Probability density functions (PDFs)
- Normal distribution
- Exponential distribution  
- Central Limit Theorem applications

**Connection**: Discrete distributions often approximate continuous ones, and vice versa

---

## Study Tips

1. **Master the basics**: PMF, CDF, expectation, variance
2. **Learn distribution characteristics**: When to use each one
3. **Practice with technology**: Get comfortable with calculators/software
4. **Work real problems**: Apply distributions to actual scenarios
5. **Check your intuition**: Do answers make practical sense?

---

## Final Thoughts

Discrete random variables are fundamental to:
- Modeling real-world phenomena
- Making statistical inferences
- Understanding probability theory
- Building more complex models

:::{.fragment}
**Key insight**: Random variables transform unpredictable outcomes into predictable patterns
:::

---

## Questions? {.center}

**Office Hours**: [Your office hours]
**Email**: [Your email]  
**Next Class**: Continuous Random Variables

*Remember: Homework due [date]*

---

## Bonus: Law of Large Numbers

As $n$ increases, the sample mean approaches the expected value:

$$\bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n} \to E[X]$$

**Example**: Flip a coin 1000 times - proportion of heads will be close to 0.5

*This justifies our interpretation of expected value as "long-run average"*

---

## Bonus: Simulation

**Monte Carlo Method**: Use computer simulation to estimate probabilities

```r
# Simulate 10,000 binomial random variables
X <- rbinom(10000, size=10, prob=0.3)
mean(X)  # Should be close to 10*0.3 = 3
var(X)   # Should be close to 10*0.3*0.7 = 2.1
```

*Simulation helps verify theoretical results and solve complex problems*