---
title: "PSTAT 5A: Sampling and Confidence Intervals"
subtitle: "Lecture 9 - Making Sense of Uncertainty"
author: "Narjes Mathlouthi"
date: today
format:
  revealjs:
    logo: /img/logo.png
    theme: default
    css: /files/lecture_notes/lecture8/new-style.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Understanding Data ‚Äì Sampling and Confidence Intervals ¬© 2025"
    transition: slide
    background-transition: fade
    incremental: false
    smaller: true
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import scipy.stats as stats
from scipy.stats import norm, t
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Custom color palette
colors = {
    'primary': '#3b82f6',
    'secondary': '#f59e0b', 
    'success': '#10b981',
    'danger': '#ef4444',
    'info': '#8b5cf6',
    'light': '#f8fafc',
    'dark': '#1f2937'
}
```

# Welcome to Lecture 9 {.title-slide .center}

**From Samples to Populations: Understanding Uncertainty**

*"In statistics, we make educated guesses about the whole by looking at a part"*

---

## What We'll Learn Today üéØ {.smaller}

:::: {.columns}

::: {.column width="40%" }
**Big Ideas:**

- How **sample means** behave (they're surprisingly predictable!)
  
- Why we use **intervals** instead of *single* numbers
  
- How confident we can be in our estimates
  
- Planning studies for the right precision
:::

::: {.column width="40%"}
**Skills You'll Gain:**

- Build **confidence intervals** step-by-step
  
- Interpret what confidence really means
  
- Choose appropriate **sample sizes**
  
- Avoid common mistakes in interpretation
:::

::::

---

## The Big Picture: From Sample to Population {.smaller}

:::: {.columns .smaller}

::: {.column width="60%" }
Think of this like **trying to understand a huge library by reading just a few books**

```{python}
#| fig-width: 12
#| fig-height: 6

fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Population (What we want to know)", "Sample (What we actually see)"),
    specs=[[{"type": "scatter"}, {"type": "scatter"}]]
)

# Population - large scattered points
np.random.seed(42)
pop_x = np.random.normal(50, 15, 1000)
pop_y = np.random.normal(50, 15, 1000)

fig.add_trace(
    go.Scatter(
        x=pop_x, y=pop_y,
        mode='markers',
        marker=dict(size=3, color=colors['primary'], opacity=0.6),
        name='Population (Œº = 50)',
        showlegend=False
    ),
    row=1, col=1
)

# Sample - highlighted subset
sample_indices = np.random.choice(1000, 50, replace=False)
sample_x = pop_x[sample_indices]
sample_y = pop_y[sample_indices]

fig.add_trace(
    go.Scatter(
        x=sample_x, y=sample_y,
        mode='markers',
        marker=dict(size=8, color=colors['danger'], opacity=0.8),
        name=f'Sample (xÃÑ = {np.mean(sample_x):.1f})',
        showlegend=False
    ),
    row=1, col=2
)

# Add mean lines
fig.add_hline(y=50, line_dash="dash", line_color=colors['primary'], 
              annotation_text="Population Mean (Œº) = 50", row=1, col=1)
fig.add_hline(y=np.mean(sample_x), line_dash="dash", line_color=colors['danger'], 
              annotation_text=f"Sample Mean (xÃÑ) = {np.mean(sample_x):.1f}", row=1, col=2)

fig.update_layout(
    #title="The Challenge: Estimate the Unknown Population from a Known Sample",
    height=400,
    showlegend=False
)

fig.show()
```

:::



::: {.column width="40%" }
**Key Terms Made Simple:**

- **Population ($\mu$)**: Everyone we care about (like all students at UCSB)
  
- **Sample ($\bar x$)**: The people we actually measured (like 100 students we surveyed)
  
- **Parameter**: The true answer we want (population mean)
  
- **Statistic**: Our best guess (sample mean)
:::
:::
---

## Why Point Estimates Aren't Enough

**Imagine asking:** "What's the average height of UCSB students?"

```{python}
#| fig-width: 10
#| fig-height: 4

# Simulate multiple samples
np.random.seed(123)
true_mean = 68  # inches
true_std = 4

sample_means = []
for i in range(20):
    sample = np.random.normal(true_mean, true_std, 30)
    sample_means.append(np.mean(sample))

fig = go.Figure()

# Add true population mean
fig.add_hline(y=true_mean, line_dash="solid", line_color=colors['success'], 
              line_width=3, annotation_text="True Population Mean = 68 inches")

# Add sample means
fig.add_trace(
    go.Scatter(
        x=list(range(1, 21)),
        y=sample_means,
        mode='markers+lines',
        marker=dict(size=10, color=colors['danger']),
        line=dict(color=colors['danger'], width=2),
        name='Sample Means'
    )
)

fig.update_layout(
    # 1. Main title with extra bottom‚Äêpad and repositioning
    title={
        "text": "20 Different Samples Give 20 Different Answers!",
        "x": 0.5,               # center horizontally
        "xanchor": "center",
        "yanchor": "top",
        "y": 0.98,              # push it down a bit
        "pad": {"t": 10, "b": 20}  # top and bottom padding
    },
    # 2. Make more room above the title
    margin={"t": 100, "b": 40, "l": 40, "r": 40},

    # (your existing axis titles etc.)
    xaxis_title="Sample Number",
    yaxis_title="Sample Mean Height (inches)"
)

# 3. Axis‚Äêtitle standoff (distance from the axis)
fig.update_xaxes(title_standoff=25)
fig.update_yaxes(title_standoff=25)

fig.show()
```

**The Problem:** Each sample gives a slightly different answer! 

**The Solution:** Use **confidence intervals** to show the range of reasonable values.

---

## Understanding Sampling Distributions {.smaller}

**Simple Analogy:** Think of sampling distributions like **"What if we repeated our study 1000 times?"**

```{python}
#| fig-width: 12
#| fig-height: 7

# Create sampling distribution demonstration
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        "Population Distribution", "One Sample (n=30)",
        "Many Sample Means", "Sampling Distribution of xÃÑ"
    ),
    specs=[[{"type": "histogram"}, {"type": "histogram"}],
           [{"type": "histogram"}, {"type": "histogram"}]]
)

# Population
np.random.seed(42)
population = np.random.exponential(2, 10000)  # Skewed population

fig.add_trace(
    go.Histogram(x=population, nbinsx=50, name="Population", 
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# One sample
one_sample = np.random.choice(population, 30)
fig.add_trace(
    go.Histogram(x=one_sample, nbinsx=15, name="One Sample", 
                marker_color=colors['danger'], opacity=0.7),
    row=1, col=2
)

# Many samples visualization
sample_means = []
for i in range(1000):
    sample = np.random.choice(population, 30)
    sample_means.append(np.mean(sample))

# Show first 100 sample means
fig.add_trace(
    go.Scatter(
        x=list(range(1, 101)),
        y=sample_means[:100],
        mode='markers',
        marker=dict(size=6, color=colors['secondary']),
        name="First 100 Sample Means"
    ),
    row=2, col=1
)

# Sampling distribution
fig.add_trace(
    go.Histogram(x=sample_means, nbinsx=40, name="Sampling Distribution", 
                marker_color=colors['success'], opacity=0.7),
    row=2, col=2
)

fig.update_layout(height=600, showlegend=False)
fig.show()
```

**Key Insights:**
1. **Population** can be any shape (skewed, normal, weird)
2. **One sample** might not look like the population
3. **Sample means** vary from sample to sample
4. **Many sample means** form a beautiful normal distribution!

---

## The Central Limit Theorem (CLT) üéØ

**The Magic Rule:** No matter what shape your population is, sample means will be approximately normal!

```{python}
#| fig-width: 12
#| fig-height: 8

# Interactive CLT demonstration
def create_clt_demo(population_type="uniform", sample_size=30):
    np.random.seed(42)
    
    # Generate different population shapes
    if population_type == "uniform":
        population = np.random.uniform(0, 10, 5000)
        pop_title = "Uniform Population"
    elif population_type == "exponential":
        population = np.random.exponential(2, 5000)
        pop_title = "Right-Skewed Population"
    elif population_type == "bimodal":
        pop1 = np.random.normal(3, 1, 2500)
        pop2 = np.random.normal(7, 1, 2500)
        population = np.concatenate([pop1, pop2])
        pop_title = "Bimodal Population"
    
    # Generate sampling distribution
    sample_means = []
    for i in range(1000):
        sample = np.random.choice(population, sample_size)
        sample_means.append(np.mean(sample))
    
    return population, sample_means, pop_title

# Create demo for different sample sizes
fig = make_subplots(
    rows=2, cols=3,
    subplot_titles=(
        "Population", "n=5 Sample Means", "n=30 Sample Means",
        "Population", "n=10 Sample Means", "n=50 Sample Means"
    )
)

# Uniform population examples
pop_uniform, means_5, _ = create_clt_demo("uniform", 5)
pop_uniform, means_30, _ = create_clt_demo("uniform", 30)

fig.add_trace(go.Histogram(x=pop_uniform, nbinsx=50, name="Population", 
                          marker_color=colors['primary'], opacity=0.7), row=1, col=1)
fig.add_trace(go.Histogram(x=means_5, nbinsx=30, name="n=5", 
                          marker_color=colors['danger'], opacity=0.7), row=1, col=2)
fig.add_trace(go.Histogram(x=means_30, nbinsx=30, name="n=30", 
                          marker_color=colors['success'], opacity=0.7), row=1, col=3)

# Exponential population examples
pop_exp, means_10, _ = create_clt_demo("exponential", 10)
pop_exp, means_50, _ = create_clt_demo("exponential", 50)

fig.add_trace(go.Histogram(x=pop_exp, nbinsx=50, name="Population", 
                          marker_color=colors['primary'], opacity=0.7), row=2, col=1)
fig.add_trace(go.Histogram(x=means_10, nbinsx=30, name="n=10", 
                          marker_color=colors['secondary'], opacity=0.7), row=2, col=2)
fig.add_trace(go.Histogram(x=means_50, nbinsx=30, name="n=50", 
                          marker_color=colors['info'], opacity=0.7), row=2, col=3)

fig.update_layout(height=600, showlegend=False, 
                 title="Central Limit Theorem: Larger Samples ‚Üí More Normal!")
fig.show()
```

**The Rule of Thumb:** With n ‚â• 30, sample means will be approximately normal, regardless of population shape!

---

## Standard Error: The Key to Everything {.smaller}

**Standard Error (SE)** tells us **how much sample means vary**

::::: {.columns}

:::: {.column width="60%"}

**Think of it like this:**
- **Standard Deviation (œÉ)**: How spread out individual people are
- **Standard Error (SE)**: How spread out sample averages are

**The Formula Broken Down:**
$$SE = \frac{\sigma}{\sqrt{n}}$$

**Where:**
- œÉ = population standard deviation (how varied people are)
- n = sample size (how many people we measured)
- ‚àön = square root of sample size (the magic that makes averages more precise)

::::

:::: {.column width="40%"}

```{python}
#| fig-width: 8
#| fig-height: 6

# Demonstrate effect of sample size on standard error
sample_sizes = np.array([5, 10, 20, 30, 50, 100, 200])
sigma = 10
standard_errors = sigma / np.sqrt(sample_sizes)

fig = go.Figure()

fig.add_trace(
    go.Scatter(
        x=sample_sizes,
        y=standard_errors,
        mode='markers+lines',
        marker=dict(size=12, color=colors['primary']),
        line=dict(color=colors['primary'], width=3),
        name='Standard Error'
    )
)

fig.update_layout(
    title="Bigger Samples = Smaller Standard Error",
    xaxis_title="Sample Size (n)",
    yaxis_title="Standard Error",
    height=400,
    showlegend=False
)

fig.add_annotation(
    x=100, y=3,
    text="The bigger the sample,<br>the more precise our estimate!",
    showarrow=True,
    arrowhead=2,
    arrowcolor=colors['danger'],
    font=dict(size=12, color=colors['danger'])
)

fig.show()
```

::::

:::::

**Key Insight:** Doubling your sample size doesn't halve the error - you need 4 times the sample for half the error!

---

## Confidence Intervals: The Intuitive Idea

**Imagine you're trying to guess someone's height just by looking at their shadow...**

```{python}
#| fig-width: 12
#| fig-height: 6

# Create confidence interval visualization
np.random.seed(42)
true_mean = 68
sample_mean = 67.2
se = 1.5

# Create range of values
x = np.linspace(60, 76, 1000)
y = norm.pdf(x, sample_mean, se)

fig = go.Figure()

# Add the normal curve
fig.add_trace(
    go.Scatter(x=x, y=y, mode='lines', name='Sampling Distribution',
              line=dict(color=colors['primary'], width=3))
)

# Add confidence interval area (95%)
ci_lower = sample_mean - 1.96 * se
ci_upper = sample_mean + 1.96 * se
x_fill = x[(x >= ci_lower) & (x <= ci_upper)]
y_fill = y[(x >= ci_lower) & (x <= ci_upper)]

fig.add_trace(
    go.Scatter(x=x_fill, y=y_fill, fill='tonexty', mode='none',
              fillcolor=f'rgba(59, 130, 246, 0.3)', name='95% Confidence Interval')
)

# Add vertical lines
fig.add_vline(x=sample_mean, line_dash="solid", line_color=colors['danger'], 
              annotation_text=f"Sample Mean = {sample_mean}")
fig.add_vline(x=true_mean, line_dash="dash", line_color=colors['success'], 
              annotation_text=f"True Mean = {true_mean}")
fig.add_vline(x=ci_lower, line_dash="dot", line_color=colors['secondary'])
fig.add_vline(x=ci_upper, line_dash="dot", line_color=colors['secondary'])

fig.update_layout(
    title="95% Confidence Interval: We're 95% Sure the True Mean is in This Range",
    xaxis_title="Height (inches)",
    yaxis_title="Probability Density",
    height=400,
    showlegend=False
)

fig.add_annotation(
    x=ci_lower, y=0.05,
    text=f"{ci_lower:.1f}",
    font=dict(size=12, color=colors['secondary']),
    showarrow=False
)

fig.add_annotation(
    x=ci_upper, y=0.05,
    text=f"{ci_upper:.1f}",
    font=dict(size=12, color=colors['secondary']),
    showarrow=False
)

fig.show()
```

**Simple Interpretation:** "We're 95% confident the true average height is between 64.3 and 70.1 inches"

---

## Building Confidence Intervals Step-by-Step

### For Population Means (Most Common Case)

**When we DON'T know the population standard deviation (œÉ):**

```{python}
#| fig-width: 12
#| fig-height: 4

# Step-by-step CI construction
fig = make_subplots(
    rows=1, cols=4,
    subplot_titles=("1. Start with Sample", "2. Find Standard Error", 
                   "3. Get Critical Value", "4. Build Interval")
)

# Step 1: Sample data
np.random.seed(42)
sample_data = np.random.normal(68, 4, 25)
x_bar = np.mean(sample_data)
s = np.std(sample_data, ddof=1)
n = len(sample_data)

fig.add_trace(
    go.Histogram(x=sample_data, nbinsx=10, name="Sample Data",
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# Step 2: Standard Error calculation
se = s / np.sqrt(n)
fig.add_trace(
    go.Bar(x=['Standard Error'], y=[se], 
           marker_color=colors['secondary'], name="SE"),
    row=1, col=2
)

# Step 3: Critical value
df = n - 1
t_critical = t.ppf(0.975, df)  # 95% confidence
fig.add_trace(
    go.Bar(x=['t* (95%)'], y=[t_critical], 
           marker_color=colors['danger'], name="Critical Value"),
    row=1, col=3
)

# Step 4: Confidence interval
margin_error = t_critical * se
ci_lower = x_bar - margin_error
ci_upper = x_bar + margin_error

fig.add_trace(
    go.Scatter(x=[ci_lower, x_bar, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[15, 20, 15], color=colors['success']),
               line=dict(color=colors['success'], width=5),
               name="Confidence Interval"),
    row=1, col=4
)

fig.update_layout(height=300, showlegend=False)
fig.show()
```

**The Formula:** $\bar{x} \pm t^* \cdot \frac{s}{\sqrt{n}}$

**Breaking it down:**
- $\bar{x}$ = our sample average (the center of our guess)
- $t^*$ = critical value (how many standard errors to go out)
- $\frac{s}{\sqrt{n}}$ = standard error (our uncertainty measure)

---

## The t-Distribution: When œÉ is Unknown {.smaller}

**Why not use the normal distribution?** Because when we estimate œÉ with s, we add extra uncertainty!

```{python}
#| fig-width: 12
#| fig-height: 6

# Compare normal vs t-distributions
x = np.linspace(-4, 4, 1000)

fig = go.Figure()

# Standard normal
y_normal = norm.pdf(x, 0, 1)
fig.add_trace(
    go.Scatter(x=x, y=y_normal, mode='lines', name='Normal (œÉ known)',
              line=dict(color=colors['primary'], width=3))
)

# t-distributions with different df
for df, color, name in [(5, colors['danger'], 't (df=5)'), 
                       (10, colors['secondary'], 't (df=10)'),
                       (30, colors['success'], 't (df=30)')]:
    y_t = t.pdf(x, df)
    fig.add_trace(
        go.Scatter(x=x, y=y_t, mode='lines', name=name,
                  line=dict(color=color, width=2, dash='dash'))
    )

fig.update_layout(
    title="t-Distribution vs Normal: More Uncertainty with Smaller Samples",
    xaxis_title="Value",
    yaxis_title="Probability Density",
    height=400
)

fig.add_annotation(
    x=0, y=0.3,
    text="As sample size increases,<br>t approaches normal",
    showarrow=True,
    arrowhead=2,
    arrowcolor=colors['info'],
    font=dict(size=12, color=colors['info'])
)

fig.show()
```

**Key Points:**
- Small samples (n < 30): Use t-distribution
- Large samples (n ‚â• 30): t ‚âà normal
- Degrees of freedom = n - 1

---

## Confidence Intervals for Proportions

**For Yes/No questions like:** "What percentage of students prefer online classes?"

```{python}
#| fig-width: 12
#| fig-height: 5

# Proportion confidence interval demonstration
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Sample Results", "Confidence Interval")
)

# Sample data
n = 200
x = 120  # successes
p_hat = x / n

# Create sample visualization
categories = ['Prefer Online', 'Prefer In-Person']
counts = [x, n - x]
colors_bar = [colors['success'], colors['danger']]

fig.add_trace(
    go.Bar(x=categories, y=counts, marker_color=colors_bar, name="Sample Counts"),
    row=1, col=1
)

# Confidence interval
se_prop = np.sqrt(p_hat * (1 - p_hat) / n)
z_critical = 1.96  # 95% confidence
margin_error = z_critical * se_prop
ci_lower = p_hat - margin_error
ci_upper = p_hat + margin_error

# CI visualization
fig.add_trace(
    go.Scatter(x=[ci_lower, p_hat, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[15, 20, 15], color=colors['primary']),
               line=dict(color=colors['primary'], width=5),
               name="95% CI"),
    row=1, col=2
)

fig.update_xaxes(title_text="Response", row=1, col=1)
fig.update_xaxes(title_text="Proportion", range=[0.5, 0.7], row=1, col=2)
fig.update_yaxes(title_text="Count", row=1, col=1)
fig.update_yaxes(title_text="", showticklabels=False, row=1, col=2)

fig.update_layout(height=400, showlegend=False)
fig.show()
```

**The Formula:** $\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$

**Results:** Sample: 60% prefer online (120/200)  
**95% CI:** (53.2%, 66.8%) - We're 95% confident the true percentage is in this range.

---

## What Does "95% Confident" Really Mean? ü§î

**The Biggest Misconception:** "There's a 95% chance the true mean is in our interval"

**Actually:** "If we repeated this study 100 times, about 95 of our intervals would contain the true mean"

```{python}
#| fig-width: 12
#| fig-height: 8

# Simulate 100 confidence intervals
np.random.seed(42)
true_mean = 50
true_std = 10
n = 25
num_intervals = 50

intervals = []
captures = []

for i in range(num_intervals):
    # Generate sample
    sample = np.random.normal(true_mean, true_std, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s / np.sqrt(n)
    
    # 95% CI
    t_crit = t.ppf(0.975, n-1)
    margin = t_crit * se
    ci_lower = x_bar - margin
    ci_upper = x_bar + margin
    
    intervals.append((ci_lower, ci_upper, x_bar))
    captures.append(ci_lower <= true_mean <= ci_upper)

# Create visualization
fig = go.Figure()

# Add intervals
for i, ((lower, upper, mean), capture) in enumerate(zip(intervals, captures)):
    color = colors['success'] if capture else colors['danger']
    # Confidence interval line
    fig.add_trace(
        go.Scatter(x=[lower, upper], y=[i, i], mode='lines',
                  line=dict(color=color, width=3),
                  showlegend=False)
    )
    # Sample mean point
    fig.add_trace(
        go.Scatter(x=[mean], y=[i], mode='markers',
                  marker=dict(size=6, color=color),
                  showlegend=False)
    )

# Add true mean line
fig.add_vline(x=true_mean, line_dash="solid", line_color=colors['primary'], 
              line_width=4, annotation_text=f"True Mean = {true_mean}")

# Count captures
capture_rate = np.mean(captures) * 100

fig.update_layout(
    title=f"50 Different 95% Confidence Intervals: {capture_rate:.0f}% Capture the True Mean",
    xaxis_title="Value",
    yaxis_title="Study Number",
    height=600
)

fig.add_annotation(
    x=45, y=40,
    text=f"Green = Contains true mean<br>Red = Misses true mean<br>Rate = {capture_rate:.0f}%",
    showarrow=False,
    bgcolor="white",
    bordercolor="black",
    borderwidth=1,
    font=dict(size=12)
)

fig.show()
```

**Remember:** The interval either contains the true value or it doesn't - there's no probability involved for a single interval!

---

## Sample Size Planning: Getting the Precision You Want {.smaller}

**The Question:** "How many people do we need to survey?"

```{python}
#| fig-width: 12
#| fig-height: 6

# Sample size vs margin of error
sigma = 10  # population standard deviation
confidence_levels = [90, 95, 99]
z_values = [1.645, 1.96, 2.576]

fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Sample Size vs Margin of Error", "Required Sample Size")
)

# Plot 1: Sample size vs margin of error for different confidence levels
sample_sizes = np.arange(10, 201, 10)

for conf, z_val, color in zip(confidence_levels, z_values, [colors['success'], colors['primary'], colors['danger']]):
    margins = z_val * sigma / np.sqrt(sample_sizes)
    fig.add_trace(
        go.Scatter(x=sample_sizes, y=margins, mode='lines+markers',
                  name=f'{conf}% Confidence',
                  line=dict(color=color, width=3)),
        row=1, col=1
    )

# Plot 2: Required sample size for different margins
desired_margins = np.arange(0.5, 5.1, 0.1)
for conf, z_val, color in zip(confidence_levels, z_values, [colors['success'], colors['primary'], colors['danger']]):
    required_n = (z_val * sigma / desired_margins) ** 2
    fig.add_trace(
        go.Scatter(x=desired_margins, y=required_n, mode='lines',
                  name=f'{conf}% Confidence',
                  line=dict(color=color, width=3),
                  showlegend=False),
        row=1, col=2
    )

fig.update_xaxes(title_text="Sample Size", row=1, col=1)
fig.update_xaxes(title_text="Desired Margin of Error", row=1, col=2)
fig.update_yaxes(title_text="Margin of Error", row=1, col=1)
fig.update_yaxes(title_text="Required Sample Size", row=1, col=2)

fig.update_layout(height=500, 
                 title="Planning Your Study: The Trade-offs")
fig.show()
```

**Key Formula for Means:** $n = \left(\frac{z^* \sigma}{ME}\right)^2$

**Trade-offs:**
- Want smaller margin of error? Need bigger sample
- Want higher confidence? Need bigger sample  
- Want to save money? Accept wider intervals

---

## Real Example: Student Sleep Study üò¥ {.smaller}

**Research Question:** How many hours do UCSB students sleep per night?

```{python}
#| fig-width: 12
#| fig-height: 8

# Simulate sleep study data
np.random.seed(123)
n = 50
sleep_data = np.random.normal(7.2, 1.5, n)  # Mean 7.2 hours, SD 1.5

# Calculate statistics
x_bar = np.mean(sleep_data)
s = np.std(sleep_data, ddof=1)
se = s / np.sqrt(n)

# 95% confidence interval
df = n - 1
t_crit = t.ppf(0.975, df)
margin = t_crit * se
ci_lower = x_bar - margin
ci_upper = x_bar + margin

# Create comprehensive visualization
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("Sample Data Distribution", "Step-by-Step Calculation",
                   "Confidence Interval Visualization", "Interpretation"),
    specs=[[{"type": "histogram"}, {"type": "table"}],
           [{"type": "scatter"}, {"type": "scatter"}]]
)

# Sample distribution
fig.add_trace(
    go.Histogram(x=sleep_data, nbinsx=15, name="Sleep Hours",
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# Calculation table
calc_data = [
    ["Sample Size (n)", f"{n}"],
    ["Sample Mean (xÃÑ)", f"{x_bar:.2f} hours"],
    ["Sample SD (s)", f"{s:.2f} hours"],
    ["Standard Error", f"{se:.3f} hours"],
    ["t* (95%, df={})".format(df), f"{t_crit:.3f}"],
    ["Margin of Error", f"{margin:.3f} hours"],
    ["95% CI Lower", f"{ci_lower:.2f} hours"],
    ["95% CI Upper", f"{ci_upper:.2f} hours"]
]

fig.add_trace(
    go.Table(
        header=dict(values=["Statistic", "Value"],
                   fill_color=colors['light'],
                   font=dict(size=12)),
        cells=dict(values=list(zip(*calc_data)),
                  fill_color='white',
                  font=dict(size=11))
    ),
    row=1, col=2
)

# CI visualization
fig.add_trace(
    go.Scatter(x=[ci_lower, x_bar, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[20, 25, 20], 
                          color=[colors['danger'], colors['success'], colors['danger']]),
               line=dict(color=colors['primary'], width=8),
               name="95% Confidence Interval"),
    row=2, col=1
)

# Add text annotations
fig.add_annotation(x=ci_lower, y=1.1, text=f"{ci_lower:.2f}", 
                  showarrow=False, row=2, col=1)
fig.add_annotation(x=x_bar, y=1.1, text=f"{x_bar:.2f}", 
                  showarrow=False, row=2, col=1)
fig.add_annotation(x=ci_upper, y=1.1, text=f"{ci_upper:.2f}", 
                  showarrow=False, row=2, col=1)

# Interpretation
interpretation_text = [
    f"We are 95% confident that the true average sleep time",
    f"for UCSB students is between {ci_lower:.2f} and {ci_upper:.2f} hours.",
    f"",
    f"This means if we repeated this study 100 times,",
    f"about 95 of our intervals would contain the true mean."
]

fig.add_trace(
    go.Scatter(x=[0.5], y=[0.5], mode='text',
              text="<br>".join(interpretation_text),
              textfont=dict(size=14),
              showlegend=False),
    row=2, col=2
)

fig.update_layout(height=700, showlegend=False,
                 title="Complete Analysis: UCSB Student Sleep Study")

# Update axes
fig.update_xaxes(title_text="Hours of Sleep", row=1, col=1)
fig.update_yaxes(title_text="Frequency", row=1, col=1)
fig.update_xaxes(title_text="Hours of Sleep", range=[6.5, 8], row=2, col=1)
fig.update_yaxes(title_text="", showticklabels=False, range=[0.8, 1.2], row=2, col=1)
fig.update_xaxes(showticklabels=False, row=2, col=2)
fig.update_yaxes(showticklabels=False, row=2, col=2)

fig.show()
```

**Bottom Line:** We're 95% confident that UCSB students sleep between 6.73 and 7.47 hours per night on average.

---

## Common Mistakes to Avoid ‚ö†Ô∏è

::::: {.columns}

:::: {.column width="50%"}

### ‚ùå Wrong Interpretations

**"95% of students sleep in this range"**
- NO! This is about the population mean, not individual students

**"There's a 95% chance Œº is in our interval"**
- NO! Œº is fixed; our interval varies

**"We can be 95% certain"**
- NO! Use "confident" not "certain"

::::

:::: {.column width="50%"}

### ‚úÖ Correct Approach

**"We are 95% confident the population mean is in this interval"**

**Key Reminders:**
- Check conditions before using formulas
- Use t-distribution when œÉ is unknown
- Larger samples give narrower intervals
- Higher confidence gives wider intervals

::::

:::::

```{python}
#| fig-width: 10
#| fig-height: 4

# Visualization of common mistakes
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("‚ùå Wrong: 95% of data in CI", "‚úÖ Right: 95% confident about mean")
)

# Wrong interpretation
np.random.seed(42)
sample_data = np.random.normal(50, 10, 100)
ci_lower, ci_upper = 47, 53

fig.add_trace(
    go.Histogram(x=sample_data, nbinsx=20, name="Individual Data Points",
                marker_color=colors['danger'], opacity=0.6),
    row=1, col=1
)

# Highlight data outside CI
outside_ci = sample_data[(sample_data < ci_lower) | (sample_data > ci_upper)]
fig.add_trace(
    go.Histogram(x=outside_ci, nbinsx=20, name="Outside CI",
                marker_color=colors['primary'], opacity=0.8),
    row=1, col=1
)

# Right interpretation - show sampling distribution
sample_means = [np.mean(np.random.normal(50, 10, 25)) for _ in range(1000)]
fig.add_trace(
    go.Histogram(x=sample_means, nbinsx=30, name="Sample Means",
                marker_color=colors['success'], opacity=0.7),
    row=1, col=2
)

fig.add_vline(x=ci_lower, line_dash="dash", line_color=colors['primary'], row=1, col=2)
fig.add_vline(x=ci_upper, line_dash="dash", line_color=colors['primary'], row=1, col=2)

fig.update_layout(height=400, showlegend=False)
fig.show()
```

---

## Practice Problems üìù

### Problem 1: Coffee Shop Revenue

A coffee shop owner samples 36 days and finds average daily revenue of $850 with standard deviation $120.

**Your turn:** Calculate a 90% confidence interval for the true average daily revenue.

::: {.callout-note collapse="true"}
## Solution

**Given:** n = 36, xÃÑ = $850, s = $120, 90% confidence

**Step 1:** Check conditions ‚úì (large sample)

**Step 2:** Find critical value  
df = 35, t* = 1.690 (90% confidence)

**Step 3:** Calculate standard error  
SE = 120/‚àö36 = 120/6 = $20

**Step 4:** Build interval  
CI = 850 ¬± 1.690 √ó 20 = 850 ¬± 33.8 = ($816.20, $883.80)

**Interpretation:** We are 90% confident the true average daily revenue is between $816.20 and $883.80.
:::

---

### Problem 2: Student Survey

In a survey of 400 students, 280 say they would recommend their major to a friend.

**Your turn:** 
1. Calculate the sample proportion
2. Build a 95% confidence interval  
3. Check if conditions are met

::: {.callout-note collapse="true"}
## Solution

**Step 1:** Sample proportion  
pÃÇ = 280/400 = 0.70 (70%)

**Step 2:** Check conditions  
npÃÇ = 400(0.70) = 280 ‚â• 10 ‚úì  
n(1-pÃÇ) = 400(0.30) = 120 ‚â• 10 ‚úì

**Step 3:** Build CI  
SE = ‚àö[0.70(0.30)/400] = ‚àö[0.000525] = 0.0229  
z* = 1.96 (95% confidence)  
CI = 0.70 ¬± 1.96(0.0229) = 0.70 ¬± 0.045 = (0.655, 0.745)

**Interpretation:** We are 95% confident that between 65.5% and 74.5% of all students would recommend their major.
:::

---

## Looking Ahead: Hypothesis Testing üîÆ

**Next week we'll learn:**
- How to test specific claims about populations
- When to reject or fail to reject hypotheses  
- The connection between confidence intervals and hypothesis tests

```{python}
#| fig-width: 10
#| fig-height: 4

# Preview of hypothesis testing
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Confidence Intervals", "Hypothesis Testing")
)

# CI approach
x = np.linspace(45, 55, 1000)
y = norm.pdf(x, 50, 2)

fig.add_trace(
    go.Scatter(x=x, y=y, mode='lines', name='Sampling Distribution',
              line=dict(color=colors['primary'], width=3)),
    row=1, col=1
)

# Add CI
ci_x = x[(x >= 46.08) & (x <= 53.92)]
ci_y = y[(x >= 46.08) & (x <= 53.92)]
fig.add_trace(
    go.Scatter(x=ci_x, y=ci_y, fill='tonexty', mode='none',
              fillcolor=f'rgba(59, 130, 246, 0.3)', name='95% CI'),
    row=1, col=1
)

# Hypothesis testing approach
fig.add_trace(
    go.Scatter(x=x, y=y, mode='lines', name='Sampling Distribution',
              line=dict(color=colors['danger'], width=3)),
    row=1, col=2
)

# Add rejection regions
reject_left = x[x <= 46.08]
reject_right = x[x >= 53.92]
fig.add_trace(
    go.Scatter(x=reject_left, y=norm.pdf(reject_left, 50, 2), 
              fill='tonexty', mode='none',
              fillcolor=f'rgba(239, 68, 68, 0.3)', name='Reject H‚ÇÄ'),
    row=1, col=2
)
fig.add_trace(
    go.Scatter(x=reject_right, y=norm.pdf(reject_right, 50, 2), 
              fill='tonexty', mode='none',
              fillcolor=f'rgba(239, 68, 68, 0.3)', name='Reject H‚ÇÄ'),
    row=1, col=2
)

fig.update_layout(height=400, showlegend=False)
fig.show()
```

---

## Key Takeaways üéØ

::::: {.columns}

:::: {.column width="50%"}

**Big Ideas:**
1. **Samples vary** - confidence intervals capture this uncertainty
2. **Larger samples** give more precise estimates  
3. **Higher confidence** means wider intervals
4. **The CLT** makes normal-based inference possible

::::

:::: {.column width="50%"}

**Practical Skills:**
- Build CIs for means and proportions
- Interpret confidence correctly
- Plan sample sizes for desired precision
- Avoid common interpretation mistakes

::::

:::::

**Remember:** Statistics is about making informed decisions under uncertainty. Confidence intervals help us quantify that uncertainty!

---

## Questions? ü§î

**Office Hours:** Thursday 11AM (link on Canvas)  
**Email:** nmathlouthi@ucsb.edu  
**Next Class:** Hypothesis Testing and p-values

*"The goal is not to eliminate uncertainty, but to understand and work with it"*