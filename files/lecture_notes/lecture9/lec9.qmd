---
title: "PSTAT 5A: Sampling and Confidence Intervals"
subtitle: "Lecture 10 - Making Sense of Uncertainty"
author: "Narjes Mathlouthi"
date: today
format:
  revealjs:
    logo: /img/logo.png
    theme: default
    css: /files/lecture_notes/lecture8/new-style.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Understanding Data – Sampling and Confidence Intervals © 2025"
    transition: slide
    background-transition: fade
    incremental: false
    smaller: true
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import scipy.stats as stats
from scipy.stats import norm, t
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("husl")

# Custom color palette
colors = {
    'primary': '#3b82f6',
    'secondary': '#f59e0b', 
    'success': '#10b981',
    'danger': '#ef4444',
    'info': '#8b5cf6',
    'light': '#f8fafc',
    'dark': '#1f2937'
}
```

# Welcome to Lecture 10 {.center}

**From Samples to Populations: Understanding Uncertainty**

*"In statistics, we make educated guesses about the whole by looking at a part"*

---

## What We'll Learn Today 🎯 {.smaller}

:::: {.columns}
::: {.column}
**Big Ideas:**

- How **sample means** behave (they're surprisingly predictable!)
- Why we use **intervals** instead of *single* numbers
- How confident we can be in our estimates
- Planning studies for the right precision
:::
::: {.column}
**Skills You'll Gain:**

- Build **confidence intervals** step-by-step
- Interpret what confidence really means
- Choose appropriate **sample sizes**
- Avoid common mistakes in interpretation
:::
::::

---

## <small> <b> From Observation to Experimentation </b></small>

```{python}

#| fig-width: 14
#| fig-height: 5

# Create experiment design visualization
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("Observational Study", "Controlled Experiment", 
                   "Random Experiment Components", "With vs Without Randomization"),
    specs=[[{"type": "scatter"}, {"type": "scatter"}],
           [{"type": "table"}, {"type": "scatter"}]],
    vertical_spacing=0.3,
    horizontal_spacing=0.08
)

# Observational Study
np.random.seed(42)
study_groups = ['Low Exercise', 'Medium Exercise', 'High Exercise']
health_scores = [65, 75, 85]

for i, (group, score) in enumerate(zip(study_groups, health_scores)):
    data = np.random.normal(score, 7, 25)
    fig.add_trace(
        go.Scatter(
            x=[i] * 25 + np.random.normal(0, 0.08, 25),
            y=data,
            mode='markers',
            marker=dict(size=6, color=colors['primary'], opacity=0.6),
            showlegend=False
        ),
        row=1, col=1
    )

# Controlled Experiment
treatment_groups = ['Control', 'Treatment A', 'Treatment B']
treatment_effects = [50, 65, 72]
for i, (group, effect) in enumerate(zip(treatment_groups, treatment_effects)):
    data = np.random.normal(effect, 5, 20)
    fig.add_trace(
        go.Scatter(
            x=[i] * 20 + np.random.normal(0, 0.08, 20),
            y=data,
            mode='markers',
            marker=dict(size=6, color=colors['success'], opacity=0.7),
            showlegend=False
        ),
        row=1, col=2
    )

# Fixed Table - Random Experiment Components
fig.add_trace(
    go.Table(
        header=dict(
            values=["Component", "Definition", "Example"],
            fill_color='#f8fafc',
            font=dict(size=10, color='#1e293b'),
            align="left"
        ),
        cells=dict(
            values=[
                ["Sample Space (S)", "Event (E)", "Probability P(E)", "Randomization", "Replication"],
                ["All possible outcomes", "Subset of sample space", "Likelihood of event", "Random assignment", "Repeated trials"],
                ["Coin: {H, T}", "Getting heads: {H}", "P(Heads) = 0.5", "Random treatment assignment", "Flip coin 100 times"]
            ],
            fill_color='white',
            font=dict(size=9),
            align="left",
            height=22
        )
    ),
    row=2, col=1
)

# Better visualization: Randomization effect demonstration
np.random.seed(123)
# Without randomization - biased groups
without_rand_control = np.random.normal(45, 8, 30)  # Sicker patients
without_rand_treatment = np.random.normal(75, 6, 30)  # Healthier patients

# With randomization - balanced groups  
with_rand_control = np.random.normal(60, 7, 30)
with_rand_treatment = np.random.normal(70, 7, 30)

# Plot without randomization
fig.add_trace(
    go.Scatter(
        x=[-0.2] * 30 + np.random.normal(0, 0.05, 30),
        y=without_rand_control,
        mode='markers',
        marker=dict(size=5, color=colors['danger'], opacity=0.6),
        name='Without Randomization',
        showlegend=False
    ),
    row=2, col=2
)

fig.add_trace(
    go.Scatter(
        x=[0.2] * 30 + np.random.normal(0, 0.05, 30),
        y=without_rand_treatment,
        mode='markers',
        marker=dict(size=5, color=colors['danger'], opacity=0.6),
        showlegend=False
    ),
    row=2, col=2
)

# Plot with randomization
fig.add_trace(
    go.Scatter(
        x=[0.8] * 30 + np.random.normal(0, 0.05, 30),
        y=with_rand_control,
        mode='markers',
        marker=dict(size=5, color=colors['success'], opacity=0.6),
        showlegend=False
    ),
    row=2, col=2
)

fig.add_trace(
    go.Scatter(
        x=[1.2] * 30 + np.random.normal(0, 0.05, 30),
        y=with_rand_treatment,
        mode='markers',
        marker=dict(size=5, color=colors['success'], opacity=0.6),
        showlegend=False
    ),
    row=2, col=2
)

# Update layout
fig.update_layout(
    height=500,
    showlegend=False,
    margin=dict(t=40, b=40, l=40, r=40)
)

# Update axes
fig.update_xaxes(title_text="Exercise Level", tickvals=[0,1,2], 
                ticktext=['Low', 'Medium', 'High'], row=1, col=1)
fig.update_yaxes(title_text="Health Score", row=1, col=1)

fig.update_xaxes(title_text="Treatment Group", tickvals=[0,1,2], 
                ticktext=['Control', 'Treatment A', 'Treatment B'], row=1, col=2)
fig.update_yaxes(title_text="Outcome", row=1, col=2)

fig.update_xaxes(title_text="Randomization Approach", 
                tickvals=[0, 1], 
                ticktext=['Without<br>Randomization', 'With<br>Randomization'], 
                row=2, col=2)
fig.update_yaxes(title_text="Health Outcome", row=2, col=2)

fig.show()

```

## <small> <b>The Big Picture: From Sample to Population </small> </b>

:::: {.columns .smaller}
::: {.column}
Think of this like **trying to understand a huge library by reading just a few books**

```{python}
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Example color palette
#colors = {'primary': '#636EFA', 'danger': '#EF553B'}

# Create figure with two subplots
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Population (What we want to know)", "Sample (What we actually see)"),
    specs=[[{"type": "scatter"}, {"type": "scatter"}]]
)

# Population - large scattered points
np.random.seed(42)
pop_x = np.random.normal(50, 15, 1000)
pop_y = np.random.normal(50, 15, 1000)

fig.add_trace(
    go.Scatter(
        x=pop_x, y=pop_y,
        mode='markers',
        marker=dict(size=3, color=colors['primary'], opacity=0.6),
        showlegend=False
    ),
    row=1, col=1
)

# Sample - highlighted subset
sample_indices = np.random.choice(1000, 50, replace=False)
sample_x = pop_x[sample_indices]
sample_y = pop_y[sample_indices]

fig.add_trace(
    go.Scatter(
        x=sample_x, y=sample_y,
        mode='markers',
        marker=dict(size=5, color=colors['danger'], opacity=0.9),
        showlegend=False
    ),
    row=1, col=2
)

# Add mean lines with styled annotations
fig.add_hline(
    y=50, line_dash="dash", line_color=colors['primary'], 
    annotation_text="Population Mean (μ) = 50",
    annotation_font_size=14,
    annotation_font_family="Arial Black",
    row=1, col=1
)
fig.add_hline(
    y=np.mean(sample_x), line_dash="dash", line_color=colors['danger'], 
    annotation_text=f"Sample Mean (x̄) = {np.mean(sample_x):.1f}",
    annotation_font_size=14,
    annotation_font_family="Arial Black",
    row=1, col=2
)

# Bold and enlarge subplot titles
for annotation in fig.layout.annotations:
    annotation.font.size = 12
    annotation.font.family = "Arial Black"

# Update overall layout for fit and font
fig.update_layout(
    font=dict(size=12, family="Arial Black"),
    margin=dict(l=30, r=30, t=30, b=30),
    width=500,
    height=350,
    autosize=True
)

fig.show()
```
:::
::: {.column}
**Key Terms Made Simple:**

- **Population ($\mu$)**: Everyone we care about (like all students at UCSB)
- **Sample ($\bar x$)**: The people we actually measured (like 100 students we surveyed)
- **Parameter**: The true answer we want (population mean)
- **Statistic**: Our best guess (sample mean)
:::
::::

---

## <small> <b>Why Point Estimates Aren't Enough </small> </b>

:::: {.columns}
::: {.column .smaller}
**Imagine asking:** "What's the average height of UCSB students?"

```{python}
#| fig-width: 10
#| fig-height: 5

# Simulate multiple samples
np.random.seed(123)
true_mean = 68  # inches
true_std = 4

sample_means = []
for i in range(20):
    sample = np.random.normal(true_mean, true_std, 30)
    sample_means.append(np.mean(sample))

fig = go.Figure()

# Add true population mean
fig.add_hline(y=true_mean, line_dash="solid", line_color=colors['success'], 
              line_width=3, annotation_text="True Population Mean = 68 inches")

# Add sample means
fig.add_trace(
    go.Scatter(
        x=list(range(1, 21)),
        y=sample_means,
        mode='markers+lines',
        marker=dict(size=10, color=colors['danger']),
        line=dict(color=colors['danger'], width=2),
        name='Sample Means'
    )
)

fig.update_layout(
    title="20 Different Samples Give 20 Different Answers!",
    xaxis_title="Sample Number",
    yaxis_title="Sample Mean Height (inches)",
    height=400,
    showlegend=False
)

fig.show()
```
:::
::: {.column .smaller}
**The Problem:** Each sample gives a slightly different answer! 

**The Solution:** Use **confidence intervals** to show the range of reasonable values.
:::
::::

---

## <small> <b>Sampling Distributions </small> </b>

:::: {.columns}
::: {.column width = "70%" .smaller}

Think of sampling distributions like 
**"What if we repeated our study 1000 times?"**

```{python}
#| fig-width: 13
#| fig-height: 5

# Create sampling distribution demonstration
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=(
        "Population Distribution", "One Sample (n=30)",
        "Many Sample Means", "Sampling Distribution of x̄"
    ),
    specs=[[{"type": "histogram"}, {"type": "histogram"}],
           [{"type": "histogram"}, {"type": "histogram"}]]
)

# Population
np.random.seed(42)
population = np.random.exponential(2, 10000)  # Skewed population

fig.add_trace(
    go.Histogram(x=population, nbinsx=50, name="Population", 
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# One sample
one_sample = np.random.choice(population, 30)
fig.add_trace(
    go.Histogram(x=one_sample, nbinsx=15, name="One Sample", 
                marker_color=colors['danger'], opacity=0.7),
    row=1, col=2
)

# Many samples visualization
sample_means = []
for i in range(1000):
    sample = np.random.choice(population, 30)
    sample_means.append(np.mean(sample))

# Show first 100 sample means
fig.add_trace(
    go.Scatter(
        x=list(range(1, 101)),
        y=sample_means[:100],
        mode='markers',
        marker=dict(size=6, color=colors['secondary']),
        name="First 100 Sample Means"
    ),
    row=2, col=1
)

# Sampling distribution
fig.add_trace(
    go.Histogram(x=sample_means, nbinsx=40, name="Sampling Distribution", 
                marker_color=colors['success'], opacity=0.7),
    row=2, col=2
)

fig.update_layout(height=400, width = 600, showlegend=False)
fig.show()
```
:::

::: {.column width="30%"}
**Key Insights:**

1. **Population** can be any shape (skewed, normal, weird)
   
2. **One sample** might not look like the population
   
3. **Sample means** vary from sample to sample
   
4. **Many sample means** form a beautiful normal distribution!
:::
::::

---

## <small> <b> The Central Limit Theorem (CLT) 🎯 </small> </b>

:::: {.columns}
::: {.column width = "70%"}

```{python}
#| fig-width: 12
#| fig-height: 8
#| fig-cap: "Central Limit Theorem: Larger Samples → More Normal!"

# Interactive CLT demonstration
def create_clt_demo(population_type="uniform", sample_size=30):
    np.random.seed(42)
    
    # Generate different population shapes
    if population_type == "uniform":
        population = np.random.uniform(0, 10, 5000)
        pop_title = "Uniform Population"
    elif population_type == "exponential":
        population = np.random.exponential(2, 5000)
        pop_title = "Right-Skewed Population"
    elif population_type == "bimodal":
        pop1 = np.random.normal(3, 1, 2500)
        pop2 = np.random.normal(7, 1, 2500)
        population = np.concatenate([pop1, pop2])
        pop_title = "Bimodal Population"
    
    # Generate sampling distribution
    sample_means = []
    for i in range(1000):
        sample = np.random.choice(population, sample_size)
        sample_means.append(np.mean(sample))
    
    return population, sample_means, pop_title

# Create demo for different sample sizes
fig = make_subplots(
    rows=2, cols=3,
    subplot_titles=(
        "Population", "n=5 Sample Means", "n=30 Sample Means",
        "Population", "n=10 Sample Means", "n=50 Sample Means"
    )
)

# Uniform population examples
pop_uniform, means_5, _ = create_clt_demo("uniform", 5)
pop_uniform, means_30, _ = create_clt_demo("uniform", 30)

fig.add_trace(go.Histogram(x=pop_uniform, nbinsx=50, name="Population", 
                          marker_color=colors['primary'], opacity=0.7), row=1, col=1)
fig.add_trace(go.Histogram(x=means_5, nbinsx=30, name="n=5", 
                          marker_color=colors['danger'], opacity=0.7), row=1, col=2)
fig.add_trace(go.Histogram(x=means_30, nbinsx=30, name="n=30", 
                          marker_color=colors['success'], opacity=0.7), row=1, col=3)

# Exponential population examples
pop_exp, means_10, _ = create_clt_demo("exponential", 10)
pop_exp, means_50, _ = create_clt_demo("exponential", 50)

fig.add_trace(go.Histogram(x=pop_exp, nbinsx=50, name="Population", 
                          marker_color=colors['primary'], opacity=0.7), row=2, col=1)
fig.add_trace(go.Histogram(x=means_10, nbinsx=30, name="n=10", 
                          marker_color=colors['secondary'], opacity=0.7), row=2, col=2)
fig.add_trace(go.Histogram(x=means_50, nbinsx=30, name="n=50", 
                          marker_color=colors['info'], opacity=0.7), row=2, col=3)

fig.update_layout(height=500, width=700, showlegend=False, 
                 #title="Central Limit Theorem: Larger Samples → More Normal!"
                 )
fig.show()
```
:::
::: {.column width = "30%" .smaller}
**The Magic Rule:** No matter what shape your population is, sample means will be approximately normal!

**The Rule of Thumb:** With **n ≥ 30**, sample means will be approximately normal, regardless of population shape!
:::
::::

---


## <small> <b>Standard Error: The Key to Everything </small> </b>

:::: {.columns}
::: {.column .smaller width = "40%"}
- **Standard Deviation (σ)**: How spread out individual people are
  
- **Standard Error (SE)**: How spread out sample averages are

**The Formula:**
$$SE = \frac{\sigma}{\sqrt{n}}$$

**Where:**

- $\sigma$ = population standard deviation (how varied people are)
  
- $n$ = sample size (how many people we measured)
  
- $\sqrt{n}$ = square root of sample size 
:::
::: {.column width = "60%"}
```{python}
#| fig-width: 8
#| fig-height: 6

# Demonstrate effect of sample size on standard error
sample_sizes = np.array([5, 10, 20, 30, 50, 100, 200])
sigma = 10
standard_errors = sigma / np.sqrt(sample_sizes)

fig = go.Figure()

fig.add_trace(
    go.Scatter(
        x=sample_sizes,
        y=standard_errors,
        mode='markers+lines',
        marker=dict(size=12, color=colors['primary']),
        line=dict(color=colors['primary'], width=3),
        name='Standard Error'
    )
)

fig.update_layout(
    title="Bigger Samples = Smaller Standard Error",
    xaxis_title="Sample Size (n)",
    yaxis_title="Standard Error",
    height=400,
    showlegend=False
)

fig.add_annotation(
    x=100, y=3,
    text="The bigger the sample,<br>the more precise our estimate!",
    showarrow=True,
    arrowhead=2,
    arrowcolor=colors['danger'],
    font=dict(size=12, color=colors['danger'])
)

fig.show()
```

:::{.callout-important}
Doubling your sample size doesn't halve the error - you need 4 times the sample for half the error!
:::

:::
:::


## <small> <b>Confidence Intervals: The Intuitive Idea </small> </b>

:::: {.columns}
::: {.column width = "70%"}
**Imagine you're trying to guess someone's height just by looking at their shadow...**

```{python}
import numpy as np
from scipy.stats import norm
import plotly.graph_objects as go



# Simulation parameters
np.random.seed(42)
true_mean    = 68
sample_mean  = 67.2
se           = 1.5
ci_lower     = sample_mean - 1.96 * se
ci_upper     = sample_mean + 1.96 * se

# X-axis values
x = np.linspace(60, 76, 1000)
# Sampling distribution PDF
y = norm.pdf(x, sample_mean, se)

# Build figure
fig = go.Figure()

# 1) Sampling distribution curve
fig.add_trace(go.Scatter(
    x=x, y=y,
    mode='lines',
    line=dict(color=colors['primary'], width=3),
    name='Sampling Dist.'
))

# 2) Shaded 95% CI region
ci_mask = (x >= ci_lower) & (x <= ci_upper)
fig.add_trace(go.Scatter(
    x=np.concatenate([x[ci_mask], [ci_upper, ci_lower]]),
    y=np.concatenate([y[ci_mask], [0, 0]]),
    fill='toself',
    fillcolor='rgba(59, 130, 246, 0.3)',
    line=dict(color='rgba(0,0,0,0)'),
    name='95% CI'
))

# 3) Vertical lines (no inline annotation_text)
for value, dash, color in [
    (ci_lower,    'dot',    colors['secondary']),
    (sample_mean, 'solid',  colors['danger']),
    (true_mean,   'dash',   colors['success']),
    (ci_upper,    'dot',    colors['secondary'])
]:
    fig.add_vline(x=value, line_dash=dash, line_color=color)

# 4) Annotations above plot for clear, bold labels
annotations = [
    dict(x=ci_lower,    y=1.05, yref='paper',
         text=f"<b>CI Lower = {ci_lower:.1f}</b>",
         showarrow=False, xanchor='center'),
    dict(x=sample_mean, y=1.10, yref='paper',
         text=f"<b>Sample Mean = {sample_mean:.1f}</b>",
         showarrow=False, xanchor='center'),
    dict(x=true_mean,   y=1.05, yref='paper',
         text=f"<b>True Mean = {true_mean}</b>",
         showarrow=False, xanchor='center'),
    dict(x=ci_upper,    y=1.10, yref='paper',
         text=f"<b>CI Upper = {ci_upper:.1f}</b>",
         showarrow=False, xanchor='center'),
]
fig.update_layout(annotations=annotations)

# 5) Layout tweaks
fig.update_layout(
    title=dict(text="95% CI: We're 95% Sure the True Mean is in This Range", x=0.3, xanchor='center'),
    xaxis=dict(title='Height (inches)', range=[60, 76], tick0=60, dtick=2),
    yaxis=dict(title='Probability Density'),
    width=1000,
    height=500,
    margin=dict(t=120, b=40, l=40, r=40),
    showlegend=True
)

fig.show()
```
:::
::: {.column width = "30%"}
**Simple Interpretation:** "We're 95% confident the true average height is between 64.3 and 70.1 inches"
:::
::::

---

<!-- ## <small> <b>Building Confidence Intervals Step-by-Step </small> </b>

:::: {.columns}
::: {.column}
### For Population Means (Most Common Case)

**When we DON'T know the population standard deviation (σ):**

```{python}
#| fig-width: 12
#| fig-height: 4

# Step-by-step CI construction
fig = make_subplots(
    rows=1, cols=4,
    subplot_titles=("1. Start with Sample", "2. Find Standard Error", 
                   "3. Get Critical Value", "4. Build Interval")
)

# Step 1: Sample data
np.random.seed(42)
sample_data = np.random.normal(68, 4, 25)
x_bar = np.mean(sample_data)
s = np.std(sample_data, ddof=1)
n = len(sample_data)

fig.add_trace(
    go.Histogram(x=sample_data, nbinsx=10, name="Sample Data",
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# Step 2: Standard Error calculation
se = s / np.sqrt(n)
fig.add_trace(
    go.Bar(x=['Standard Error'], y=[se], 
           marker_color=colors['secondary'], name="SE"),
    row=1, col=2
)

# Step 3: Critical value
df = n - 1
t_critical = t.ppf(0.975, df)  # 95% confidence
fig.add_trace(
    go.Bar(x=['t* (95%)'], y=[t_critical], 
           marker_color=colors['danger'], name="Critical Value"),
    row=1, col=3
)

# Step 4: Confidence interval
margin_error = t_critical * se
ci_lower = x_bar - margin_error
ci_upper = x_bar + margin_error

fig.add_trace(
    go.Scatter(x=[ci_lower, x_bar, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[15, 20, 15], color=colors['success']),
               line=dict(color=colors['success'], width=5),
               name="Confidence Interval"),
    row=1, col=4
)

fig.update_layout(height=300, showlegend=False)
fig.show()
```
:::
::: {.column}
**The Formula:** $\bar{x} \pm t^* \cdot \frac{s}{\sqrt{n}}$

**Breaking it down:**
- $\bar{x}$ = our sample average (the center of our guess)
- $t^*$ = critical value (how many standard errors to go out)
- $\frac{s}{\sqrt{n}}$ = standard error (our uncertainty measure)
:::
::::

---

## The t-Distribution: When σ is Unknown {.smaller}

:::: {.columns}
::: {.column}
**Why not use the normal distribution?** Because when we estimate σ with s, we add extra uncertainty!

```{python}
#| fig-width: 12
#| fig-height: 6

# Compare normal vs t-distributions
x = np.linspace(-4, 4, 1000)

fig = go.Figure()

# Standard normal
y_normal = norm.pdf(x, 0, 1)
fig.add_trace(
    go.Scatter(x=x, y=y_normal, mode='lines', name='Normal (σ known)',
              line=dict(color=colors['primary'], width=3))
)

# t-distributions with different df
for df, color, name in [(5, colors['danger'], 't (df=5)'), 
                       (10, colors['secondary'], 't (df=10)'),
                       (30, colors['success'], 't (df=30)')]:
    y_t = t.pdf(x, df)
    fig.add_trace(
        go.Scatter(x=x, y=y_t, mode='lines', name=name,
                  line=dict(color=color, width=2, dash='dash'))
    )

fig.update_layout(
    title="t-Distribution vs Normal: More Uncertainty with Smaller Samples",
    xaxis_title="Value",
    yaxis_title="Probability Density",
    height=400
)

fig.add_annotation(
    x=0, y=0.3,
    text="As sample size increases,<br>t approaches normal",
    showarrow=True,
    arrowhead=2,
    arrowcolor=colors['info'],
    font=dict(size=12, color=colors['info'])
)

fig.show()
```
:::
::: {.column}
**Key Points:**
- Small samples (n < 30): Use t-distribution
- Large samples (n ≥ 30): t ≈ normal
- Degrees of freedom = n - 1
:::
::::

---

## Confidence Intervals for Proportions {.smaller}

:::: {.columns}
::: {.column}
**For Yes/No questions like:** "What percentage of students prefer online classes?"

```{python}
#| fig-width: 12
#| fig-height: 5

# Proportion confidence interval demonstration
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Sample Results", "Confidence Interval")
)

# Sample data
n = 200
x = 120  # successes
p_hat = x / n

# Create sample visualization
categories = ['Prefer Online', 'Prefer In-Person']
counts = [x, n - x]
colors_bar = [colors['success'], colors['danger']]

fig.add_trace(
    go.Bar(x=categories, y=counts, marker_color=colors_bar, name="Sample Counts"),
    row=1, col=1
)

# Confidence interval
se_prop = np.sqrt(p_hat * (1 - p_hat) / n)
z_critical = 1.96  # 95% confidence
margin_error = z_critical * se_prop
ci_lower = p_hat - margin_error
ci_upper = p_hat + margin_error

# CI visualization
fig.add_trace(
    go.Scatter(x=[ci_lower, p_hat, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[15, 20, 15], color=colors['primary']),
               line=dict(color=colors['primary'], width=5),
               name="95% CI"),
    row=1, col=2
)

fig.update_xaxes(title_text="Response", row=1, col=1)
fig.update_xaxes(title_text="Proportion", range=[0.5, 0.7], row=1, col=2)
fig.update_yaxes(title_text="Count", row=1, col=1)
fig.update_yaxes(title_text="", showticklabels=False, row=1, col=2)

fig.update_layout(height=400, showlegend=False)
fig.show()
```
:::
::: {.column}
**The Formula:** $\hat{p} \pm z^* \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$

**Results:** Sample: 60% prefer online (120/200)  
**95% CI:** (53.2%, 66.8%) - We're 95% confident the true percentage is in this range.
:::
::::

---

## What Does "95% Confident" Really Mean? 🤔 {.smaller}

:::: {.columns}
::: {.column}
**The Biggest Misconception:** "There's a 95% chance the true mean is in our interval"

**Actually:** "If we repeated this study 100 times, about 95 of our intervals would contain the true mean"

```{python}
#| fig-width: 12
#| fig-height: 8

# Simulate 100 confidence intervals
np.random.seed(42)
true_mean = 50
true_std = 10
n = 25
num_intervals = 50

intervals = []
captures = []

for i in range(num_intervals):
    # Generate sample
    sample = np.random.normal(true_mean, true_std, n)
    x_bar = np.mean(sample)
    s = np.std(sample, ddof=1)
    se = s / np.sqrt(n)
    
    # 95% CI
    t_crit = t.ppf(0.975, n-1)
    margin = t_crit * se
    ci_lower = x_bar - margin
    ci_upper = x_bar + margin
    
    intervals.append((ci_lower, ci_upper, x_bar))
    captures.append(ci_lower <= true_mean <= ci_upper)

# Create visualization
fig = go.Figure()

# Add intervals
for i, ((lower, upper, mean), capture) in enumerate(zip(intervals, captures)):
    color = colors['success'] if capture else colors['danger']
    # Confidence interval line
    fig.add_trace(
        go.Scatter(x=[lower, upper], y=[i, i], mode='lines',
                  line=dict(color=color, width=3),
                  showlegend=False)
    )
    # Sample mean point
    fig.add_trace(
        go.Scatter(x=[mean], y=[i], mode='markers',
                  marker=dict(size=6, color=color),
                  showlegend=False)
    )

# Add true mean line
fig.add_vline(x=true_mean, line_dash="solid", line_color=colors['primary'], 
              line_width=4, annotation_text=f"True Mean = {true_mean}")

# Count captures
capture_rate = np.mean(captures) * 100

fig.update_layout(
    title=f"50 Different 95% Confidence Intervals: {capture_rate:.0f}% Capture the True Mean",
    xaxis_title="Value",
    yaxis_title="Study Number",
    height=600
)

fig.add_annotation(
    x=45, y=40,
    text=f"Green = Contains true mean<br>Red = Misses true mean<br>Rate = {capture_rate:.0f}%",
    showarrow=False,
    bgcolor="white",
    bordercolor="black",
    borderwidth=1,
    font=dict(size=12)
)

fig.show()
```
:::
::: {.column}
**Remember:** The interval either contains the true value or it doesn't - there's no probability involved for a single interval!
:::
::::

---

## Sample Size Planning: Getting the Precision You Want {.smaller}

:::: {.columns}
::: {.column}
**The Question:** "How many people do we need to survey?"

```{python}
#| fig-width: 12
#| fig-height: 6

# Sample size vs margin of error
sigma = 10  # population standard deviation
confidence_levels = [90, 95, 99]
z_values = [1.645, 1.96, 2.576]

fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Sample Size vs Margin of Error", "Required Sample Size")
)

# Plot 1: Sample size vs margin of error for different confidence levels
sample_sizes = np.arange(10, 201, 10)

for conf, z_val, color in zip(confidence_levels, z_values, [colors['success'], colors['primary'], colors['danger']]):
    margins = z_val * sigma / np.sqrt(sample_sizes)
    fig.add_trace(
        go.Scatter(x=sample_sizes, y=margins, mode='lines+markers',
                  name=f'{conf}% Confidence',
                  line=dict(color=color, width=3)),
        row=1, col=1
    )

# Plot 2: Required sample size for different margins
desired_margins = np.arange(0.5, 5.1, 0.1)
for conf, z_val, color in zip(confidence_levels, z_values, [colors['success'], colors['primary'], colors['danger']]):
    required_n = (z_val * sigma / desired_margins) ** 2
    fig.add_trace(
        go.Scatter(x=desired_margins, y=required_n, mode='lines',
                  name=f'{conf}% Confidence',
                  line=dict(color=color, width=3),
                  showlegend=False),
        row=1, col=2
    )

fig.update_xaxes(title_text="Sample Size", row=1, col=1)
fig.update_xaxes(title_text="Desired Margin of Error", row=1, col=2)
fig.update_yaxes(title_text="Margin of Error", row=1, col=1)
fig.update_yaxes(title_text="Required Sample Size", row=1, col=2)

fig.update_layout(height=500, 
                 title="Planning Your Study: The Trade-offs")
fig.show()
```
:::
::: {.column}
**Key Formula for Means:** $n = \left(\frac{z^* \sigma}{ME}\right)^2$

**Trade-offs:**
- Want smaller margin of error? Need bigger sample
- Want higher confidence? Need bigger sample  
- Want to save money? Accept wider intervals
:::
::::

---

## Real Example: Student Sleep Study 😴 {.smaller}

:::: {.columns}
::: {.column}
**Research Question:** How many hours do UCSB students sleep per night?

```{python}
#| fig-width: 12
#| fig-height: 8

# Simulate sleep study data
np.random.seed(123)
n = 50
sleep_data = np.random.normal(7.2, 1.5, n)  # Mean 7.2 hours, SD 1.5

# Calculate statistics
x_bar = np.mean(sleep_data)
s = np.std(sleep_data, ddof=1)
se = s / np.sqrt(n)

# 95% confidence interval
df = n - 1
t_crit = t.ppf(0.975, df)
margin = t_crit * se
ci_lower = x_bar - margin
ci_upper = x_bar + margin

# Create comprehensive visualization
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("Sample Data Distribution", "Step-by-Step Calculation",
                   "Confidence Interval Visualization", "Interpretation"),
    specs=[[{"type": "histogram"}, {"type": "table"}],
           [{"type": "scatter"}, {"type": "scatter"}]]
)

# Sample distribution
fig.add_trace(
    go.Histogram(x=sleep_data, nbinsx=15, name="Sleep Hours",
                marker_color=colors['primary'], opacity=0.7),
    row=1, col=1
)

# Calculation table
calc_data = [
    ["Sample Size (n)", f"{n}"],
    ["Sample Mean (x̄)", f"{x_bar:.2f} hours"],
    ["Sample SD (s)", f"{s:.2f} hours"],
    ["Standard Error", f"{se:.3f} hours"],
    ["t* (95%, df={})".format(df), f"{t_crit:.3f}"],
    ["Margin of Error", f"{margin:.3f} hours"],
    ["95% CI Lower", f"{ci_lower:.2f} hours"],
    ["95% CI Upper", f"{ci_upper:.2f} hours"]
]

fig.add_trace(
    go.Table(
        header=dict(values=["Statistic", "Value"],
                   fill_color=colors['light'],
                   font=dict(size=12)),
        cells=dict(values=list(zip(*calc_data)),
                  fill_color='white',
                  font=dict(size=11))
    ),
    row=1, col=2
)

# CI visualization
fig.add_trace(
    go.Scatter(x=[ci_lower, x_bar, ci_upper], 
               y=[1, 1, 1],
               mode='markers+lines',
               marker=dict(size=[20, 25, 20], 
                          color=[colors['danger'], colors['success'], colors['danger']]),
               line=dict(color=colors['primary'], width=8),
               name="95% Confidence Interval"),
    row=2, col=1
)

# Add text annotations
fig.add_annotation(x=ci_lower, y=1.1, text=f"{ci_lower:.2f}", 
                  showarrow=False, row=2, col=1)
fig.add_annotation(x=x_bar, y=1.1, text=f"{x_bar:.2f}", 
                  showarrow=False, row=2, col=1)
fig.add_annotation(x=ci_upper, y=1.1, text=f"{ci_upper:.2f}", 
                  showarrow=False, row=2, col=1)

# Interpretation
interpretation_text = [
    f"We are 95% confident that the true average sleep time",
    f"for UCSB students is between {ci_lower:.2f} and {ci_upper:.2f} hours.",
    f"",
    f"This means if we repeated this study 100 times,",
    f"about 95 of our intervals would contain the true mean."
]

fig.add_trace(
    go.Scatter(x=[0.5], y=[0.5], mode='text',
              text="<br>".join(interpretation_text),
              textfont=dict(size=14),
              showlegend=False),
    row=2, col=2
)

fig.update_layout(height=700, showlegend=False,
                 title="Complete Analysis: UCSB Student Sleep Study")

# Update axes
fig.update_xaxes(title_text="Hours of Sleep", row=1, col=1)
fig.update_yaxes(title_text="Frequency", row=1, col=1)
fig.update_xaxes(title_text="Hours of Sleep", range=[6.5, 8], row=2, col=1)
fig.update_yaxes(title_text="", showticklabels=False, range=[0.8, 1.2], row=2, col=1)
fig.update_xaxes(showticklabels=False, row=2, col=2)
fig.update_yaxes(showticklabels=False, row=2, col=2)

fig.show()
```
:::
::: {.column}
**Bottom Line:** We're 95% confident that UCSB students sleep between 6.73 and 7.47 hours per night on average.
:::
::::

---

## Common Mistakes to Avoid ⚠️ {.smaller}

:::: {.columns}
::: {.column}
### ❌ Wrong Interpretations

**"95% of students sleep in this range"**
- NO! This is about the population mean, not individual students

**"There's a 95% chance μ is in our interval"**
- NO! μ is fixed; our interval varies

**"We can be 95% certain"**
- NO! Use "confident" not "certain"
:::
::: {.column}
### ✅ Correct Approach

**"We are 95% confident the population mean is in this interval"**

**Key Reminders:**
- Check conditions before using formulas
- Use t-distribution when σ is unknown
- Larger samples give narrower intervals
- Higher confidence gives wider intervals
:::
::::

---

## Practice Problems 📝 {.smaller}

:::: {.columns}
::: {.column}
### Problem 1: Coffee Shop Revenue

A coffee shop owner samples 36 days and finds average daily revenue of $850 with standard deviation $120.

**Your turn:** Calculate a 90% confidence interval for the true average daily revenue.

::: {.callout-note collapse="true"}
## Solution

**Given:** n = 36, x̄ = $850, s = $120, 90% confidence

**Step 1:** Check conditions ✓ (large sample)

**Step 2:** Find critical value  
df = 35, t* = 1.690 (90% confidence)

**Step 3:** Calculate standard error  
SE = 120/√36 = 120/6 = $20

**Step 4:** Build interval  
CI = 850 ± 1.690 × 20 = 850 ± 33.8 = ($816.20, $883.80)

**Interpretation:** We are 90% confident the true average daily revenue is between $816.20 and $883.80.
:::
:::
::: {.column}
### Problem 2: Student Survey

In a survey of 400 students, 280 say they would recommend their major to a friend.

**Your turn:** 
1. Calculate the sample proportion
2. Build a 95% confidence interval  
3. Check if conditions are met

::: {.callout-note collapse="true"}
## Solution

**Step 1:** Sample proportion  
p̂ = 280/400 = 0.70 (70%)

**Step 2:** Check conditions  
np̂ = 400(0.70) = 280 ≥ 10 ✓  
n(1-p̂) = 400(0.30) = 120 ≥ 10 ✓

**Step 3:** Build CI  
SE = √[0.70(0.30)/400] = √[0.000525] = 0.0229  
z* = 1.96 (95% confidence)  
CI = 0.70 ± 1.96(0.0229) = 0.70 ± 0.045 = (0.655, 0.745)

**Interpretation:** We are 95% confident that between 65.5% and 74.5% of all students would recommend their major.
:::
:::
::::

--- -->

## Looking Ahead: Hypothesis Testing 🔮 {.smaller}

:::: {.columns}
::: {.column}
**Next week we'll learn:**
- How to test specific claims about populations
- When to reject or fail to reject hypotheses  
- The connection between confidence intervals and hypothesis tests

:::

::: {.column}

```{python}
#| fig-width: 10
#| fig-height: 4
#| eval: false

# Preview of hypothesis testing
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=("Confidence Intervals", "Hypothesis Testing")
)

# CI approach
x = np.linspace(45, 55, 1000)
y = norm.pdf(x, 50, 2)

fig.add_trace(
    go.Scatter(x=x, y=y, mode='lines', name='Sampling Distribution',
              line=dict(color=colors['primary'], width=3)),
    row=1, col=1
)

# Add CI
ci_x = x[(x >= 46.08) & (x <= 53.92)]
ci_y = y[(x >= 46.08) & (x <= 53.92)]
fig.add_trace(
    go.Scatter(x=ci_x, y=ci_y, fill='tonexty', mode='none',
              fillcolor=f'rgba(59, 130, 246, 0.3)', name='95% CI'),
    row=1, col=1
)

# Hypothesis testing approach
fig.add_trace(
    go.Scatter(x=x, y=y, mode='lines', name='Sampling Distribution',
              line=dict(color=colors['danger'], width=3)),
    row=1, col=2
)

# Add rejection regions
reject_left = x[x <= 46.08]
reject_right = x[x >= 53.92]
fig.add_trace(
    go.Scatter(x=reject_left, y=norm.pdf(reject_left, 50, 2), 
              fill='tonexty', mode='none',
              fillcolor=f'rgba(239, 68, 68, 0.3)', name='Reject H₀'),
    row=1, col=2
)
fig.add_trace(
    go.Scatter(x=reject_right, y=norm.pdf(reject_right, 50, 2), 
              fill='tonexty', mode='none',
              fillcolor=f'rgba(239, 68, 68, 0.3)', name='Reject H₀'),
    row=1, col=2
)

fig.update_layout(height=400, width=600, showlegend=False)
fig.show()
```
:::
::: {.column}
<!-- Optionally add a summary or preview graphic here -->
:::
::::

---

## Key Takeaways 🎯 {.smaller}

:::: {.columns}
::: {.column}
**Big Ideas:**
1. **Samples vary** - confidence intervals capture this uncertainty
2. **Larger samples** give more precise estimates  
3. **Higher confidence** means wider intervals
4. **The CLT** makes normal-based inference possible
:::
::: {.column}
**Practical Skills:**
- Build CIs for means and proportions
- Interpret confidence correctly
- Plan sample sizes for desired precision
- Avoid common interpretation mistakes
:::
::::

---

## Questions? 🤔 {.smaller}

:::: {.columns}
::: {.column}
**Office Hours:** Thursday 11AM (link on Canvas)  
**Email:** nmathlouthi@ucsb.edu  
**Next Class:** Hypothesis Testing and p-values

*"The goal is not to eliminate uncertainty, but to understand and work with it"*
:::
::: {.column}
<!-- Optionally add a closing note or image here -->
:::
::::