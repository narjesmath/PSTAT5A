---
title: "Descriptive Statistics Part II"
subtitle: "Variability, Position, Shape & Visualization"
author: "Narjes Mathlouthi"
date: today
format: 
  revealjs:
    logo: /img/logo.png
    theme: default
    css: /files/lecture_notes/theme/lecture-styles.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Understanding Data - Descriptive Statistics Part II ¬© 2025 Narjes Mathlouthi"
    transition: slide
    background-transition: fade
    execute:
        eval: false
jupyter: python3
---

## Learning Objectives {.smaller}

By the end of this lecture, you will be able to:

- Calculate and interpret measures of variability (range, variance, standard deviation)
- Understand and compute measures of position (percentiles, quartiles, z-scores)
- Assess distribution shape using skewness and kurtosis
- Create and interpret histograms with appropriate bin widths
- Construct and analyze boxplots for data exploration
- Identify trends, patterns, and outliers in data visualizations
- Apply Python for comprehensive descriptive analysis and visualization

---

## Lecture Outline {.smaller}

::: {.two-columns}
::: {}
**Part I: Measures of Variability (25 min)**
- Range, Variance, Standard Deviation
- Coefficient of Variation
- Python Implementation

**Part II: Measures of Position (20 min)**
- Percentiles and Quartiles
- Z-scores and Standardization
:::

::: {}
**Part III: Distribution Shape (10 min)**
- Skewness and Kurtosis

**Part IV: Data Visualization (20 min)**
- Histograms and Bin Width Selection
- Boxplots and Interpretation

**Part V: Identifying Patterns (5 min)**
:::
:::

---

# Measures of Variability
*25 minutes*



## What is Variability?{.smaller}

::: {.concept-box}
#### üéØ Definition
**Variability** (or dispersion) measures how spread out or scattered the data points are around the center.
:::

### Why Variability Matters

- Two datasets can have the same mean but very different spreads
- Variability indicates **consistency** and **predictability**
- Essential for **risk assessment** and **quality control**
- Helps determine **confidence** in our central tendency measures

---

## Comparing Datasets with Same Mean

::: {.example}
**Dataset A:** 98, 99, 100, 101, 102 (Mean = 100)

**Dataset B:** 80, 90, 100, 110, 120 (Mean = 100)
:::

:::{.fragment}
::: {.key-difference}
**Both have the same mean (100), but Dataset B is much more variable!**

This is why we need measures of variability.
:::
:::

---

## Range

::: {.formula-box}
**Range** = Maximum value - Minimum value
:::

### Example
**Data:** 12, 15, 18, 22, 25, 30, 35

:::{.fragment}  
**Range** = 35 - 12 = 23
:::

---

## Properties of Range

::: {.two-columns}
::: {.example}
#### ‚úÖ Advantages
- **Simple to calculate** and understand
- **Quick measure** of spread
- **Easy to communicate**
:::

::: {.important}
#### ‚ùå Disadvantages
- **Uses only two values** (ignores all others)
- **Sensitive to outliers**
- **Limited information** about distribution
:::
:::

---

## When to Use Range

::: {.concept-box}
#### Use range when:
- Need a **quick, simple measure** of spread
- Working with **small datasets**
- Communicating to **non-technical audiences**

#### Avoid range when:
- **Outliers** are present
- Need **detailed information** about variability
- Working with **large datasets**
:::

---

# Variance and Standard Deviation

---

## Variance Definition

::: {.concept-box}
#### üéØ Definition
**Variance** measures the average squared deviation from the mean.
:::

![](/files/lecture_notes/lecture3/img/variance.png)

---

## Population vs Sample Variance {.center}

Understanding the mathematical foundations

---

## Recall: Population vs Sample

![](/files/lecture_notes/lecture3/img/sampling.png)

---

## Side-by-Side Comparison

::: {.variance-container}

::: {.variance-section}
::: {.variance-header}
Population Variance
:::

::: {.formula-box}
$$\sigma^2 = \frac{\sum_{i=1}^{N} (x_i - \mu)^2}{N}$$
:::

::: {.definitions}
::: {.definition-item}
$\sigma^2$ = population variance
:::
::: {.definition-item}
$x_i$ = value of $i^{th}$ element
:::
::: {.definition-item}
$\mu$ = population mean
:::
::: {.definition-item}
$N$ = population size
:::
:::
:::

::: {.variance-section}
::: {.variance-header}
Sample Variance
:::

::: {.formula-box}
$$s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1}$$
:::

::: {.definitions}
::: {.definition-item}
$s^2$ = sample variance
:::
::: {.definition-item}
$x_i$ = value of $i^{th}$ element
:::
::: {.definition-item}
$\bar{x}$ = sample mean
:::
::: {.definition-item}
$n$ = sample size
:::
:::
:::

:::

---

## Key Differences

::: {.key-difference}
**Key Difference:** Sample variance uses $(n-1)$ instead of $N$ in the denominator
:::

**Why $(n-1)$?**
- When we use sample mean $\bar{x}$ to estimate population mean $\mu$
- We lose one **degree of freedom**
- Called **Bessel's correction**
- Makes sample variance an **unbiased estimator**

---

## When to Use Each Formula

::: {.two-columns}
::: {.concept-box}
#### Population Variance ($\sigma^2$)
- You have data for the **entire population**
- You know the true population mean $\mu$
- Example: Test scores for all students in a small class
:::

::: {.concept-box}
#### Sample Variance ($s^2$)
- You have data from a **sample only**
- Want to estimate population variance
- Example: Survey responses from 100 people out of 10,000
:::
:::

---

## Degrees of Freedom Concept

::: {.concept-box}
#### üéØ Definition
**Degrees of Freedom** = Number of independent pieces of information available for estimating a parameter
:::

---

## Understanding Degrees of Freedom

::: {.two-columns}

::: {.example}
#### üìä Population Case
**All observations are independent**

- We know the true population mean $\mu$
- Each of the $N$ observations provides independent information
- No constraints on the data

:::{.fragment}
$$\text{Degrees of Freedom} = N$$
:::
:::

::: {.important}
#### üìà Sample Case
**Constraint introduced by sample mean**

- We must estimate $\mu$ using $\bar{x}$
- Once we know $\bar{x}$ and $(n-1)$ observations, the last one is determined
- We "lose" one degree of freedom

:::{.fragment}
$$\text{Degrees of Freedom} = n-1$$
:::
:::

:::

---

## Why Does This Matter?

::: {.key-difference}
**Key Insight:** Using $\bar{x}$ instead of $\mu$ creates dependency among observations
:::

::: {.concept-box}
#### üß† Intuitive Explanation
Imagine you have 5 numbers that must average to 10:

**If the mean is fixed at 10:**
- Choose any 4 numbers freely: 8, 12, 7, 15
- The 5th number is forced: $5 \times 10 - (8+12+7+15) = 8$
- Only 4 degrees of freedom, not 5!

**This is exactly what happens with sample variance**
:::

---

## Mathematical Consequences

::: {.two-columns}

::: {.theorem}
#### ‚úÖ With $(n-1)$: Unbiased
$$E[s^2] = E\left[\frac{\sum(x_i - \bar{x})^2}{n-1}\right] = \sigma^2$$

Sample variance is an **unbiased estimator** of population variance
:::

::: {.note}
#### ‚ùå With $n$: Biased
$$E\left[\frac{\sum(x_i - \bar{x})^2}{n}\right] = \frac{n-1}{n}\sigma^2$$

Systematically **underestimates** the true variance by factor $\frac{n-1}{n}$
:::

:::

---

## Practical Impact

::: {.variance-container}

::: {.variance-section}
::: {.variance-header}
Small Samples (n = 5)
:::

**Bias factor:** $\frac{n-1}{n} = \frac{4}{5} = 0.8$

**20% underestimation** if using $n$

::: {.key-difference}
**Large impact on inference!**
:::
:::

::: {.variance-section}
::: {.variance-header}
Large Samples (n = 100)
:::

**Bias factor:** $\frac{n-1}{n} = \frac{99}{100} = 0.99$

**1% underestimation** if using $n$

::: {.concept-box}
**Minimal practical difference**
:::
:::

:::

---

## Example Calculation

**Data:** 3, 7, 2, 8, 5

::: {.two-columns}
::: {.example}
#### If this is the entire population:
- $\mu = \frac{3+7+2+8+5}{5} = 5$
- $\sigma^2 = \frac{(3-5)^2+(7-5)^2+(2-5)^2+(8-5)^2+(5-5)^2}{5} = \frac{22}{5} = 4.4$
:::

::: {.solution}
#### If this is a sample:
- $\bar{x} = 5$ (same calculation)
- $s^2 = \frac{22}{5-1} = \frac{22}{4} = 5.5$
:::
:::

---

## Standard Deviation Definition

::: {.concept-box}
#### üéØ Definition
**Standard Deviation** is the square root of variance.
:::

::: {.two-columns}
::: {.formula-box}
#### Sample Standard Deviation
$$s = \sqrt{s^2} = \sqrt{\frac{\sum(x_i - \bar{x})^2}{n-1}}$$
:::

::: {.formula-box}
#### Population Standard Deviation
$$\sigma = \sqrt{\sigma^2} = \sqrt{\frac{\sum(x_i - \mu)^2}{N}}$$
:::
:::

---

## Step-by-Step Standard Deviation Example

### Given Data

::: {.concept-box}
#### Sample Data: 82, 95, 83, 60, 92
#### Sample Size: n = 5
:::

---

## Step 1: Calculate the Sample Mean

::: {.formula-box}
$$\bar{x} = \frac{\sum x_i}{n} = \frac{82 + 95 + 83 + 60 + 92}{5} = \frac{412}{5} = 82.4$$
:::

---

## Step 2: Calculate Deviations and Squared Deviations

| $x_i$ | $x_i - \bar{x}$ | $(x_i - \bar{x})^2$ |
|-------|-----------------|---------------------|
| 82    | $82 - 82.4 = -0.4$  | $(-0.4)^2 = 0.16$      |
| 95    | $95 - 82.4 = 12.6$  | $(12.6)^2 = 158.76$    |
| 83    | $83 - 82.4 = 0.6$   | $(0.6)^2 = 0.36$       |
| 60    | $60 - 82.4 = -22.4$ | $(-22.4)^2 = 501.76$   |
| 92    | $92 - 82.4 = 9.6$   | $(9.6)^2 = 92.16$      |
| **Sum** |               | $\sum = 753.2$         |

---

## Step 3: Calculate Sample Variance

::: {.formula-box}
$$s^2 = \frac{\sum(x_i - \bar{x})^2}{n-1} = \frac{753.2}{5-1} = \frac{753.2}{4} = 188.3$$
:::

---

## Step 4: Calculate Sample Standard Deviation

::: {.formula-box}
$$s = \sqrt{s^2} = \sqrt{188.3} = 13.72$$
:::

---

## Final Answer

::: {.key-difference}
**Sample Standard Deviation = 13.72**
:::

::: {.concept-box}
#### Summary of Results:
- **Sample Mean:** $\bar{x} = 82.4$
- **Sample Variance:** $s^2 = 188.3$  
- **Sample Standard Deviation:** $s = 13.72$
:::

---

## Properties of Standard Deviation

::: {.concept-box}
#### Key Properties:
- **Same units** as the original data
- **Always non-negative**
- **Zero** only when all values are identical
- **Larger values** indicate more variability
- **Approximately 68%** of data within 1 SD of mean (for normal distributions)
- **Approximately 95%** of data within 2 SD of mean
:::

---

## Empirical Rule (68-95-99.7 Rule)

::: {.important}
#### For approximately normal distributions:
- **68%** of data falls within 1 standard deviation of the mean
- **95%** of data falls within 2 standard deviations of the mean
- **99.7%** of data falls within 3 standard deviations of the mean

This rule helps us understand what constitutes "typical" vs "unusual" values.
:::

---

## Coefficient of Variation

::: {.formula-box}
**Coefficient of Variation (CV)** = $\frac{\text{Standard Deviation}}{\text{Mean}} \times 100\%$
:::

### Why Use CV?
- **Compares variability** across different units or scales
- **Relative measure** of variability
- Useful when **means differ substantially**

---

## CV Example: Comparing Variability

::: {.two-columns}
::: {.example}
#### Stock A
- Mean return = $50
- SD = $10
- **CV = 20%**
:::

::: {.example}
#### Stock B
- Mean return = $500
- SD = $50
- **CV = 10%**
:::
:::

:::{.fragment}
::: {.key-difference}
**Stock B has higher absolute variability ($50 vs $10) but lower relative variability (10% vs 20%)**

Stock B is relatively less risky per dollar invested.
:::
:::

---

## Python Implementation - Variability

```{python}
#| echo: true
#| eval: false
import numpy as np
import pandas as pd

# Sample data
data = [10, 12, 14, 16, 18, 22, 25]

# Calculate measures of variability
range_val = np.max(data) - np.min(data)
variance_sample = np.var(data, ddof=1)  # Sample variance
std_sample = np.std(data, ddof=1)       # Sample standard deviation
cv = (std_sample / np.mean(data)) * 100

print(f"Range: {range_val}")
print(f"Variance: {variance_sample:.2f}")
print(f"Standard Deviation: {std_sample:.2f}")
print(f"Coefficient of Variation: {cv:.1f}%")
```

---

# Measures of Position
*20 minutes*

---

## What are Measures of Position?

::: {.concept-box}
#### üéØ Purpose
**Measures of position** tell us where a particular value stands relative to the rest of the data.
:::

They answer questions like:
- "What percentage of students scored below 85?"
- "Is this value typical or unusual?"
- "How does this observation compare to others?"

---

## Percentiles Definition

::: {.definition}
#### The k-th percentile
The value below which k% of the data falls.
:::

### Examples:
- **50th percentile** = Median (50% of data below this value)
- **90th percentile** = 90% of data falls below this value
- **25th percentile** = 25% of data falls below this value

---

## Quartiles

::: {.concept-box}
#### Quartiles divide the data into four equal parts:

- **Q1 (First Quartile)** = 25th percentile
- **Q2 (Second Quartile)** = 50th percentile = Median
- **Q3 (Third Quartile)** = 75th percentile
:::

---

## Interquartile Range (IQR)

::: {.formula-box}
**IQR = Q3 - Q1**
:::

### Properties of IQR:
- Contains the **middle 50%** of the data
- **Resistant to outliers**
- Used in **boxplot construction**
- Useful for **outlier detection**

---

## Five-Number Summary

::: {.concept-box}
#### The five-number summary provides a complete picture of data distribution:

1. **Minimum**
2. **Q1 (25th percentile)**
3. **Median (50th percentile)**
4. **Q3 (75th percentile)**
5. **Maximum**

This forms the basis for boxplots!
:::

---

## Z-scores and Standardization

::: {.formula-box}
**Z-score** tells us how many standard deviations a value is from the mean.

$$z = \frac{x - \mu}{\sigma} \text{ or } z = \frac{x - \bar{x}}{s}$$
:::

---

## Interpreting Z-scores

::: {.concept-box}
#### Z-score Interpretation:
- **z = 0**: Value equals the mean
- **z = 1**: Value is 1 standard deviation above the mean
- **z = -2**: Value is 2 standard deviations below the mean
- **|z| > 2**: Often considered "unusual" (beyond 95% of data)
- **|z| > 3**: Very unusual (beyond 99.7% of data)
:::

---

## Z-score Example

::: {.example}
#### Problem:
Student's test score: 85  
Class mean: 78, Class standard deviation: 6

$$z = \frac{85 - 78}{6} = \frac{7}{6} = 1.17$$

**Interpretation:** This student scored 1.17 standard deviations above the class average.
:::

---

## Benefits of Standardization

::: {.concept-box}
#### Why use z-scores?
- **Compare across different scales** (test scores vs income)
- **Identify outliers** systematically  
- **Combine different variables** meaningfully
- **Prepare data** for certain statistical methods
:::

---

## Python Implementation - Position & Shape

```{python}
#| echo: true
#| eval: false
import numpy as np
import pandas as pd
from scipy import stats

data = [12, 15, 18, 22, 25, 28, 30, 35, 40, 45]

# Percentiles and quartiles
q1 = np.percentile(data, 25)
median = np.percentile(data, 50)
q3 = np.percentile(data, 75)
iqr = q3 - q1

# Z-scores
z_scores = stats.zscore(data)

# Shape measures
skewness = stats.skew(data)
kurt = stats.kurtosis(data)

print(f"Q1: {q1}, Median: {median}, Q3: {q3}")
print(f"IQR: {iqr}")
print(f"Skewness: {skewness:.3f}")
print(f"Kurtosis: {kurt:.3f}")
```

---

# Distribution Shape
*10 minutes*

---

## Skewness

::: {.concept-box}
#### üéØ Definition
**Skewness** measures the asymmetry of a distribution.
:::

### Types of Skewness:
- **Symmetric (Skewness ‚âà 0)**: Mean ‚âà Median ‚âà Mode
- **Right-skewed (Positive skewness)**: Mean > Median, long tail to the right
- **Left-skewed (Negative skewness)**: Mean < Median, long tail to the left

---

## Visual Examples of Skewness

::: {.two-columns}
::: {.example}
#### Right-skewed (Income data):
- Most people earn moderate amounts
- Few people earn very high amounts
- Mean > Median
:::

::: {.example}
#### Left-skewed (Test scores with ceiling effect):
- Most students score high
- Few students score very low  
- Mean < Median
:::
:::

---

## Kurtosis

::: {.concept-box}
#### üéØ Definition
**Kurtosis** measures the "tailedness" of a distribution.
:::

### Types:
- **Mesokurtic (Normal-like)**: Kurtosis ‚âà 3
- **Leptokurtic (Heavy tails)**: Kurtosis > 3, more peaked
- **Platykurtic (Light tails)**: Kurtosis < 3, flatter

**Excess Kurtosis** = Kurtosis - 3 (makes normal distributions have excess kurtosis of 0)

---

# Data Visualization
*20 minutes*

---

## What is a Histogram?

::: {.concept-box}
#### üéØ Definition
A **histogram** displays the distribution of a continuous variable by dividing data into bins and showing the frequency of observations in each bin.
:::

### Key Components:
- **X-axis**: Variable values (continuous)
- **Y-axis**: Frequency or density
- **Bins**: Intervals that group the data
- **Bars**: Height represents frequency in each bin

---

## Choosing Bin Width: Critical Decision

::: {.important}
#### Bin width dramatically affects histogram interpretation!
:::

::: {.two-columns}
::: {.note}
#### Too Few Bins (Wide bins):
- **Oversmoothing** - lose important details
- May **hide multimodality**
- Distribution appears simpler than it is
:::

::: {.note}
#### Too Many Bins (Narrow bins):
- **Undersmoothing** - too much noise
- May create **artificial gaps**
- Hard to see overall pattern
:::
:::

---

## Bin Width Guidelines

::: {.concept-box}
#### Rule of Thumb Methods:

1. **Square Root Rule**: Number of bins ‚âà $\sqrt{n}$
2. **Sturges' Rule**: Number of bins = $1 + \log_2(n)$
3. **Scott's Rule**: Bin width = $\frac{3.5 \times \text{SD}}{n^{1/3}}$
4. **Freedman-Diaconis Rule**: Bin width = $\frac{2 \times \text{IQR}}{n^{1/3}}$

**Best practice:** Try multiple bin widths and choose based on the story your data tells!
:::

---

## Interpreting Histograms

::: {.concept-box}
#### What to Look For:

1. **Shape**: Normal, skewed, uniform, bimodal?
2. **Center**: Where is the "typical" value?
3. **Spread**: How variable is the data?
4. **Outliers**: Any unusual values?
5. **Gaps**: Are there missing values in certain ranges?
6. **Multiple peaks**: Suggests multiple subgroups
:::

---

## Anatomy of a Boxplot

::: {.concept-box}
#### Boxplot Components:

**The Box:**
- **Left edge**: Q1 (25th percentile)
- **Middle line**: Median (Q2, 50th percentile)  
- **Right edge**: Q3 (75th percentile)
- **Box width**: IQR (contains middle 50% of data)

**The Whiskers:**
- **Extend to**: Most extreme values within 1.5 √ó IQR from box edges

**Outliers:**
- **Points beyond whiskers**: Values > Q3 + 1.5√óIQR or < Q1 - 1.5√óIQR
:::

---

## What Boxplots Tell Us

::: {.two-columns}
::: {.concept-box}
#### Distribution Shape:
- **Symmetric**: Median in center of box, whiskers equal length
- **Right-skewed**: Median closer to Q1, longer upper whisker
- **Left-skewed**: Median closer to Q3, longer lower whisker
:::

::: {.concept-box}
#### Variability:
- **Wide box**: High variability in middle 50%
- **Long whiskers**: High overall variability
- **Many outliers**: Extreme variability
:::
:::

---

## Comparing Groups with Boxplots

```{python}
#| echo: true
#| eval: false
# Comparing multiple groups
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Create sample data for different groups
np.random.seed(42)
group_a = np.random.normal(70, 10, 100)
group_b = np.random.normal(75, 15, 100)  
group_c = np.random.normal(80, 8, 100)

# Combine into DataFrame
df = pd.DataFrame({
    'Score': np.concatenate([group_a, group_b, group_c]),
    'Group': ['A']*100 + ['B']*100 + ['C']*100
})

# Create comparative boxplot
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='Group', y='Score')
plt.title('Comparing Score Distributions Across Groups')
plt.ylabel('Test Score')
plt.show()
```

---

# Identifying Trends and Patterns
*5 minutes*

---

## Common Patterns in Data

::: {.concept-box}
#### Distribution Patterns:
1. **Normal/Bell-shaped**: Symmetric, single peak
2. **Uniform**: All values equally likely
3. **Bimodal**: Two distinct peaks (suggests subgroups)
4. **Multimodal**: Multiple peaks
5. **U-shaped**: High values at extremes, low in middle

#### Outlier Patterns:
- **Individual outliers**: Data entry errors, measurement errors
- **Clustered outliers**: Distinct subpopulation
- **Systematic outliers**: May indicate process changes
:::

---

## Red Flags in Data Visualization

::: {.important}
#### Warning Signs:
1. **Gaps in histograms**: Missing data or measurement limitations
2. **Heaping**: Values cluster at round numbers (10, 50, 100)
3. **Truncation**: Data cut off at certain values
4. **Digit preference**: People prefer certain ending digits
5. **Multiple modes**: Hidden subgroups in your data
:::

---

# Key Takeaways

---

## Essential Concepts to Remember

::: {.two-columns}
::: {.concept-box}
#### Variability:
- **Standard deviation** is preferred over range for most analyses
- **CV** allows comparison across different scales
- **IQR** is resistant to outliers

#### Position:
- **Percentiles** and **quartiles** provide relative position
- **Z-scores** standardize across different distributions
- **Five-number summary** gives complete overview
:::

::: {.concept-box}
#### Visualization:
- **Bin width choice** is critical for histogram interpretation
- **Boxplots** excel at comparing groups and identifying outliers
- **Multiple visualizations** provide different insights

#### General:
- **Always visualize** before calculating statistics
- **Use multiple measures** - no single statistic tells the whole story
:::
:::

---

## Practice Problems

::: {.problem}
#### Try These Exercises

1. **Calculate** the five-number summary for: 12, 15, 18, 22, 25, 28, 30, 35, 40, 45

2. **Create histograms** with 5, 15, and 50 bins for the same dataset. What patterns do you observe?

3. **Interpret this scenario**: Dataset A has mean=50, SD=5. Dataset B has mean=100, SD=10. Which is more variable?

4. **A student scores 85** on a test where the class mean is 78 and SD is 6. Calculate and interpret the z-score.

5. **Design a boxplot comparison** for three different customer segments in your business.
:::

---

## Questions? {.center}

**Thank you for your attention!**

*Remember: The goal of descriptive statistics is to understand your data story - let the visualizations guide your insights!*

---

## Additional Resources

::: {.concept-box}
#### For Further Learning:
- **Matplotlib gallery**: Histogram and boxplot examples
- **Seaborn documentation**: Statistical visualizations
- **NumPy/SciPy**: Statistical functions reference
- **Recommended reading**: Chapter 4-5 in course textbook

#### Next Steps:
- **Correlation and association** between variables
- **Time series analysis** for temporal patterns
- **Statistical inference** based on descriptive findings
:::