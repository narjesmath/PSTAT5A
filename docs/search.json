[
  {
    "objectID": "files/labs/lab3/lab3.html",
    "href": "files/labs/lab3/lab3.html",
    "title": "Lab 3: Descriptive Statistics",
    "section": "",
    "text": "It’s finally time for us to revisit our notions of descriptive statistics (from Week 1 of the course), now in the context of Python!"
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-1",
    "href": "files/labs/lab3/lab3.html#task-1",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 1",
    "text": "Task 1\n\n⏱️ Estimated time: 3 minutes\n\n\nImport the numpy module as np, and check that np.sin(0) returns a value of 0.\nImport the datascience module as ds, and check that\n\n\nds.Table().with_columns(\n  \"Col1\", [1, 2, 3],\n  \"Col2\", [2, 3, 4]\n)\n\nprints correctly as :\n\n\n\n\n\nCol1\nCol2\n\n\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you import a module with an abbreviation &lt;abbreviation&gt;, you must always use the abbreviation when referencing the module; not the original module name. For example, after importing numpy as np, running numpy.sin() would return an error."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#numerical-summaries",
    "href": "files/labs/lab3/lab3.html#numerical-summaries",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nMeasures of Central Tendency\n\n\n⏱️ Estimated time: 5 minutes\n\nRecall that for a list of numbers (\\(x_1,x_2,\\ldots, x_n\\)), \\(X = \\{ x_i\\}_{i=1}^{n}\\)​, the mean is defined as\n\n\\(\\bar x= \\frac{1}{n} \\sum_{i=1}^{n} x_i​\\)\n\nComputing the mean of a list or array of numbers in Python is relatively simple, using the np.mean() function [recall that we imported the numpy module with the abbreviation np, meaning np.mean() is a shorthand for numpy.mean()]. Similarly, to compute the median of a list or array we can use np.median().\n\nUnderstanding Functions and Parameters\nBefore we dive into examples, let’s understand what functions are and how they work with parameters. A function is a block of code that performs a specific task. Functions can take parameters (also called arguments) as input, process them, and return a result.\nThe general syntax for calling a function is:\n\nfunction_name(parameter1, parameter2, ...)\n\nFor statistical functions like np.mean() and np.median(), the main parameter is the data (list or array) you want to analyze. You can read more about about each function in the official documentation for the numpy module.\nHere are some examples:\n\nExample with np.mean():\n\n# Create a simple list of numbers\nnumbers = [2, 4, 6, 8, 10]\n\n# Calculate the mean - the function takes 'numbers' as a parameter\n\naverage = np.mean(numbers)\nprint(f\"The mean is: {average}\")  # Output: The mean is: 6.0\n\nThe mean is: 6.0\n\n\n\n\nExample with np.median():\n\n# Using the same list\nnumbers = [2, 4, 6, 8, 10]\n\n# Calculate the median - the function takes 'numbers' as a parameter  \nmiddle_value = np.median(numbers)\nprint(f\"The median is: {middle_value}\")  # Output: The median is: 6.0\n\n# Try with an odd number of elements\nodd_numbers = [1, 3, 5, 7, 9, 11, 13]\nmedian_odd = np.median(odd_numbers)\nprint(f\"The median of odd list is: {median_odd}\")  # Output: The median of odd list is: 7.0\n\nThe median is: 6.0\nThe median of odd list is: 7.0\n\n\nAnother example with different data:\n\n# Example with decimal numbers\ngrades = [85.5, 92.0, 78.5, 96.0, 88.5, 91.0]\n\nmean_grade = np.mean(grades)\nmedian_grade = np.median(grades)\n\nprint(f\"Mean grade: {mean_grade:.2f}\")     # Output: Mean grade: 88.58\nprint(f\"Median grade: {median_grade:.2f}\") # Output: Median grade: 89.75\n\nMean grade: 88.58\nMedian grade: 89.75\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice how the functions take our data as a parameter (the input inside the parentheses) and return a calculated value that we can store in a variable or use directly."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-2",
    "href": "files/labs/lab3/lab3.html#task-2",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 2",
    "text": "Task 2\n\n⏱️ Estimated time: 3 minutes\n\nLet x_list be a list containing the elements 1, 2, and 3, and let x_array be an array containing the elements 1, 2, and 3. Compute the mean and median of x_list and x_array using the appropriate functions from the numpy module."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#measures-of-spread",
    "href": "files/labs/lab3/lab3.html#measures-of-spread",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Measures of Spread",
    "text": "Measures of Spread\n\n⏱️ Estimated time: 5 minutes\n\nRecall that we also discussed several measures of spread including:\n\nStandard deviation\nIQR (Interquartile Range)\nRange\n\nSure enough, the numpy module contains several functions which help us compute these measures. Let’s examine each separately."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-3",
    "href": "files/labs/lab3/lab3.html#task-3",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 3",
    "text": "Task 3\n\n⏱️ Estimated time: 4 minutes\n\nLook up the documentation of the function np.ptp(), and describe what it does. Also, answer the question: Q: what does ptp actually stand for?\nNow, apply the np.ptp() function on your x_list and x_array variables from Task 1 above and check that it functions like you expect."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-4",
    "href": "files/labs/lab3/lab3.html#task-4",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 4",
    "text": "Task 4\n\n⏱️ Estimated time: 6 minutes\n\nCompute the standard deviation \\(s\\) of the x_list variable from Task 1 by hand, and write down the answer using a comment or Markdown cell.\nNow, run np.std(x_list). Does this answer agree with what you found in part (a) above?\nNow, recompute the standard deviation \\(s\\) of x_list by hand but this time use n instead of n−1 in the formula. How does this answer compare with the result of np.std(x_list)?"
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-4-contd",
    "href": "files/labs/lab3/lab3.html#task-4-contd",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 4 (cont’d)",
    "text": "Task 4 (cont’d)\n\n⏱️ Estimated time: 2 minutes\n\nRun np.std(x_list, ddof = 1) and check whether this matches the result of part (a) above."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#optional-task-creating-your-own-iqr-function",
    "href": "files/labs/lab3/lab3.html#optional-task-creating-your-own-iqr-function",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Optional Task: Creating Your Own IQR Function",
    "text": "Optional Task: Creating Your Own IQR Function\n\n⏱️ Estimated time: 3 minutes\n\nNow that you understand how to use functions and their parameters, let’s create our own custom function to calculate the IQR! This will help you understand how to define your own functions in Python.\n1. Understanding Function Definition Syntax\nTo create a function in Python, we use the def keyword followed by the function name and parameters:\n\ndef function_name(parameter1, parameter2):\n    # Code that does something with the parameters\n    result = some_calculation\n    return result  # Give back the calculated value\n\n2. Create the IQR Function\nLet’s create a function called calculate_iqr that takes a list or array as input and returns the IQR:\n\ndef calculate_iqr(data):\n    \"\"\"\n    Calculate the Interquartile Range (IQR) of a dataset.\n    \n    Parameter:\n    data - a list or array of numbers\n    \n    Returns:\n    The IQR value (Q3 - Q1)\n    \"\"\"\n    # Calculate the IQR using the numpy method we learned\n    iqr_value = np.diff(np.percentile(data, [25, 75]))[0]\n    return iqr_value\n\n3. Test Your Function\nNow let’s test our custom function with some example data:\n\n# Create some test data\n# Example - test_scores = [72, 85, 90, 78, 92, 88, 76, 94, 82, 89, 91, 77]\n\n# Create your data\ntest_scores = [ ]\n\n# Use our custom function\nmy_iqr = calculate_iqr( ) # Call the function created above here \n\nprint(f\"IQR using our function: {my_iqr}\")\n\n# Compare with the direct method\ndirect_iqr = np.diff(np.percentile(test_scores, [25, 75]))[0]\nprint(f\"IQR using direct method: {direct_iqr}\")\n\n# They should be the same!"
  },
  {
    "objectID": "files/labs/lab3/lab3.html#visualizations",
    "href": "files/labs/lab3/lab3.html#visualizations",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Visualizations",
    "text": "Visualizations\n\n⏱️ Estimated time: 3 minutes\n\nIt’s finally time to make pretty plots! The module we will use to generate visualizations is the matplotlib module (though there are quite a few other modules that work for visualizations as well). The official website for matplotlib can be found at https://matplotlib.org/.\nBefore we generate any plots, we will need to run the following code once:\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nHere’s what these lines of code are doing:\n\n%matplotlib inline tells Jupyter to display our plots directly underneath our code. It removes the need to use plt.show().\nimport matplotlib imports the matplotlib module\nimport matplotlib.pyplot as plt imports the pyplot submodule (a submodule is just a module contained within another larger module) with the abbreviation plt.\nplt.style.use('seaborn-v0_8-whitegrid') tells Jupyter to use a specific theme (called seaborn-v0_8-whitegrid) when generating plots.\n\n\n\n\n\n\n\nNote\n\n\n\nAgain, notice the beauty of the import &lt;module&gt; as &lt;abbreviation&gt; syntax.\n\nafter running the third line above, we no longer need to write matplotlib.pyplot in our code, we just use pltto call the module!\n\n\n\nAlso, there are lots of other themes you can use when generating your plots: after completing this lab, I encourage you to consult this reference guide for a list of a few other pyplot themes.\n\nBoxplots and Histograms\nNow, let’s proceed on to make some plots. The first two types of plots we will look at are the two we used to describe numerical data: namely, boxplots and histograms.\nThe functions we will use are the plt.boxplot() and plt.hist() functions, respectively."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-5",
    "href": "files/labs/lab3/lab3.html#task-5",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 5",
    "text": "Task 5\n\n⏱️ Estimated time: 6 minutes\n\n\nMake a list called y that contains the following elements: [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2].\nRun plt.boxplot(y); (be sure to include the semicolon!). Your plot should look like a basic vertical boxplot.\n\n\n\n\n\n\n\n\n\n\nLet’s make our boxplot horizontal, as opposed to vertical.\n\n\n\n\n\n\nTip\n\n\n\nRecall, that functions take on parameters (or arguments). For detailed list or arguments that can be passed on to plt.boxplot, consult the function documentationmatplotlib.pyplot.boxplot().\n\n\nFor plt.boxplot(), there is an optional param called orientation. It accepts the values “vertical” or “horizental” with ” vertical” being the default.\nTo use in your pyton code, you can simply add , after your data and add orientation =' ' . Inside \" \" specify the desired direction and close the parenthesis.\n\n\n\n\n\n\n\n\n\nNext, let’s add some color to our plot. Within your call to plt.boxplot(), after your data and orientation param, add a semicolon , to seperate the parameters and add the following: patch_artist=True, boxprops = dict(facecolor = \"aquamarine\") . Your boxplot should look like this:\n\n\n\n\n\n\n\n\n\n\nWhat do these parameters do?\n\npatch_artist=True: This parameter tells matplotlib to draw the boxplot elements (like the box itself) as “patch” objects, which can be filled with colors. By default, boxplots are drawn as simple lines without fill capability. Setting this to True enables us to add colors and other visual effects to the box.\nboxprops = dict(facecolor = \"aquamarine\"): This parameter controls the properties of the box (the rectangle part of the boxplot). The dict() creates a dictionary of properties we want to set:\n\nfacecolor specifies the fill color inside the box “aquamarine” is the color name (you could use other colors like “lightblue”, “pink”, “yellow”, etc.)\nThink of it like this: patch_artist=True gives us a “paintable” box, and boxprops = dict(facecolor = \"aquamarine\") tells Python “paint the inside of that box aquamarine color!”\nTry experimenting: after completing this task, try changing “aquamarine” to “lightcoral” or “lightsteelblue” to see different colors!\n\nFinally, let’s add a Title! Right below your call to plt.boxplot(), add the following:\n\nplt.title(\"My First Python Boxplot\"); (again, don’t forget the semicolons).\n\n\n\n\n\n\n\n\n\n\n\nBased on the boxplot we just generated, what is the IQR of y? Write your answer in a Markdown cell. Then, use the syntax discussed in the previous section of this Lab to use Python to compute the IQR of y, and comment on the result."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-6",
    "href": "files/labs/lab3/lab3.html#task-6",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 6",
    "text": "Task 6\n\n⏱️ Estimated time: 5 minutes\n\nCall the plt.hist() function on the y list defined in Task 5, and use the function documentation to add arguments to your call to plt.hist() function to generate a histogram with appropriate number of bins. Add a label for the x-axis plt.xlabel('Values'), a label for the y-axis plt.ylabel('Frequency') and a title using plt.title('Histogram of y values').\nFor example, to create a basic histogram using plt.hist(), you would use the following:\n\n# Basic histogram\nplt.hist(y);\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can use explicit bin edges like bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5] to center your integer values.\nAlternatively, you could use range=(0.5, 5.5) with bins=5.\nUse rwidth=0.9 to make bars 90% width (creates 10% spacing)\nAdd edge colors with edgecolor='black' for visual separation\n\nYour histogram, will look like this!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPay attention to the number of bins! With 5 distinct values (1, 2, 3, 4, 5), we use 5 bins to clearly show the frequency of each value."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#scatterplots",
    "href": "files/labs/lab3/lab3.html#scatterplots",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Scatterplots",
    "text": "Scatterplots\nWe should also quickly discuss how to generate scatterplots in Python."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-7",
    "href": "files/labs/lab3/lab3.html#task-7",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 7",
    "text": "Task 7\n\n⏱️ Estimated time: 4 minutes\n\n\nCopy-paste the following code into a code cell, and then run that cell (don’t worry about what this code is doing- we’ll discuss that in a future lab).\n\n\nnp.random.seed(5)\n\nx1 = np.random.normal(0, 1, 100)\nx2 = x1 + np.random.normal(0, 1, 100)\n\nplt.scatter(x1, x2);\n\n\n\n\n\n\n\n\n\nAdd an x-axis label that says “x1” and a y-axis label that says “x2”, along with the title “My First Python Scatterplot”."
  },
  {
    "objectID": "files/labs/lab3/lab3.html#task-8",
    "href": "files/labs/lab3/lab3.html#task-8",
    "title": "Lab 3: Descriptive Statistics",
    "section": "Task 8",
    "text": "Task 8\n\n⏱️ Estimated time: 4 minutes\n\nGenerate a plot of the function \\(f(x)=x-x^2 \\sin(x)\\) between \\(x=−10\\) and \\(x=10\\).\nExperiment around with the number of values generated using np.linspace(start, stop, num) to ensure your plot is relatively smooth.\nBe sure to include axis labels; also, change the color of the graph to red. Your final plot should look something like this:"
  },
  {
    "objectID": "files/worksheets/worksheet3.html",
    "href": "files/worksheets/worksheet3.html",
    "title": "PSTAT 5A Practice Worksheet 3",
    "section": "",
    "text": "Instructions and Overview\n⏰ Time Allocation:\n\nSection A (Warm-up): 8 minutes\nSection B (Intermediate): 15 minutes\nSection C (Advanced): 12 minutes\nSection D (Review): 15 minutes\nTotal: 50 minutes\n\n\n📝 Important Instructions:\n\nUse the formulas provided for guidance\nRound final answers to 4 decimal places unless otherwise specified\nIdentify your approach before calculating\nUse calculator as needed\n\n\n\n📚 Key Formulas Reference:\nBasic Probability:\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\cdot P(B_i)\\)\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)\\)\n\nCounting:\n\nMultiplication Rule: If a procedure consists of \\(k\\) steps, with \\(n_1\\) ways for step 1, \\(n_2\\) for step 2, …, \\(n_k\\) for step \\(k\\), then total ways: \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\)\nFactorial: \\(n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\)\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\n\n\n\n\nSection A: Probability\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions\nEach row in the table below is a proposed grade distribution for a class. Identify each as a valid or invalid probability distribution, and explain your reasoning.\n\n\n\nClass\nA\nB\nC\nD\nF\n\n\n\n\n(a)\n0.3\n0.3\n0.3\n0.2\n0.1\n\n\n(b)\n0\n0\n1\n0\n0\n\n\n(c)\n0.3\n0.3\n0.3\n0\n0\n\n\n(d)\n0.3\n0.5\n0.2\n0.1\n-0.1\n\n\n(e)\n0.2\n0.4\n0.2\n0.1\n0.1\n\n\n(f)\n0\n-0.1\n1.1\n0\n0\n\n\n\n\nWork Space:\n\n\n\nSection B: Permutations and Combination\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations\nA cybersecurity team needs to create a secure access protocol.\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\n\n\n\n\n\n\nTip\n\n\n\nSince letters must come before digits, think of this as two separate arrangement problems:\n\nFirst, arrange the 3 letters in the first 3 positions\nThen, arrange the 3 digits in the last 3 positions\nUse the multiplication principle to combine these results\n\n\n\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\n\n\n\n\n\n\nTip\n\n\n\nSince order doesn’t matter, this is a combination problem. Ask yourself:\n\nAre we arranging people in specific positions, or just selecting a group?\nWhich formula should you use: \\(P(n,r)\\) or \\(C(n,r)\\)?\n\n\n\n\nWork Space:\n\n\n\nSection C: Conditional Probability\n⏱️ Estimated time: 12 minutes\n\nProblem C1: Drawing Cards (Without Replacement)\nYou draw two cards, one after the other, from a standard 52-card deck without putting the first card back. Let\nA = \\(\\{\\text{“first card is a heart”}\\}\\),\nB = \\(\\{\\text{“second card is an ace”}\\}\\).\n\nP(A)\n\\(P\\bigl(A\\text{ and }B\\bigr)\\)\n\\(P\\bigl(B\\mid A\\bigr)\\)\nP(B)\nCompare your answers in (3) vs. (4). Why are they different (or the same)? What does this tell you about drawing cards without replacement?\n\n\nWork Space:\n\n\n\nSection D: Conditional Probability\n⏱️ Estimated time: 15 minutes\n\nProblem D1: Advanced Counting with Restrictions\nA restaurant offers a prix fixe menu where customers must choose:\n\n1 appetizer from 6 options\n1 main course from 8 options\n1 dessert from 5 options\n\nHowever, there are restrictions:\n\nIf you choose the seafood appetizer, you cannot choose the vegetarian main course\nIf you choose the chocolate dessert, you must choose either the beef or chicken main course (3 of the 8 main courses)\n\nPart (a): How many valid meal combinations are possible?\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\n\nWork Space:"
  },
  {
    "objectID": "files/worksheets/worksheet3_sln.html",
    "href": "files/worksheets/worksheet3_sln.html",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "",
    "text": "📚 Key Formulas Reference:\nBasic Probability:\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\cdot P(B_i)\\)\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)\\)\n\nCounting:\n\nMultiplication Rule: If a procedure consists of \\(k\\) steps, with \\(n_1\\) ways for step 1, \\(n_2\\) for step 2, …, \\(n_k\\) for step \\(k\\), then total ways: \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\)\nFactorial: \\(n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\)\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\n\n\n\nSection A: Probability - SOLUTIONS\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions - SOLUTION\nFor a valid probability distribution, two conditions must be met:\n\nAll probabilities must be non-negative (≥ 0)\nThe sum of all probabilities must equal 1\n\nAnalysis:\n(a) Invalid \\[\\text{Sum} = 0.3 + 0.3 + 0.3 + 0.2 + 0.1 = 1.2 &gt; 1\\] The probabilities sum to more than 1, violating condition 2.\n(b) Valid \\[\\text{Sum} = 0 + 0 + 1 + 0 + 0 = 1\\] All probabilities are non-negative and sum to 1. This represents a class where everyone receives a C.\n(c) Invalid \\[\\text{Sum} = 0.3 + 0.3 + 0.3 + 0 + 0 = 0.9 &lt; 1\\] The probabilities sum to less than 1, violating condition 2.\n(d) Invalid Contains \\(P(F) = -0.1 &lt; 0\\) Although the sum would equal 1.0, the probability for grade F is negative, violating condition 1.\n(e) Valid \\[\\text{Sum} = 0.2 + 0.4 + 0.2 + 0.1 + 0.1 = 1.0\\] All probabilities are non-negative and sum to 1.\n(f) Invalid Contains \\(P(B) = -0.1 &lt; 0\\) Although the sum equals 1.0, the probability for grade B is negative, violating condition 1.\n\n\n\nSection B: Permutations and Combinations - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations - SOLUTION\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\nSolution: Since letters must come before digits, we have a fixed structure: LLL DDD\nStep 1: Arrange 3 letters in the first 3 positions\n\nThis is a permutation: \\(P(3,3) = \\frac{3!}{(3-3)!} = \\frac{3!}{0!} = 3! = 6\\) ways\n\nStep 2: Arrange 3 digits in the last 3 positions\n\nThis is a permutation: \\(P(3,3) = \\frac{3!}{(3-3)!} = \\frac{3!}{0!} = 3! = 6\\) ways\n\nStep 3: Apply multiplication principle \\[\\text{Total passwords} = 6 \\times 6 = \\boxed{36 \\text{ passwords}}\\]\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\nSolution: Since order doesn’t matter, this is a combination problem.\n\\[C(12,4) = \\binom{12}{4} = \\frac{12!}{4!(12-4)!} = \\frac{12!}{4! \\cdot 8!}\\]\n\\[= \\frac{12 \\times 11 \\times 10 \\times 9}{4 \\times 3 \\times 2 \\times 1} = \\frac{11,880}{24} = \\boxed{495 \\text{ ways}}\\]\n\n\n\nSection C: Conditional Probability - SOLUTIONS\n⏱️ Estimated time: 12 minutes\n\nProblem C1: Drawing Cards (Without Replacement) - SOLUTION\nGiven Information:\n\nStandard 52-card deck\nDrawing two cards without replacement\n\\(A = \\{\\text{\"first card is a heart\"}\\}\\)\n\\(B = \\{\\text{\"second card is an ace\"}\\}\\)\n\nSolution:\n1. P(A)\nThere are 13 hearts in a 52-card deck. \\[P(A) = \\frac{13}{52} = \\boxed{\\frac{1}{4} = 0.2500}\\]\n2. P(A and B)\nWe need both events to occur: first card is a heart AND second card is an ace.\nCase 1: First card is the Ace of Hearts - \\(P(\\text{1st card is Ace of Hearts}) = \\frac{1}{52}\\)\n\n\\(P(\\text{2nd card is an ace | 1st card is Ace of Hearts}) = \\frac{3}{51}\\) (3 aces left)\n\\(P(\\text{Case 1}) = \\frac{1}{52} \\times \\frac{3}{51} = \\frac{3}{2652}\\)\n\nCase 2: First card is a non-ace heart - \\(P(\\text{1st card is non-ace heart}) = \\frac{12}{52}\\) (12 non-ace hearts)\n\n\\(P(\\text{2nd card is an ace | 1st card is non-ace heart}) = \\frac{4}{51}\\) (4 aces left)\n\\(P(\\text{Case 2}) = \\frac{12}{52} \\times \\frac{4}{51} = \\frac{48}{2652}\\)\n\n\\[P(A \\text{ and } B) = \\frac{3}{2652} + \\frac{48}{2652} = \\frac{51}{2652} = \\boxed{\\frac{1}{52} = 0.0192}\\]\n3. P(B|A)\nUsing the definition of conditional probability: \\[P(B|A) = \\frac{P(A \\text{ and } B)}{P(A)} = \\frac{\\frac{1}{52}}{\\frac{1}{4}} = \\frac{1}{52} \\times \\frac{4}{1} = \\boxed{\\frac{4}{52} = \\frac{1}{13} = 0.0769}\\]\nAlternative approach: Given that the first card is a heart:\n\nIf it’s the Ace of Hearts: 3 aces remain out of 51 cards\nIf it’s a non-ace heart: 4 aces remain out of 51 cards\n\\(P(B|A) = \\frac{1}{13} \\times \\frac{3}{51} + \\frac{12}{13} \\times \\frac{4}{51} = \\frac{3 + 48}{13 \\times 51} = \\frac{51}{663} = \\frac{4}{52} = \\frac{1}{13}\\)\n\n4. P(B)\nUsing the Law of Total Probability. Let \\(A^c\\) = “first card is not a heart”\n\\[P(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)\\]\nWe know:\n\n\\(P(A) = \\frac{1}{4}\\) and \\(P(A^c) = \\frac{3}{4}\\)\n\\(P(B|A) = \\frac{1}{13}\\) (from part 3)\n\\(P(B|A^c) = \\frac{4}{51}\\) (if first card isn’t a heart, all 4 aces remain)\n\n\\[P(B) = \\frac{1}{13} \\times \\frac{1}{4} + \\frac{4}{51} \\times \\frac{3}{4} = \\frac{1}{52} + \\frac{12}{204} = \\frac{1}{52} + \\frac{3}{51}\\]\n\\[= \\frac{51 + 156}{52 \\times 51} = \\frac{207}{2652} = \\boxed{\\frac{4}{51} = 0.0784}\\]\n5. Comparison of P(B|A) vs P(B)\n\\[P(B|A) = \\frac{1}{13} = 0.0769\\] \\[P(B) = \\frac{4}{51} = 0.0784\\]\nAnalysis: \\(P(B|A) &lt; P(B)\\)\nExplanation: The probability of getting an ace on the second draw is slightly lower when we know the first card is a heart compared to when we don’t know anything about the first card. This happens because:\n\nWhen the first card is a heart, there’s a \\(\\frac{1}{13}\\) chance it’s the Ace of Hearts, removing one ace from the deck\nThis makes it slightly less likely to draw an ace on the second draw\nThis demonstrates dependence - the events are not independent because drawing without replacement creates dependence between successive draws\n\n\n\n\nSection D: Advanced Counting with Restrictions - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem D1: Advanced Counting with Restrictions - SOLUTION\nGiven:\n\n6 appetizer options (including 1 seafood)\n8 main course options (including 1 vegetarian, and 3 that are beef or chicken)\n5 dessert options (including 1 chocolate)\n\nRestrictions:\n\nSeafood appetizer → cannot choose vegetarian main course\nChocolate dessert → must choose beef or chicken main course (3 specific options)\n\nPart (a): How many valid meal combinations are possible?\nSolution using cases:\nCase 1: Seafood appetizer is chosen\n\n1 appetizer choice (seafood)\n7 main course choices (8 total minus 1 vegetarian)\n5 dessert choices (no restrictions)\nTotal: \\(1 \\times 7 \\times 5 = 35\\) combinations\n\nCase 2: Non-seafood appetizer + chocolate dessert\n\n5 appetizer choices (6 total minus 1 seafood)\n3 main course choices (only beef or chicken allowed with chocolate)\n1 dessert choice (chocolate)\nTotal: \\(5 \\times 3 \\times 1 = 15\\) combinations\n\nCase 3: Non-seafood appetizer + non-chocolate dessert\n\n5 appetizer choices (6 total minus 1 seafood)\n8 main course choices (no restrictions since no seafood appetizer)\n4 dessert choices (5 total minus 1 chocolate)\nTotal: \\(5 \\times 8 \\times 4 = 160\\) combinations\n\nTotal valid combinations: \\[35 + 15 + 160 = \\boxed{210 \\text{ combinations}}\\]\nVerification using complementary counting:\n\nTotal unrestricted combinations: \\(6 \\times 8 \\times 5 = 240\\)\nInvalid combinations to subtract:\n\nSeafood + vegetarian + any dessert: \\(1 \\times 1 \\times 5 = 5\\)\nNon-seafood + chocolate + non-beef/chicken: \\(5 \\times 5 \\times 1 = 25\\)\n\nValid combinations: \\(240 - 5 - 25 = 210\\) ✓\n\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\nSolution: From our case analysis, combinations with chocolate dessert come only from Case 2:\n\nCombinations with chocolate dessert: 15\nTotal valid combinations: 210\n\n\\[P(\\text{chocolate dessert}) = \\frac{15}{210} = \\frac{1}{14} = \\boxed{0.0714}\\]\nAlternative verification:\nWe can also calculate this directly:\n\nNon-seafood appetizers: 5 choices\nWith chocolate dessert, must choose from 3 main courses\nValid chocolate combinations: \\(5 \\times 3 = 15\\)\nProbability: \\(\\frac{15}{210} = \\frac{1}{14} = 0.0714\\) ✓\n\n\n\n\nSummary of Key Concepts\n\nProbability Distributions\n\nValid distributions require: all probabilities \\(\\geq 0\\) and sum \\(= 1\\)\nCheck both conditions systematically\n\n\n\nPermutations vs Combinations\n\nPermutations: Order matters, use \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: Order doesn’t matter, use \\(C(n,r) = \\frac{n!}{r!(n-r)!}\\)\nMultiplication principle: Combine independent choices\n\n\n\nConditional Probability\n\nWithout replacement: Creates dependence between events\nUse definition: \\(P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\)\nLaw of Total Probability: For calculating marginal probabilities\n\n\n\nAdvanced Counting\n\nCase analysis: Break complex problems into manageable parts\nHandle restrictions: Consider what’s allowed vs. not allowed\nVerification: Use complementary counting or direct calculation"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Class Schedule",
    "section": "",
    "text": "⚠️\n\nImportant: This schedule is subject to change. Please check back regularly for updates and announcements.\n\n\n\n\n\n\nLabs & Worksheets\n\n\n\n\n\nQuizzes & Exams\n\n\n\n\n\nLecture Materials\n\n\n\n\n\n\n\nAnonymous Feedback Survey\n\n\n\n\nWeek 1: Introduction & Descriptive Statistics\n\n\n\n\n1\n\n\n6/23\n\n\nIntroduction\n\n\nLab 1\nlab1 Solutions\n\n\nWorksheet 1\n\n\n\n\n\n\n\n6/24\n\n\nNo class (lecture canceled)\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n6/25\n\n\nDescriptive Statistics I\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n6/26\n\n\nDescriptive Statistics II\nLinear Transformations (Worksheet 1 Q3)\n\n\n\n\n\n\n\n—\n\n\n\n\n\n\nWeek 2: Probability Foundations\n\n\n\n\n2\n\n\n6/30\n\n\nDescriptive Statistics II (Continued)\n\n\nLab2\nLab2 Notebook\nlab2 Solutions\n\n\nWorksheet 2\nWorksheet 2 Solutions\n\n\n\n\n\n\n\n7/01\n\n\nIntro to Probability\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/02\n\n\nIntro to Probability (Continued)\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/03\n\n\nConditional Probability, Independence, & Bayes Theorem\n\n\n—\n\n\n—\n\n\n\n\n\n\nWeek 3: Conditional Probability, Counting & Random Variables \n\n\n\n\n3\n\n\n7/07\n\n\nConditional Probability & Bayes Theorem\n\n\nLab3\nLab3 Solutions\n\n\nWorksheet 3\nWorksheet 3 Solutions\n\n\n\n\n\n\n\n7/08\n\n\nIntro to Counting & Permutations\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/09\n\n\nIntro to Counting & Combinations\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/10\n\n\nDiscrete Random Variables\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/11\n\n\nQuiz 1 (Weeks 1–2)\n\n\nQuiz 1 Solutions\n\n\n—\n\n\n\n\n\n\nWeek 4: Random Variables & Distributions - Confidence Intervals\n\n\n\n\n4\n\n\n7/14\n\n\nContinuous Random Variables & Distributions\n\n\nLab 4\nlab 4 Solutions\n\n\nWorksheet 4\nWorksheet 4 Solutions\n\n\n\n\n\n\n\n7/15\n\n\nContinuous Random Variables & Distributions Continued\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/16\n\n\nContinuous Random Variables & Distributions Wrap Up & Exercises\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/17\n\n\nRandom Variables, Sampling & Intro to Confidence Intervals\n\n\n—\n\n\n—\n\n\n\n\n\n\nWeek 5: Inference & Statistical Modeling\n\n\n\n\n5\n\n\n7/21\n\n\nSampling Principles and Strategies\n\n\nLab 5\nLab5 Solutions\n\n\nWorksheet 5 \n\n\n\n\n\n\n\n\n7/22\n\n\nConfidence Intervals (Means & Proportions)\n\nZ-table, t-table\n\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/23\n\n\nIntro to Hypothesis Testing\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/24\n\n\nHypothesis Testing Continued\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/25\n\n\nQuiz 2 (Weeks 3–4)\n\n\n—\n\n\n—\n\n\n\n\n\n\nWeek 6: Regression & Course Wrap-Up\n\n\n\n\n6\n\n\n7/28\n\n\nRegression Analysis\n\n\nLab 6\n\n\nWorksheet 6\n\n\n\n\n\n\n\n7/29\n\n\nRegression Diagnostics, Sampling\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/30\n\n\nWrap-Up\n\n\n—\n\n\n—\n\n\n\n\n\n\n\n7/31\n\n\nQuiz 3 (Weeks 5–6)\n\n\n—\n\n\n—"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Download the full syllabus as a PDF\n\n\n\n\n📚\n\n\nCourse Information\n\n\n\n\n\nLecture Time\n\n\nM/W/T/R 8:00 AM–9:30 AM\n\n\n\n\nLecture Hall\n\n\nHSSB 1173\n\n\n\n\nSections\n\n\nAs scheduled on GOLD (see Canvas for Zoom links)\n\n\n\n\nEmail\n\n\nnmathlouthi@ucsb.edu\n\n\n\n\nOffice\n\n\nEllison Hall 5829\n\n\n\n\nOffice Hours\n\n\nThursdays 11:00 AM–12:00 PM (via Zoom or by appointment)\n\n\n\n\n\nNote: Zoom links are posted on the Canvas page for the class.\n\n\n\n\n\n\n👥\n\n\nTeaching Assistants\n\n\n\n\n\nSL\n\n\nSummer Le\n\n\nsle@ucsb.edu\n\n\n\n\nMH\n\n\nMingzhu He\n\n\nmingzhuhe@ucsb.edu\n\n\n\n\n\nEmail policy: Include [PSTAT 5A] in your subject. Allow 24–48 hours for a reply (avoid weekends).\n\n\n\n\n\n\n🎯\n\n\nCourse Description\n\n\nThis introductory course covers the foundations of statistical thinking, including data description, probability, and inference. Students will learn how to summarize data, compute basic probabilities, and make informed decisions using statistical tools.\n\nStudent Learning Objectives\nBy the end of this course, you will be able to:\n\nSummarize data using descriptive statistics\nUnderstand fundamental probability rules and distributions\nConduct basic inferential procedures (confidence intervals, hypothesis tests)\nInterpret results and communicate findings\n\n\n\n\n\n\n📖\n\n\nCourse Materials\n\n\n\n\n\nCanvas\n\n\nAnnouncements, Zoom links, and grades (canvas.ucsb.edu)\n\n\n\n\nCalculator\n\n\nScientific calculator for in-class and quiz work\n\n\n\n\nComputer\n\n\nUse our JupyterHub instance\n\n\n\n\nRecommended Texts\n\n\nOpenIntro Statistics (free online)\nThink Stats by Allen Downey (free online)\n\n\n\n\n\n\n\n📅\n\n\nClass Schedule\n\n\n\n\nNote: For the most up-to-date details, please visit the Class Schedule tab on our website: Class Schedule\n\n\n\n\n\n\n📊\n\n\nGrading\n\n\n\n\nGrade Breakdown:\n\n\n\nLecture attendance: 5%\nSection attendance: 5%\nQuiz 1: 30%\nQuiz 2: 30%\nQuiz 3: 30%\n\n\n\nGrading Scale:\n\n\n\n\nA Grades\nB Grades\nC Grades\nD/F Grades\n\n\n\n\nA+: 97–100\nB+: 87–89\nC+: 77–79\nD+: 67–69\n\n\nA: 93–96\nB: 83–86\nC: 73–76\nD: 60–66\n\n\nA–: 90–92\nB–: 80–82\nC–: 70–72\nF: &lt; 60\n\n\n\n\n\n\nGrades round to the nearest whole number (e.g., 89.7 → 90).\n\n\n\n\n\n\n📝\n\n\nQuizzes\n\n\n\n\n\n1\n\n\n\nQuiz 1: Weeks 1–2\n\n\nJuly 11th\n\n\nCovers introduction, descriptive statistics and Intro to Probability\n\n\n\n\n\n2\n\n\n\nQuiz 2: Weeks 3–4\n\n\nJuly 25th\n\n\nCovers Conditional Probability, Counting & Random Variables (Discrete & Continuous)\n\n\n\n\n\n3\n\n\n\nQuiz 3: Weeks 5–6\n\n\nJuly 31st\n\n\nCovers Confidence Intervals & Hypothesis testing\n\n\n\n\n\n\nFormat: Multiple choice & short answer (open book)\nPlatform: Canvas /-or Gradescope\nAvailability: Fridays 7 AM–12 AM (1‑hour limit)\nMake‑up policy: Notify within 48 h; documentation required.\n\n\n\n\n\n\n🎯\n\n\nHow to Succeed\n\n\n\nAttend lectures & sections\nEngage actively & ask questions\nUse office hours for help\n\n\nClassroom Expectations\nRespect peers & TAs. Stay engaged. Seek support if needed.\n\n\nCommunication Guidelines\n\nUse UCSB email with clear subject\nAllow 24–48 h for replies\nUse office hours or appointments\n\n\n\n\n\n\n🛡️\n\n\nAcademic Integrity\n\n\nDo your own work. Cite sources properly. See:\n\nAcademic Integrity Policy\nStudent Conduct Code\n\n\n\n\n\n🤝\n\n\nStudent Resources\n\n\n\n\n🦽 DSP & Accommodations Disability services and accommodations\n\n\n📚 CLAS Campus Learning Assistance Services\n\n\n🏥 Student Health Health and wellness services\n\n\n🍎 Basic Needs Food security and basic needs support\n\n\n💚 CAPS Counseling & Psychological Services\n\n\n🎓 EOP Educational Opportunity Program\n\n\n👥 ONDAS First-Generation Support\n\n\n📋 Undocumented Services Support for undocumented students\n\n\n🔄 Transfer Center Transfer student support\n\n\n\n\n\n\n📅\n\n\nImportant Dates\n\n\n\n\n\nAdd w/o Code\n\n\nJune 29\n\n\n\n\nDrop w/ Refund\n\n\nJune 29\n\n\n\n\nAdd w/ Code\n\n\nJuly 3\n\n\n\n\nDrop Course\n\n\nJuly 9\n\n\n\n\nChange Grade Option\n\n\nAugust 1"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html",
    "href": "files/lecture_notes/lecture9/lecture9.html",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "",
    "text": "By the end of this session, you will be able to:\n\nDefine what a random variable is\nDistinguish between different types of random variables\nIdentify examples of random variables in your field of study\nConnect probability concepts to real-world applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#welcome-to-lecture-7",
    "href": "files/lecture_notes/lecture9/lecture9.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness\n Figure: A random variable maps outcomes (e.g., die rolls) to numbers."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture9/lecture9.html#todays-learning-objectives",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this session, you will be able to:\n\nDefine what a random variable is\nDistinguish between different types of random variables\nIdentify examples of random variables in your field of study\nConnect probability concepts to real-world applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#what-is-a-random-variable",
    "href": "files/lecture_notes/lecture9/lecture9.html#what-is-a-random-variable",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\n\n\nA random variable (r.v.) is a function that assigns numerical values to the outcomes of a random experiment\nNotation: Usually denoted by capital letters (X, Y, Z)\nIt’s a bridge between the sample space and real numbers\nThink of it as a “rule” that translates outcomes into numbers\n\n\n\nKey Point: It’s not actually “random”, it’s a deterministic function applied to random outcomes!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#why-use-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#why-use-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Why Use Random Variables?",
    "text": "Why Use Random Variables?\nRandom variables allow us to:\n\nQuantify outcomes numerically\nCalculate means, variances, and other statistics\nModel real-world phenomena mathematically\nMake predictions and decisions\nCompare different random processes\n\n Figure: Examples of random variables in real life (height, test scores, etc.).\n\nExamples: Height, test scores, number of defects, wait times, stock prices"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#types-of-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#types-of-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\nDiscrete Random Variable: - Takes on a countable number of values - Can list all possible values - Examples: dice rolls, number of emails, quiz scores\nContinuous Random Variable: - Takes on uncountably many values (intervals) - Cannot list all possible values\n- Examples: height, weight, time, temperature\n\n\n\n\n\ngraph TD\n  A[Discrete: 0, 1, 2, ...] --|Gaps|--&gt; D[Examples: Dice, Emails]\n  B[Continuous: Any real value] --|No gaps|--&gt; C[Examples: Height, Time]\n\n\n\n\n\n\n\nToday we focus on discrete random variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#examples-of-discrete-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#examples-of-discrete-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Examples of Discrete Random Variables",
    "text": "Examples of Discrete Random Variables\n\\(X\\) = Number of heads in 3 coin flips - Possible values: \\(\\{0, 1, 2, 3\\}\\)\n\\(Y\\) = Number of customers in an hour - Possible values: \\(\\{0, 1, 2, 3, \\ldots\\}\\)\n\\(Z\\) = Score on a 10-question quiz - Possible values: \\(\\{0, 1, 2, \\ldots, 10\\}\\)\n Figure: Example of a discrete random variable: number of heads in coin flips.\n\nNotice: All values are integers with gaps between them"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#probability-mass-function-pmf",
    "href": "files/lecture_notes/lecture9/lecture9.html#probability-mass-function-pmf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nThe probability mass function of a discrete random variable \\(X\\) is:\n\\[P(X = x) = \\text{probability that } X \\text{ takes the value } x\\]\nProperties of PMF: 1. \\(P(X = x) \\geq 0\\) for all \\(x\\) 2. \\(\\sum_{\\text{all } x} P(X = x) = 1\\)\n\n\n\n\n\n%% Bar chart for PMF of a fair die\n%% (Use Quarto's mermaid bar chart for illustration)\nbar\n    title PMF of a Fair Die\n    x-axis 1 2 3 4 5 6\n    y-axis Probability\n    1: 0.1667\n    2: 0.1667\n    3: 0.1667\n    4: 0.1667\n    5: 0.1667\n    6: 0.1667\n\n\n\n\n\n\n\nNotation: Sometimes written as \\(p(x)\\) or \\(f(x)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#pmf-example-fair-die",
    "href": "files/lecture_notes/lecture9/lecture9.html#pmf-example-fair-die",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Fair Die",
    "text": "PMF Example: Fair Die\nLet \\(X\\) = outcome of rolling a fair six-sided die\n\\[P(X = x) = \\begin{cases}\n\\frac{1}{6} & \\text{if } x \\in \\{1, 2, 3, 4, 5, 6\\} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n Figure: Bar chart of PMF for a fair die.\n\nVerification: - All probabilities ≥ 0 ✓ - Sum = \\(6 \\times \\frac{1}{6} = 1\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#pmf-example-two-coin-flips",
    "href": "files/lecture_notes/lecture9/lecture9.html#pmf-example-two-coin-flips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Two Coin Flips",
    "text": "PMF Example: Two Coin Flips\nLet \\(X\\) = number of heads in two coin flips\nSample space: \\(\\{HH, HT, TH, TT\\}\\)\n\n\n\n\n\nbar\n    title PMF: Number of Heads in 2 Coin Flips\n    x-axis 0 1 2\n    y-axis Probability\n    0: 0.25\n    1: 0.5\n    2: 0.25\n\n\n\n\n\n\n\n\n\n\n\\(x\\)\nOutcomes\n\\(P(X = x)\\)\n\n\n\n\n0\nTT\n1/4\n\n\n1\nHT, TH\n2/4 = 1/2\n\n\n2\nHH\n1/4\n\n\n\nVerification: \\(\\frac{1}{4} + \\frac{1}{2} + \\frac{1}{4} = 1\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-1",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-1",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\nA box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let \\(X\\) = number of red balls drawn.\nFind the PMF of \\(X\\).\n\nSolution: \\(X\\) can be 0, 1, or 2\n\\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{2}{2}}{\\binom{5}{2}} = \\frac{1 \\times 1}{10} = \\frac{1}{10}\\)\n\\(P(X = 1) = \\frac{\\binom{3}{1}\\binom{2}{1}}{\\binom{5}{2}} = \\frac{3 \\times 2}{10} = \\frac{6}{10}\\)\n\\(P(X = 2) = \\frac{\\binom{3}{2}\\binom{2}{0}}{\\binom{5}{2}} = \\frac{3 \\times 1}{10} = \\frac{3}{10}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#cumulative-distribution-function-cdf",
    "href": "files/lecture_notes/lecture9/lecture9.html#cumulative-distribution-function-cdf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function of a random variable \\(X\\) is:\n\\[F(x) = P(X \\leq x)\\]\nProperties of CDF: 1. \\(F(x)\\) is non-decreasing 2. \\(\\lim_{x \\to -\\infty} F(x) = 0\\) 3. \\(\\lim_{x \\to \\infty} F(x) = 1\\) 4. \\(F(x)\\) is right-continuous\n\n\n\n\n\n%% Step function for CDF of two coin flips\n%% (Mermaid doesn't support step plots, so use a line chart for illustration)\nline\n    title CDF: Number of Heads in 2 Coin Flips\n    x-axis -1 0 1 2 3\n    y-axis F(x)\n    -1: 0\n    0: 0.25\n    1: 0.75\n    2: 1\n    3: 1"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#cdf-example-two-coin-flips",
    "href": "files/lecture_notes/lecture9/lecture9.html#cdf-example-two-coin-flips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "CDF Example: Two Coin Flips",
    "text": "CDF Example: Two Coin Flips\nFrom our previous example where \\(X\\) = number of heads:\n\\[F(x) = \\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n\\frac{1}{4} & \\text{if } 0 \\leq x &lt; 1 \\\\\n\\frac{3}{4} & \\text{if } 1 \\leq x &lt; 2 \\\\\n1 & \\text{if } x \\geq 2\n\\end{cases}\\]\n\nKey insight: CDF is a step function for discrete random variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#relationship-between-pmf-and-cdf",
    "href": "files/lecture_notes/lecture9/lecture9.html#relationship-between-pmf-and-cdf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Relationship Between PMF and CDF",
    "text": "Relationship Between PMF and CDF\nFrom PMF to CDF: \\(F(x) = \\sum_{k \\leq x} P(X = k)\\)\nFrom CDF to PMF: \\(P(X = k) = F(k) - F(k^-)\\)\nwhere \\(F(k^-)\\) is the limit from the left\n\nUseful CDF Properties: - \\(P(X &gt; x) = 1 - F(x)\\) - \\(P(a &lt; X \\leq b) = F(b) - F(a)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-2",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-2",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\nUsing the red balls example from Problem 1, find:\n\n\\(F(0.5)\\)\n\\(F(1)\\)\n\n\\(P(X &gt; 1)\\)\n\\(P(0.5 &lt; X \\leq 1.5)\\)\n\n\nSolutions: a) \\(F(0.5) = P(X \\leq 0.5) = P(X = 0) = \\frac{1}{10}\\) b) \\(F(1) = P(X \\leq 1) = P(X = 0) + P(X = 1) = \\frac{1}{10} + \\frac{6}{10} = \\frac{7}{10}\\) c) \\(P(X &gt; 1) = 1 - F(1) = 1 - \\frac{7}{10} = \\frac{3}{10}\\) d) \\(P(0.5 &lt; X \\leq 1.5) = F(1.5) - F(0.5) = \\frac{7}{10} - \\frac{1}{10} = \\frac{6}{10}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#expected-value-mean",
    "href": "files/lecture_notes/lecture9/lecture9.html#expected-value-mean",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value (Mean)",
    "text": "Expected Value (Mean)\nThe expected value of a discrete random variable \\(X\\) is:\n\\[E[X] = \\mu = \\sum_{\\text{all } x} x \\cdot P(X = x)\\]\n Figure: Expected value as the long-run average of repeated coin flips."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#expected-value-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#expected-value-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value Example",
    "text": "Expected Value Example\nFor two coin flips where \\(X\\) = number of heads:\n\\[E[X] = 0 \\cdot P(X = 0) + 1 \\cdot P(X = 1) + 2 \\cdot P(X = 2)\\]\n\\[= 0 \\cdot \\frac{1}{4} + 1 \\cdot \\frac{1}{2} + 2 \\cdot \\frac{1}{4} = 0 + \\frac{1}{2} + \\frac{1}{2} = 1\\]\n\nMakes sense: On average, we expect 1 head in 2 coin flips"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-3",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-3",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\nFind the expected value for the red balls example (Problem 1).\n\nSolution: \\[E[X] = 0 \\cdot \\frac{1}{10} + 1 \\cdot \\frac{6}{10} + 2 \\cdot \\frac{3}{10}\\]\n\\[= 0 + \\frac{6}{10} + \\frac{6}{10} = \\frac{12}{10} = 1.2\\]\nOn average, we expect to draw 1.2 red balls"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#properties-of-expected-value",
    "href": "files/lecture_notes/lecture9/lecture9.html#properties-of-expected-value",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Properties of Expected Value",
    "text": "Properties of Expected Value\nLinearity of Expectation: 1. \\(E[c] = c\\) (constant) 2. \\(E[cX] = c \\cdot E[X]\\) (scaling) 3. \\(E[X + Y] = E[X] + E[Y]\\) (additivity) 4. \\(E[aX + bY + c] = aE[X] + bE[Y] + c\\)\n\nImportant: Property 3 holds even if \\(X\\) and \\(Y\\) are dependent!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#expected-value-of-functions",
    "href": "files/lecture_notes/lecture9/lecture9.html#expected-value-of-functions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value of Functions",
    "text": "Expected Value of Functions\nFor a function \\(g(X)\\) of a random variable \\(X\\):\n\\[E[g(X)] = \\sum_{\\text{all } x} g(x) \\cdot P(X = x)\\]\n\nCommon example: \\(g(X) = X^2\\)\n\\[E[X^2] = \\sum_{\\text{all } x} x^2 \\cdot P(X = x)\\]\nNote: Generally \\(E[g(X)] \\neq g(E[X])\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#variance",
    "href": "files/lecture_notes/lecture9/lecture9.html#variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Variance",
    "text": "Variance\nThe variance of a random variable \\(X\\) measures spread around the mean:\n\\[\\text{Var}(X) = \\sigma^2 = E[(X - \\mu)^2]\\]\n Figure: Variance measures the spread of possible values around the mean."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#variance-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#variance-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Variance Example",
    "text": "Variance Example\nFor two coin flips where \\(X\\) = number of heads, \\(E[X] = 1\\):\nFirst, find \\(E[X^2]\\): \\[E[X^2] = 0^2 \\cdot \\frac{1}{4} + 1^2 \\cdot \\frac{1}{2} + 2^2 \\cdot \\frac{1}{4} = 0 + \\frac{1}{2} + 1 = \\frac{3}{2}\\]\n\nThen: \\(\\text{Var}(X) = E[X^2] - (E[X])^2 = \\frac{3}{2} - 1^2 = \\frac{1}{2}\\)\nStandard deviation: \\(\\sigma = \\sqrt{\\frac{1}{2}} \\approx 0.707\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#properties-of-variance",
    "href": "files/lecture_notes/lecture9/lecture9.html#properties-of-variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Properties of Variance",
    "text": "Properties of Variance\n\n\\(\\text{Var}(c) = 0\\) (constants have no variability)\n\\(\\text{Var}(cX) = c^2 \\text{Var}(X)\\) (scaling by \\(c\\) scales variance by \\(c^2\\))\n\\(\\text{Var}(X + c) = \\text{Var}(X)\\) (shifting doesn’t change spread)\nIf \\(X\\) and \\(Y\\) are independent: \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\)\n\n\nNote: Property 4 requires independence, unlike expectation!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-4",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-4",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\nFor the red balls example, find the variance and standard deviation.\nWe found \\(E[X] = 1.2\\). First find \\(E[X^2]\\):\n\n\\[E[X^2] = 0^2 \\cdot \\frac{1}{10} + 1^2 \\cdot \\frac{6}{10} + 2^2 \\cdot \\frac{3}{10}\\] \\[= 0 + \\frac{6}{10} + \\frac{12}{10} = \\frac{18}{10} = 1.8\\]\n\\[\\text{Var}(X) = 1.8 - (1.2)^2 = 1.8 - 1.44 = 0.36\\]\n\\[\\sigma = \\sqrt{0.36} = 0.6\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#bernoulli-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#bernoulli-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nA Bernoulli random variable models a single trial with two outcomes:\n\nSuccess (1) with probability \\(p\\)\nFailure (0) with probability \\(1-p\\)\n\n\n\n\n\n\nbar\n    title Bernoulli PMF (p=0.7)\n    x-axis 0 1\n    y-axis Probability\n    0: 0.3\n    1: 0.7"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#bernoulli-properties",
    "href": "files/lecture_notes/lecture9/lecture9.html#bernoulli-properties",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bernoulli Properties",
    "text": "Bernoulli Properties\nFor \\(X \\sim \\text{Bernoulli}(p)\\):\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\nStandard deviation: \\(\\sigma = \\sqrt{p(1-p)}\\)\n\nIntuition: - Mean \\(p\\) makes sense: probability of success - Variance maximized when \\(p = 0.5\\) (most uncertainty)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#binomial-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#binomial-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial random variable counts successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\)\n Figure: Binomial experiment: sequence of independent trials.\n\n\n\n\n\nbar\n    title Binomial PMF (n=5, p=0.5)\n    x-axis 0 1 2 3 4 5\n    y-axis Probability\n    0: 0.031\n    1: 0.156\n    2: 0.312\n    3: 0.312\n    4: 0.156\n    5: 0.031"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#binomial-properties",
    "href": "files/lecture_notes/lecture9/lecture9.html#binomial-properties",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Binomial Properties",
    "text": "Binomial Properties\nFor \\(X \\sim \\text{Binomial}(n, p)\\):\nMean: \\(E[X] = np\\)\nVariance: \\(\\text{Var}(X) = np(1-p)\\)\nStandard deviation: \\(\\sigma = \\sqrt{np(1-p)}\\)\n\nDerivation: Sum of \\(n\\) independent Bernoulli\\((p)\\) random variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#binomial-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#binomial-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Binomial Example",
    "text": "Binomial Example\nA multiple choice quiz has 10 questions, each with 4 options. A student guesses randomly on each question. Let \\(X\\) = number of correct answers.\n\\(X \\sim \\text{Binomial}(10, 0.25)\\)\nWhat’s the probability of getting exactly 3 correct?\n\n\\[P(X = 3) = \\binom{10}{3} (0.25)^3 (0.75)^7\\] \\[= 120 \\times 0.015625 \\times 0.1335 \\approx 0.250\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-5",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-5",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 5",
    "text": "Practice Problem 5\nFor the quiz example:\n\nWhat’s the expected number of correct answers?\nWhat’s the standard deviation?\nWhat’s the probability of getting at least 4 correct?\n\n\nSolutions: a) \\(E[X] = np = 10 \\times 0.25 = 2.5\\) b) \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{10 \\times 0.25 \\times 0.75} = \\sqrt{1.875} \\approx 1.37\\) c) \\(P(X \\geq 4) = 1 - P(X \\leq 3) = 1 - [P(X=0) + P(X=1) + P(X=2) + P(X=3)]\\) Use binomial table or calculator: \\(P(X \\geq 4) \\approx 0.224\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#geometric-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#geometric-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nA geometric random variable counts the number of trials until the first success\n Figure: Geometric experiment: waiting for the first success.\n\n\n\n\n\nbar\n    title Geometric PMF (p=0.3)\n    x-axis 1 2 3 4 5\n    y-axis Probability\n    1: 0.3\n    2: 0.21\n    3: 0.147\n    4: 0.1029\n    5: 0.072"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#geometric-properties",
    "href": "files/lecture_notes/lecture9/lecture9.html#geometric-properties",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Geometric Properties",
    "text": "Geometric Properties\nFor \\(X \\sim \\text{Geometric}(p)\\):\nMean: \\(E[X] = \\frac{1}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\)\nMemoryless property: \\(P(X &gt; s + t | X &gt; s) = P(X &gt; t)\\)\n\nIntuition: If \\(p = 0.1\\), expect to wait \\(\\frac{1}{0.1} = 10\\) trials on average"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#geometric-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#geometric-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Geometric Example",
    "text": "Geometric Example\nA basketball player has a 60% free throw percentage. What’s the probability they make their first shot on the 3rd attempt?\n\\(X \\sim \\text{Geometric}(0.6)\\)\n\n\\[P(X = 3) = (1-0.6)^{3-1} \\times 0.6 = (0.4)^2 \\times 0.6 = 0.16 \\times 0.6 = 0.096\\]\nExpected number of attempts: \\(E[X] = \\frac{1}{0.6} \\approx 1.67\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-6",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-6",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\nA quality control inspector tests items until finding the first defective one. If 5% of items are defective:\n\nWhat’s the probability the first defective item is the 10th one tested?\nWhat’s the expected number of items tested?\nWhat’s the probability of testing more than 20 items?\n\n\nSolutions: a) \\(P(X = 10) = (0.95)^9 \\times 0.05 \\approx 0.0315\\) b) \\(E[X] = \\frac{1}{0.05} = 20\\) items c) \\(P(X &gt; 20) = (0.95)^{20} \\approx 0.358\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#poisson-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#poisson-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nA Poisson random variable counts events occurring in a fixed interval when events happen at a constant average rate\n Figure: Poisson process: random events in time.\n\n\n\n\n\nbar\n    title Poisson PMF (λ=3)\n    x-axis 0 1 2 3 4 5 6\n    y-axis Probability\n    0: 0.0498\n    1: 0.1494\n    2: 0.2240\n    3: 0.2240\n    4: 0.1680\n    5: 0.1008\n    6: 0.0504"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#poisson-properties",
    "href": "files/lecture_notes/lecture9/lecture9.html#poisson-properties",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Poisson Properties",
    "text": "Poisson Properties\nFor \\(X \\sim \\text{Poisson}(\\lambda)\\):\nMean: \\(E[X] = \\lambda\\)\nVariance: \\(\\text{Var}(X) = \\lambda\\)\nStandard deviation: \\(\\sigma = \\sqrt{\\lambda}\\)\n\nUnique property: Mean equals variance!\nApproximation: Binomial\\((n, p)\\) ≈ Poisson\\((np)\\) when \\(n\\) large, \\(p\\) small"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#poisson-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#poisson-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Poisson Example",
    "text": "Poisson Example\nA call center receives an average of 3 calls per minute. What’s the probability of receiving exactly 5 calls in the next minute?\n\\(X \\sim \\text{Poisson}(3)\\)\n\n\\[P(X = 5) = \\frac{3^5 e^{-3}}{5!} = \\frac{243 \\times e^{-3}}{120} \\approx \\frac{243 \\times 0.0498}{120} \\approx 0.101\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-7",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\nFor the call center example:\n\nWhat’s the probability of no calls in a minute?\nWhat’s the probability of at most 2 calls?\nIn a 2-minute period, what’s the expected number of calls?\n\n\nSolutions: a) \\(P(X = 0) = \\frac{3^0 e^{-3}}{0!} = e^{-3} \\approx 0.0498\\) b) \\(P(X \\leq 2) = P(X=0) + P(X=1) + P(X=2) \\approx 0.0498 + 0.1494 + 0.2240 = 0.423\\) c) For 2 minutes: \\(Y \\sim \\text{Poisson}(6)\\), so \\(E[Y] = 6\\) calls"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Hypergeometric Distribution",
    "text": "Hypergeometric Distribution\nA hypergeometric random variable counts successes when sampling without replacement from a finite population\n Figure: Hypergeometric sampling: drawing balls from a box without replacement."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-properties",
    "href": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-properties",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Hypergeometric Properties",
    "text": "Hypergeometric Properties\nFor \\(X \\sim \\text{Hypergeometric}(N, K, n)\\):\nMean: \\(E[X] = n \\cdot \\frac{K}{N}\\)\nVariance: \\(\\text{Var}(X) = n \\cdot \\frac{K}{N} \\cdot \\frac{N-K}{N} \\cdot \\frac{N-n}{N-1}\\)\n\nConnection to Binomial: When \\(N\\) is large relative to \\(n\\), hypergeometric ≈ binomial\\((n, K/N)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-example",
    "href": "files/lecture_notes/lecture9/lecture9.html#hypergeometric-example",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Hypergeometric Example",
    "text": "Hypergeometric Example\nA batch of 20 light bulbs contains 3 defective ones. If 5 bulbs are randomly selected, what’s the probability exactly 1 is defective?\n\\(X \\sim \\text{Hypergeometric}(20, 3, 5)\\)\n\n\\[P(X = 1) = \\frac{\\binom{3}{1}\\binom{17}{4}}{\\binom{20}{5}} = \\frac{3 \\times 2380}{15504} = \\frac{7140}{15504} \\approx 0.461\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-8",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-8",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 8",
    "text": "Practice Problem 8\nFor the light bulb example:\n\nWhat’s the expected number of defective bulbs in the sample?\nWhat’s the probability of no defective bulbs?\nWhat’s the probability of at least 2 defective bulbs?\n\n\nSolutions: a) \\(E[X] = 5 \\times \\frac{3}{20} = 0.75\\) bulbs b) \\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{17}{5}}{\\binom{20}{5}} = \\frac{6188}{15504} \\approx 0.399\\) c) \\(P(X \\geq 2) = 1 - P(X = 0) - P(X = 1) \\approx 1 - 0.399 - 0.461 = 0.140\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#comparing-distributions",
    "href": "files/lecture_notes/lecture9/lecture9.html#comparing-distributions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Comparing Distributions",
    "text": "Comparing Distributions\n\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nMean\nVariance\nUse Case\n\n\n\n\nBernoulli\n\\(p\\)\n\\(p\\)\n\\(p(1-p)\\)\nSingle trial\n\n\nBinomial\n\\(n, p\\)\n\\(np\\)\n\\(np(1-p)\\)\nFixed trials\n\n\nGeometric\n\\(p\\)\n\\(1/p\\)\n\\((1-p)/p^2\\)\nWait for success\n\n\nPoisson\n\\(\\lambda\\)\n\\(\\lambda\\)\n\\(\\lambda\\)\nCount events\n\n\nHypergeometric\n\\(N, K, n\\)\n\\(n(K/N)\\)\nComplex\nSample w/o replacement"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#technology-and-software",
    "href": "files/lecture_notes/lecture9/lecture9.html#technology-and-software",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Technology and Software",
    "text": "Technology and Software\nCalculators: - Binomial: binompdf(), binomcdf() - Poisson: poissonpdf(), poissoncdf()\nR: - dbinom(), pbinom(), rbinom() - dpois(), ppois(), rpois() - dgeom(), pgeom(), rgeom()\nPython: - scipy.stats.binom, scipy.stats.poisson - numpy.random for simulation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture9/lecture9.html#problem-solving-strategy",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\nIdentify the scenario: What type of process?\nCheck assumptions: Independence, constant probability, etc.\nChoose distribution: Match scenario to distribution\nIdentify parameters: \\(n\\), \\(p\\), \\(\\lambda\\), etc.\nCalculate probabilities: Use PMF, CDF, or technology\nInterpret results: Does the answer make sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#real-world-applications",
    "href": "files/lecture_notes/lecture9/lecture9.html#real-world-applications",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nQuality Control: - Binomial for defect rates - Hypergeometric for batch sampling\nReliability Engineering: - Geometric for time to failure - Poisson for system failures\nEpidemiology: - Binomial for disease spread - Poisson for rare events\nFinance: - Poisson for market events - Binomial for option pricing"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-9",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-9",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 9",
    "text": "Practice Problem 9\nA website receives an average of 2 orders per minute. Assuming orders follow a Poisson process:\n\nWhat’s the probability of exactly 3 orders in a minute?\nWhat’s the probability of no orders in 30 seconds?\nWhat’s the probability of more than 5 orders in 2 minutes?\n\n\nSolutions: a) \\(X \\sim \\text{Poisson}(2)\\): \\(P(X = 3) = \\frac{2^3 e^{-2}}{3!} \\approx 0.180\\) b) \\(Y \\sim \\text{Poisson}(1)\\): \\(P(Y = 0) = e^{-1} \\approx 0.368\\) c) \\(Z \\sim \\text{Poisson}(4)\\): \\(P(Z &gt; 5) = 1 - P(Z \\leq 5) \\approx 0.215\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-preview",
    "href": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-preview",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Central Limit Theorem Preview",
    "text": "Central Limit Theorem Preview\nFor large \\(n\\), many discrete distributions approach normal:\nBinomial: \\(X \\sim \\text{Binomial}(n, p)\\) → \\(N(np, np(1-p))\\) when \\(np &gt; 5\\) and \\(n(1-p) &gt; 5\\)\nPoisson: \\(X \\sim \\text{Poisson}(\\lambda)\\) → \\(N(\\lambda, \\lambda)\\) when \\(\\lambda &gt; 10\\)\n\nThis connection will be crucial for hypothesis testing and confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-mistakes",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-mistakes",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\nWrong distribution choice: Check assumptions carefully\nParameter confusion: Is it \\(n\\), \\(p\\), or \\(\\lambda\\)?\nInclusion errors: “At least 3” vs “More than 3”\nIndependence assumption: Sampling with/without replacement\nTechnology errors: pdf vs cdf functions"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-10",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-10",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 10",
    "text": "Practice Problem 10\nA multiple choice test has 20 questions with 5 options each. A student knows the answers to 12 questions and guesses on the rest.\n\nWhat’s the expected score?\nWhat’s the probability of scoring at least 15?\nWhat’s the standard deviation of the score?\n\n\nSolution: Let \\(X\\) = known correct, \\(Y\\) = guessed correct - \\(X = 12\\) (deterministic) - \\(Y \\sim \\text{Binomial}(8, 0.2)\\) - Total score \\(S = X + Y = 12 + Y\\)\n\n\\(E[S] = 12 + E[Y] = 12 + 8(0.2) = 13.6\\)\n\\(P(S \\geq 15) = P(Y \\geq 3) \\approx 0.203\\)\n\n\\(\\sigma_S = \\sigma_Y = \\sqrt{8(0.2)(0.8)} = \\sqrt{1.28} \\approx 1.13\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#extensions-and-advanced-topics",
    "href": "files/lecture_notes/lecture9/lecture9.html#extensions-and-advanced-topics",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Extensions and Advanced Topics",
    "text": "Extensions and Advanced Topics\nNegative Binomial: Number of trials to get \\(r\\) successes\nMultinomial: Extension of binomial to more than 2 categories\nCompound Distributions: Sums of random variables\nGenerating Functions: Advanced technique for deriving properties\n\nThese topics appear in advanced probability courses"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#historical-context",
    "href": "files/lecture_notes/lecture9/lecture9.html#historical-context",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Historical Context",
    "text": "Historical Context\nJacob Bernoulli (1654-1705): Bernoulli trials and law of large numbers\nSiméon Denis Poisson (1781-1840): Poisson distribution and approximation\nAbraham de Moivre (1667-1754): Normal approximation to binomial\nModern Applications: Computer science, machine learning, bioinformatics"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-student-questions",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-student-questions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Student Questions",
    "text": "Common Student Questions\nQ: “How do I choose between binomial and hypergeometric?” A: Use binomial for sampling with replacement, hypergeometric without\nQ: “When can I use Poisson approximation?” A: When \\(n\\) is large, \\(p\\) is small, and \\(np\\) is moderate\nQ: “Why does Poisson have mean = variance?” A: Mathematical property arising from its derivation as a limit\nQ: “How do I know if trials are independent?” A: Check if outcome of one trial affects others"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture9/lecture9.html#key-formulas-summary",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\nExpected Value: \\(E[X] = \\sum x \\cdot P(X = x)\\)\nVariance: \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\)\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nGeometric: \\(P(X = k) = (1-p)^{k-1} p\\)\nPoisson: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#looking-ahead",
    "href": "files/lecture_notes/lecture9/lecture9.html#looking-ahead",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Hypothesis Testing\nTopics we’ll cover:\n\nNull and alternative hypotheses\nTest statistics and p-values\nType I and Type II errors\n\n\nConnection: Confidence intervals and hypothesis tests are two sides of the same statistical inference coin"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#study-tips",
    "href": "files/lecture_notes/lecture9/lecture9.html#study-tips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Study Tips",
    "text": "Study Tips\n\nMaster the basics: PMF, CDF, expectation, variance\nLearn distribution characteristics: When to use each one\nPractice with technology: Get comfortable with calculators/software\nWork real problems: Apply distributions to actual scenarios\nCheck your intuition: Do answers make practical sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#final-thoughts",
    "href": "files/lecture_notes/lecture9/lecture9.html#final-thoughts",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nDiscrete random variables are fundamental to: - Modeling real-world phenomena - Making statistical inferences - Understanding probability theory - Building more complex models\n\nKey insight: Random variables transform unpredictable outcomes into predictable patterns"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#questions",
    "href": "files/lecture_notes/lecture9/lecture9.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions?",
    "text": "Questions?\n\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and Statistical Significance"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#bonus-law-of-large-numbers",
    "href": "files/lecture_notes/lecture9/lecture9.html#bonus-law-of-large-numbers",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bonus: Law of Large Numbers",
    "text": "Bonus: Law of Large Numbers\nAs \\(n\\) increases, the sample mean approaches the expected value:\n\\[\\bar{X}_n = \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\to E[X]\\]\nExample: Flip a coin 1000 times - proportion of heads will be close to 0.5\nThis justifies our interpretation of expected value as “long-run average”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#bonus-simulation",
    "href": "files/lecture_notes/lecture9/lecture9.html#bonus-simulation",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bonus: Simulation",
    "text": "Bonus: Simulation\nMonte Carlo Method: Use computer simulation to estimate probabilities\n# Simulate 10,000 binomial random variables\nX &lt;- rbinom(10000, size=10, prob=0.3)\nmean(X)  # Should be close to 10*0.3 = 3\nvar(X)   # Should be close to 10*0.3*0.7 = 2.1\nSimulation helps verify theoretical results and solve complex problems"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html",
    "href": "files/lecture_notes/lecture6/lecture6.html",
    "title": "PSTAT 5A: Counting",
    "section": "",
    "text": "The art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-objectives",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-objectives",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#mutually-exclusive-vs.-independent",
    "href": "files/lecture_notes/lecture6/lecture6.html#mutually-exclusive-vs.-independent",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent",
    "text": "Mutually Exclusive vs. Independent\n\nMutually Exclusive (left): the circles A and B do not overlap, so \\(P(A\\cap B)=0\\).\nIndependent (right): the circles overlap, and we’ve sized the intersection so that \\(P(A\\cap B)=P(A)\\,P(B)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#mutually-exclusive-vs.-independent-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#mutually-exclusive-vs.-independent-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent Example",
    "text": "Mutually Exclusive vs. Independent Example\n\n\n\nDraw a single card from a 52-card deck:\n\nLet A={“draw an Ace”}, so P(A)=4/52.\nLet B={“draw a King”}, so P(B)=4/52.\n\nQ: What is \\(P(A\\cap B)\\) ?\n\n\n\n\n\nSolution. They’re disjoint (you can’t draw an Ace and a King), so \\(P(A\\cap B) = 0\\).\nBut \\(P(A)\\,P(B) = \\frac{4}{52}\\times\\frac{4}{52} = \\frac{16}{2704} \\neq 0\\).\nHence, \\(P(A\\cap B)\\neq P(A)P(B)\\), so they’re not independent."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#multiplication-rule",
    "href": "files/lecture_notes/lecture6/lecture6.html#multiplication-rule",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nGeneral case: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nIndependent events: \\(P(A \\cap B) = P(A) \\times P(B)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#tree-diagrams",
    "href": "files/lecture_notes/lecture6/lecture6.html#tree-diagrams",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\n\n🎯 Definition Tree diagrams help visualize sequential events and calculate probabilities."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#tree-diagram-examples",
    "href": "files/lecture_notes/lecture6/lecture6.html#tree-diagram-examples",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagram Examples",
    "text": "Tree Diagram Examples"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-problem-2",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-problem-2",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\n\n\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement.\n\nWhat’s the probability both balls are red?\nWhat’s the probability the first is red and second is blue?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{both red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)\n\\(P(\\text{red then blue}) = \\frac{5}{8} \\times \\frac{3}{7} = \\frac{15}{56}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#law-of-total-probability",
    "href": "files/lecture_notes/lecture6/lecture6.html#law-of-total-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n🎯 Definition\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space, then:\n\\[P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#law-of-total-probability-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\n\nA factory has two machines:\n\nMachine 1: Produces 60% of items, 5% defective\nMachine 2: Produces 40% of items, 3% defective\n\nQ: What’s the overall probability an item is defective?\n\n\n\nSolution. \\(P(\\text{defective}) = P(D|M_1)P(M_1) + P(D|M_2)P(M_2)\\)\n\\(= 0.05 \\times 0.6 + 0.03 \\times 0.4 = 0.03 + 0.012 = 0.042\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem",
    "href": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n🎯 Definition \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nThis allows us to “reverse” conditional probabilities\nNamed after Thomas Bayes (1701-1761)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-components",
    "href": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-components",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\n\\(A,B\\): Events\n\\(P(A|B)\\): Posterior probability - what we want to find\n\\(P(B|A)\\): Likelihood - given \\(A\\), probability of observing \\(B\\)\n\\(P(A)\\): Prior probability - initial probability of \\(A\\)\n\\(P(B)\\): Marginal probability - total probability of \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Example",
    "text": "Bayes’ Theorem Example\n\n\n\nMedical test for a disease: 1\n\nDisease affects 1% of population\nTest is 95% accurate for sick people\nTest is 90% accurate for healthy people\n\nQ:If someone tests positive, what’s the probability they have the disease?\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-solution",
    "href": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-solution",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution",
    "text": "Bayes’ Theorem Solution\nLet:\n\n\\(D\\): Person has disease\n\\(T^+\\): Test is positive\n\nGiven:\n\n\\(P(D) = 0.01\\)\n\\(P(T^+|D) = 0.95\\)\n\\(P(T^-|D^c) = 0.90\\), so \\(P(T^+|D^c) = 0.10\\)\n\n\n\nSolution. \\(P(T^+) = P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\)\n\\(= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.1085\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-solution-cont.",
    "href": "files/lecture_notes/lecture6/lecture6.html#bayes-theorem-solution-cont.",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution (cont.)",
    "text": "Bayes’ Theorem Solution (cont.)\n\\[P(D|T^+) = \\frac{P(T^+|D) \\times P(D)}{P(T^+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\]\n\n\n\nSurprising result: Even with a positive test, there’s only an 8.8% chance of having the disease!\nThis is due to the low base rate of the disease"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#common-probability-mistakes",
    "href": "files/lecture_notes/lecture6/lecture6.html#common-probability-mistakes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nConfusing \\(P(A|B)\\) with \\(P(B|A)\\)\n\n\nProsecutor’s fallacy is a specific error in interpreting conditional probabilities. Confusing\n\\(P(\\text{Evidence}\\mid\\text{Innocent})\n\\quad\\text{with}\\quad\nP(\\text{Innocent}\\mid\\text{Evidence})\\).\nEx: OJ Simpson Case 1\n\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#common-probability-mistakes-1",
    "href": "files/lecture_notes/lecture6/lecture6.html#common-probability-mistakes-1",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nAssuming independence when events are dependent\nIgnoring base rates (as in the medical test example)\n\n\nBase rate fallacy is when you ignore or underweight the prior probability \\(P(H)\\) of a hypothesis, focusing only on the new evidence \\(E\\).\n\n\nDouble counting in union calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-problem-3",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-problem-3",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nTwo fair dice are rolled. Find:\n\n\\(P(\\text{sum} = 7)\\)\n\\(P(\\text{sum} = 7 | \\text{first die shows 3})\\)\nAre these events independent?\n\n\n\n\nSolution. \n\n6 ways out of 36: \\(P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\nGiven first die is 3, need second die to be 4: \\(P(\\text{sum} = 7 | \\text{first} = 3) = \\frac{1}{6}\\)\nYes, they’re independent since \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#counting-and-probability",
    "href": "files/lecture_notes/lecture6/lecture6.html#counting-and-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\nSometimes we need to count outcomes:\n\nMultiplication Principle: If task 1 can be done in \\(m\\) ways and task 2 in \\(n\\) ways, both can be done in \\(m \\times n\\) ways\n\n\nPermutations: Arrangements where order matters \\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\nCombinations: Selections where order doesn’t matter \\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#counting-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#counting-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting Example",
    "text": "Counting Example\n\nQ: How many ways can you arrange 5 people in a row?\n\n\n\nSolution. This is a permutation: \\(P(5,5) = 5! = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#probability-with-counting",
    "href": "files/lecture_notes/lecture6/lecture6.html#probability-with-counting",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Probability with Counting",
    "text": "Probability with Counting\n\nExample: A committee of 3 people is chosen from 8 people (5 women, 3 men). What’s the probability all 3 are women?\n\n\n\nSolution. Total ways to choose 3 from 8: \\(\\binom{8}{3} = 56\\)\nWays to choose 3 women from 5: \\(\\binom{5}{3} = 10\\)\nProbability: \\(\\frac{10}{56} = \\frac{5}{28}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#real-world-applications",
    "href": "files/lecture_notes/lecture6/lecture6.html#real-world-applications",
    "title": "PSTAT 5A: Counting",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nLottery:\n\nPowerball: Choose 5 from 69, then 1 from 26\nOdds: \\(\\frac{1}{\\binom{69}{5} \\times 26} \\approx \\frac{1}{292,000,000}\\)\n\nCryptography:\n\nKey space size determines security\nRSA encryption relies on large number factorization\n\nSports Tournaments:\n\nMarch Madness bracket: \\(2^{63}\\) possible outcomes\nRound-robin tournaments: \\(\\binom{n}{2}\\) games"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture6/lecture6.html#key-formulas-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\n\n\n\n\n\n\nSummary of Key Formulas\n\n\n\n\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\nWith repetition: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)\nInclusion-Exclusion: \\(|A \\cup B| = |A| + |B| - |A \\cap B|\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture6/lecture6.html#problem-solving-strategy",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\nIdentify the sample space and events\nDetermine if events are independent or mutually exclusive\nChoose the appropriate rule or formula\nCalculate step by step\nCheck if your answer makes sense"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA bag contains 4 red, 3 blue, and 2 green marbles. Three marbles are drawn without replacement.\nFind the probability that: a) All three are red b) No two are the same color c) At least one is blue"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4-solutions",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4-solutions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4 Solutions",
    "text": "Practice Problem 4 Solutions\n\nSolution. \n\nAll red: \\(\\frac{4}{9} \\times \\frac{3}{8} \\times \\frac{2}{7} = \\frac{24}{504} = \\frac{1}{21}\\)\nDifferent colors: \\(\\frac{4 \\times 3 \\times 2}{9 \\times 8 \\times 7} \\times 3! = \\frac{24 \\times 6}{504} = \\frac{144}{504} = \\frac{2}{7}\\)\nAt least one blue: \\(1 - P(\\text{no blue}) = 1 - \\frac{6 \\times 5 \\times 4}{9 \\times 8 \\times 7} = 1 - \\frac{120}{504} = \\frac{384}{504} = \\frac{16}{21}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#common-questions",
    "href": "files/lecture_notes/lecture6/lecture6.html#common-questions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Questions",
    "text": "Common Questions\n\nQ1.: “Why isn’t \\(P(A \\cup B) = P(A) + P(B)\\) always?”\nA: We’d double-count outcomes in both events\nQ2.: “How do I know if events are independent?”\nA: Check if \\(P(A|B) = P(A)\\) or if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nQ3.: “When do I use Bayes’ theorem?”\nA: When you want to “reverse” a conditional probability\n\n\nQ3 note (Bayes Example)\n\nForward: I know my test picks up disease 95% of the time ⇒ \\(P(+\\mid D)=0.95\\).\nReverse: I want the chance I really have the disease when the test is positive ⇒ \\(P(D\\mid +)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#looking-ahead",
    "href": "files/lecture_notes/lecture6/lecture6.html#looking-ahead",
    "title": "PSTAT 5A: Counting",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\n\n\n\nNext Lecture\n\n\n\nNext lecture: Discrete Probability Distributions - Binomial distribution (using combinations!)\n\nHypergeometric distribution\nGeometric distribution\nExpected value and variance\n\nConnection: Today’s counting techniques are essential for probability calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#final-thoughts",
    "href": "files/lecture_notes/lecture6/lecture6.html#final-thoughts",
    "title": "PSTAT 5A: Counting",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nCounting is fundamental to:\n\nProbability calculations\nStatistical inference\nComputer algorithms\nScientific modeling\n\n\n\n\n\n\n\nKnow the Basics\n\n\n\nPermutations and combinations are the building blocks for advanced statistical concepts"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#questions",
    "href": "files/lecture_notes/lecture6/lecture6.html#questions",
    "title": "PSTAT 5A: Counting",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#resources",
    "href": "files/lecture_notes/lecture6/lecture6.html#resources",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#footnotes",
    "href": "files/lecture_notes/lecture6/lecture6.html#footnotes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)↩︎\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent.↩︎"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html",
    "title": "Linear Transformation Properties",
    "section": "",
    "text": "Given: Let \\(X = \\{x_i\\}_{i=1}^{n}\\) and define \\(Y = \\{y_i\\}_{i=1}^{n}\\) by \\(y_i = a x_i\\) for some fixed constant \\(a \\neq 0\\).\nProve the following relationships:\n\\[\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i, \\quad \\bar{Y} = a \\bar{X}, \\quad S_Y^2 = a^2 S_X^2\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#problem-statement",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#problem-statement",
    "title": "Linear Transformation Properties",
    "section": "Problem Statement",
    "text": "Problem Statement\n\nGiven: Let \\(X = \\{x_i\\}_{i=1}^{n}\\) and define \\(Y = \\{y_i\\}_{i=1}^{n}\\) by \\(y_i = a x_i\\) for some fixed constant \\(a \\neq 0\\).\nProve the following relationships:\n\\[\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i, \\quad \\bar{Y} = a \\bar{X}, \\quad S_Y^2 = a^2 S_X^2\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#understanding-the-notation",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#understanding-the-notation",
    "title": "Linear Transformation Properties",
    "section": "Understanding the Notation",
    "text": "Understanding the Notation\nSet Notation\n\n\\(X = \\{x_i\\}_{i=1}^{n}\\) means:\n\n\\(X\\) is a dataset containing \\(n\\) observations\nThe observations are labeled \\(x_1, x_2, x_3, \\ldots, x_n\\)\n\\(i\\) is an index that runs from 1 to \\(n\\)\nThis is read as: “X is the set of \\(x_i\\) for \\(i\\) from 1 to \\(n\\)”\n\n\nExamples:\n\nIf \\(n = 5\\): \\(X = \\{x_1, x_2, x_3, x_4, x_5\\}\\)\nIf \\(X = \\{2, 4, 6, 8, 10\\}\\), then \\(x_1 = 2, x_2 = 4, x_3 = 6, x_4 = 8, x_5 = 10\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#summation-notation",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#summation-notation",
    "title": "Linear Transformation Properties",
    "section": "Summation Notation",
    "text": "Summation Notation\n\n\\(\\sum_{i=1}^{n} x_i\\) means:\n\nAdd up all the \\(x_i\\) values\nStart with \\(i = 1\\) and go up to \\(i = n\\)\nThis equals: \\(x_1 + x_2 + x_3 + \\cdots + x_n\\)\n\n\nExample:\nIf \\(X = \\{2, 4, 6, 8, 10\\}\\), then: \\[\\sum_{i=1}^{5} x_i = x_1 + x_2 + x_3 + x_4 + x_5 = 2 + 4 + 6 + 8 + 10 = 30\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#linear-transformation",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#linear-transformation",
    "title": "Linear Transformation Properties",
    "section": "Linear Transformation",
    "text": "Linear Transformation\n\n\\(Y = \\{y_i\\}_{i=1}^{n}\\) where \\(y_i = a x_i\\) means:\n\nEach element of \\(Y\\) is obtained by multiplying the corresponding element of \\(X\\) by the constant \\(a\\)\nThis is called a linear transformation\n\\(a\\) is called the scaling factor\n\n\nExample:\nIf \\(X = \\{2, 4, 6, 8, 10\\}\\) and \\(a = 3\\), then:\n\n\\(y_1 = 3 \\times 2 = 6\\)\n\\(y_2 = 3 \\times 4 = 12\\)\n\\(y_3 = 3 \\times 6 = 18\\)\n\\(y_4 = 3 \\times 8 = 24\\)\n\\(y_5 = 3 \\times 10 = 30\\)\n\nSo \\(Y = \\{6, 12, 18, 24, 30\\}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#sample-mean-notation",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#sample-mean-notation",
    "title": "Linear Transformation Properties",
    "section": "Sample Mean Notation",
    "text": "Sample Mean Notation\n\nSample Mean: \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\\)\nThis means:\n\nAdd up all observations: \\(\\sum_{i=1}^{n} x_i\\)\nDivide by the number of observations: \\(n\\)\nThe “bar” over \\(X\\) indicates the mean\n\n\nExample:\nFor \\(X = \\{2, 4, 6, 8, 10\\}\\): \\[\\bar{X} = \\frac{1}{5}(2 + 4 + 6 + 8 + 10) = \\frac{30}{5} = 6\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#sample-variance-notation",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#sample-variance-notation",
    "title": "Linear Transformation Properties",
    "section": "Sample Variance Notation",
    "text": "Sample Variance Notation\n\nSample Variance: \\(S_X^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2\\)\nThis means:\n\nFor each observation, find its deviation from the mean: \\((x_i - \\bar{X})\\)\nSquare each deviation: \\((x_i - \\bar{X})^2\\)\nAdd up all squared deviations: \\(\\sum_{i=1}^{n} (x_i - \\bar{X})^2\\)\nDivide by \\((n-1)\\): This gives the sample variance"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-1-sum-relationship-1",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-1-sum-relationship-1",
    "title": "Linear Transformation Properties",
    "section": "Proof 1: Sum Relationship",
    "text": "Proof 1: Sum Relationship\n\nStep 1: Start with the definition of \\(y_i\\) \\[y_i = a x_i \\text{ for all } i = 1, 2, \\ldots, n\\]\nStep 2: Write out the sum of all \\(y_i\\) \\[\\sum_{i=1}^{n} y_i = y_1 + y_2 + y_3 + \\cdots + y_n\\]\nStep 3: Substitute the definition \\(y_i = a x_i\\) \\[\\sum_{i=1}^{n} y_i = a x_1 + a x_2 + a x_3 + \\cdots + a x_n\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-1-sum-relationship-continued",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-1-sum-relationship-continued",
    "title": "Linear Transformation Properties",
    "section": "Proof 1: Sum Relationship (continued)",
    "text": "Proof 1: Sum Relationship (continued)\n\n\n\nStep 4: Factor out the constant \\(a\\) \\[\\sum_{i=1}^{n} y_i = a(x_1 + x_2 + x_3 + \\cdots + x_n)\\]\nStep 5: Recognize the sum notation \\[\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i\\]\nTherefore: \\(\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i\\) ✓\n\n\n\nKey Property Used: Constants can be factored out of sums \\[\\sum_{i=1}^{n} (c \\cdot x_i) = c \\sum_{i=1}^{n} x_i\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#example-sum-relationship",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#example-sum-relationship",
    "title": "Linear Transformation Properties",
    "section": "Example: Sum Relationship",
    "text": "Example: Sum Relationship\n\nGiven: \\(X = \\{2, 4, 6\\}\\) and \\(a = 5\\)\nThen: \\(Y = \\{10, 20, 30\\}\\) (since \\(y_i = 5x_i\\))\nCheck our formula:\n\n\\(\\sum_{i=1}^{3} x_i = 2 + 4 + 6 = 12\\)\n\\(\\sum_{i=1}^{3} y_i = 10 + 20 + 30 = 60\\)\n\\(a \\sum_{i=1}^{3} x_i = 5 \\times 12 = 60\\) ✓\n\nVerification: \\(60 = 60\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-2-sample-mean-relationship-1",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-2-sample-mean-relationship-1",
    "title": "Linear Transformation Properties",
    "section": "Proof 2: Sample Mean Relationship",
    "text": "Proof 2: Sample Mean Relationship\n\nStep 1: Start with the definition of sample mean \\[\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\]\nStep 2: Use our result from Proof 1 We proved that \\(\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i\\)\nStep 3: Substitute this result \\[\\bar{Y} = \\frac{1}{n} \\cdot a \\sum_{i=1}^{n} x_i\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-2-sample-mean-relationship-continued",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-2-sample-mean-relationship-continued",
    "title": "Linear Transformation Properties",
    "section": "Proof 2: Sample Mean Relationship (continued)",
    "text": "Proof 2: Sample Mean Relationship (continued)\n\n\n\nStep 4: Rearrange the constants \\[\\bar{Y} = a \\cdot \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]\nStep 5: Recognize the definition of \\(\\bar{X}\\) \\[\\bar{Y} = a \\bar{X}\\]\nTherefore: \\(\\bar{Y} = a \\bar{X}\\) ✓\n\n\n\nKey Property Used: Constants can be moved outside of fractions \\[\\frac{1}{n} \\cdot a \\sum_{i=1}^{n} x_i = a \\cdot \\frac{1}{n} \\sum_{i=1}^{n} x_i\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#example-sample-mean-relationship",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#example-sample-mean-relationship",
    "title": "Linear Transformation Properties",
    "section": "Example: Sample Mean Relationship",
    "text": "Example: Sample Mean Relationship\n\nGiven: \\(X = \\{2, 4, 6\\}\\) and \\(a = 5\\)\nThen: \\(Y = \\{10, 20, 30\\}\\)\nCheck our formula:\n\n\\(\\bar{X} = \\frac{1}{3}(2 + 4 + 6) = \\frac{12}{3} = 4\\)\n\\(\\bar{Y} = \\frac{1}{3}(10 + 20 + 30) = \\frac{60}{3} = 20\\)\n\\(a \\bar{X} = 5 \\times 4 = 20\\) ✓\n\nVerification: \\(20 = 20\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-3-sample-variance-relationship-1",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-3-sample-variance-relationship-1",
    "title": "Linear Transformation Properties",
    "section": "Proof 3: Sample Variance Relationship",
    "text": "Proof 3: Sample Variance Relationship\n\nStep 1: Start with the definition of sample variance \\[S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{Y})^2\\]\nStep 2: Substitute \\(y_i = a x_i\\) and \\(\\bar{Y} = a \\bar{X}\\) \\[S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (a x_i - a \\bar{X})^2\\]\nStep 3: Factor out \\(a\\) from the parentheses \\[S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} [a(x_i - \\bar{X})]^2\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#proof-3-sample-variance-relationship-continued",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#proof-3-sample-variance-relationship-continued",
    "title": "Linear Transformation Properties",
    "section": "Proof 3: Sample Variance Relationship (continued)",
    "text": "Proof 3: Sample Variance Relationship (continued)\n\nStep 4: Use the property \\((ab)^2 = a^2 b^2\\) \\[S_Y^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} a^2 (x_i - \\bar{X})^2\\]\nStep 5: Factor out the constant \\(a^2\\) from the sum \\[S_Y^2 = \\frac{a^2}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2\\]\nStep 6: Recognize the definition of \\(S_X^2\\) \\[S_Y^2 = a^2 \\cdot \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{X})^2 = a^2 S_X^2\\]\nTherefore: \\(S_Y^2 = a^2 S_X^2\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#key-properties-used-in-variance-proof",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#key-properties-used-in-variance-proof",
    "title": "Linear Transformation Properties",
    "section": "Key Properties Used in Variance Proof",
    "text": "Key Properties Used in Variance Proof\n\nProperties Used:\n\nFactoring: \\(ax_i - a\\bar{X} = a(x_i - \\bar{X})\\)\nSquaring: \\([a(x_i - \\bar{X})]^2 = a^2(x_i - \\bar{X})^2\\)\nConstants in sums: \\(\\sum_{i=1}^{n} a^2 f(x_i) = a^2 \\sum_{i=1}^{n} f(x_i)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#example-sample-variance-relationship",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#example-sample-variance-relationship",
    "title": "Linear Transformation Properties",
    "section": "Example: Sample Variance Relationship",
    "text": "Example: Sample Variance Relationship\n\nGiven: \\(X = \\{1, 3, 5\\}\\) and \\(a = 2\\)\nCalculate \\(S_X^2\\):\n\n\\(\\bar{X} = \\frac{1+3+5}{3} = 3\\)\n\\(S_X^2 = \\frac{1}{2}[(1-3)^2 + (3-3)^2 + (5-3)^2] = \\frac{1}{2}[4 + 0 + 4] = 4\\)\n\nFor \\(Y = \\{2, 6, 10\\}\\): - \\(\\bar{Y} = \\frac{2+6+10}{3} = 6 = 2 \\times 3\\) ✓\n\n\\(S_Y^2 = \\frac{1}{2}[(2-6)^2 + (6-6)^2 + (10-6)^2] = \\frac{1}{2}[16 + 0 + 16] = 16\\)\n\nCheck: \\(a^2 S_X^2 = 2^2 \\times 4 = 16\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#complete-summary-of-results",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#complete-summary-of-results",
    "title": "Linear Transformation Properties",
    "section": "Complete Summary of Results",
    "text": "Complete Summary of Results\n\nFor the linear transformation \\(Y = \\{y_i\\}_{i=1}^{n}\\) where \\(y_i = a x_i\\):\n\nSum: \\(\\sum_{i=1}^{n} y_i = a \\sum_{i=1}^{n} x_i\\)\nMean: \\(\\bar{Y} = a \\bar{X}\\)\nVariance: \\(S_Y^2 = a^2 S_X^2\\)\n\nNote: The standard deviation relationship is \\(S_Y = |a| S_X\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#practical-implications",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#practical-implications",
    "title": "Linear Transformation Properties",
    "section": "Practical Implications",
    "text": "Practical Implications\n\n\nScaling Up (a &gt; 1):\n\nSums and means increase by factor \\(a\\)\nVariance increases by factor \\(a^2\\)\nData becomes more spread out\n\nExample: Converting inches to feet\n\nIf \\(a = 12\\), variance increases by \\(12^2 = 144\\)\n\n\n\nScaling Down (0 &lt; a &lt; 1):\n\nSums and means decrease by factor \\(a\\)\nVariance decreases by factor \\(a^2\\)\nData becomes less spread out\n\nExample: Converting dollars to cents\n\nIf \\(a = 0.01\\), variance decreases by \\((0.01)^2 = 0.0001\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#real-world-applications",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#real-world-applications",
    "title": "Linear Transformation Properties",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nTemperature Conversion:\n\nCelsius to Fahrenheit: \\(F = \\frac{9}{5}C + 32\\) (not linear!)\nCelsius to Kelvin: \\(K = C + 273.15\\) (not linear!)\nBut scaling: \\(C_{doubled} = 2C\\) is linear with \\(a = 2\\)\n\nUnit Conversions: - Meters to centimeters: \\(a = 100\\)\n\nDollars to cents: \\(a = 100\\)\nHours to minutes: \\(a = 60\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#why-these-properties-matter",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#why-these-properties-matter",
    "title": "Linear Transformation Properties",
    "section": "Why These Properties Matter",
    "text": "Why These Properties Matter\n\nStatistical Significance:\n\nStandardization: Converting to z-scores uses linear transformations\nUnit Changes: Results remain proportionally correct\nData Analysis: Understanding how transformations affect summary statistics\nModeling: Linear regression relies on these properties\n\nKey Insight: Linear transformations preserve the shape of the distribution while changing location and scale."
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#practice-problem",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#practice-problem",
    "title": "Linear Transformation Properties",
    "section": "Practice Problem",
    "text": "Practice Problem\n\nTry This: A dataset has mean \\(\\bar{X} = 15\\) and variance \\(S_X^2 = 9\\).\nIf we transform the data using \\(y_i = 3x_i - 2\\), what are the new mean and variance?\n\n\n\n\n\n\nTip\n\n\nHint: This is not a pure linear transformation! You need \\(y_i = 3x_i - 2 = 3(x_i - \\frac{2}{3})\\)\n\n\n\n\nAnswer:\n\nNew mean: \\(\\bar{Y} = 3(15) - 2 = 43\\)\nNew variance: \\(S_Y^2 = 3^2 \\times 9 = 81\\)\n(The constant \\(-2\\) doesn’t affect variance!)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#common-mistakes-to-avoid",
    "title": "Linear Transformation Properties",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n❌ Wrong: \\(S_Y^2 = a S_X^2\\)\n✅ Correct: \\(S_Y^2 = a^2 S_X^2\\)\nWhy: Variance involves squared deviations, so the scaling factor gets squared too.\n❌ Wrong: Adding constants affects variance\n✅ Correct: Only multiplication affects variance; addition only shifts the mean"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#conclusion",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#conclusion",
    "title": "Linear Transformation Properties",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat We Proved:\nFor the linear transformation \\(y_i = ax_i\\) with constant \\(a \\neq 0\\):\n\nSums scale linearly: Factor of \\(a\\)\nMeans scale linearly: Factor of \\(a\\)\n\nVariances scale quadratically: Factor of \\(a^2\\)\n\nKey Takeaway: Understanding these relationships is fundamental for:\n\nData transformations\nStatistical modeling\nUnit conversions\nStandardization procedures"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#questions",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#questions",
    "title": "Linear Transformation Properties",
    "section": "Questions?",
    "text": "Questions?\nKey Concepts Covered:\n\nSummation notation and indexing\nLinear transformations\nProperties of means and variances\nStep-by-step mathematical proofs\n\nNext Steps:\n\nApply to real datasets\nExplore non-linear transformations\nPractice with different scaling factors"
  },
  {
    "objectID": "files/lecture_notes/lecture3/q3_worksheet.html#additional-practice",
    "href": "files/lecture_notes/lecture3/q3_worksheet.html#additional-practice",
    "title": "Linear Transformation Properties",
    "section": "Additional Practice",
    "text": "Additional Practice\n\nExercise 1: If \\(X = \\{10, 20, 30, 40\\}\\) and \\(Y = \\{-5, -10, -15, -20\\}\\), what is the value of \\(a\\)?\nExercise 2: A dataset has \\(\\bar{X} = 50\\) and \\(S_X = 10\\). After transformation \\(y_i = 0.5x_i\\), find \\(\\bar{Y}\\) and \\(S_Y\\).\nExercise 3: Prove that if \\(y_i = ax_i + b\\) (adding a constant), then \\(S_Y^2 = a^2 S_X^2\\) (the constant doesn’t affect variance)."
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html",
    "href": "files/lecture_notes/lecture4/lecture4.html",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#sec-objectives",
    "href": "files/lecture_notes/lecture4/lecture4.html#sec-objectives",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#sec-prob",
    "href": "files/lecture_notes/lecture4/lecture4.html#sec-prob",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "What is Probability?",
    "text": "What is Probability?\n\n🎯 Definition\nProbability is a measure of the likelihood that an event will occur"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#probability-range",
    "href": "files/lecture_notes/lecture4/lecture4.html#probability-range",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Range",
    "text": "Probability Range\n\n\n\nRanges from 0 to 1 (or 0% to 100%)\n0: Event will never occur (impossible)\n1: Event will certainly occur (certain)\n0.5: Event has equal chance of occurring or not"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#three-ways-to-express-probability",
    "href": "files/lecture_notes/lecture4/lecture4.html#three-ways-to-express-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Three Ways to Express Probability",
    "text": "Three Ways to Express Probability\n\nFraction: \\(\\frac{1}{2}\\), \\(\\frac{3}{4}\\), \\(\\frac{2}{6}\\)\nDecimal: 0.5, 0.75, 0.33\nPercentage: 50%, 75%, 33%"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#example",
    "href": "files/lecture_notes/lecture4/lecture4.html#example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Example",
    "text": "Example\nWhen we roll a die, there are six possible outcomes:\n1, 2, 3, 4, 5, 6.\nThe probability of any of them turning up is 1/6 or 16%."
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#why-study-probability",
    "href": "files/lecture_notes/lecture4/lecture4.html#why-study-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Why Study Probability?",
    "text": "Why Study Probability?\nProbability helps us:\n\nMake decisions under uncertainty\nUnderstand random processes\nAnalyze data and draw conclusions\nModel real-world phenomena\nAssess risk and likelihood\n\n\n\nApplications: Weather forecasting, medical diagnosis, finance, quality control, gaming, insurance"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#random-experiments",
    "href": "files/lecture_notes/lecture4/lecture4.html#random-experiments",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Random Experiments",
    "text": "Random Experiments\nA random experiment is a process that:\n\nCan be repeated under similar conditions\nHas multiple possible outcomes\nThe outcome cannot be predicted with certainty\n\n\n\nExamples\n\n🪙 Flipping a coin\n🎲 Rolling a die\n🃏 Drawing a card from a deck\n💡 Measuring the lifetime of a light bulb"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#sample-space",
    "href": "files/lecture_notes/lecture4/lecture4.html#sample-space",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space\n\n🎯 Definition The sample space (denoted \\(S\\) or \\(\\Omega\\)) is the set of all possible outcomes of a random experiment"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#sample-space-examples",
    "href": "files/lecture_notes/lecture4/lecture4.html#sample-space-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Sample Space Examples",
    "text": "Sample Space Examples\n\nCoin flip: \\(S = \\{H, T\\}\\)\nTwo coin flips: \\(S = \\{HH, HT, TH, TT\\}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#types-of-sample-spaces",
    "href": "files/lecture_notes/lecture4/lecture4.html#types-of-sample-spaces",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Types of Sample Spaces",
    "text": "Types of Sample Spaces\n\n\nFinite Sample Space\n\nLimited number of outcomes\n\nExample: Rolling a die\n\n\n\n\n\n\n\n\n\n\n\nInfinite Sample Space\n\nUnlimited outcomes (countable or uncountable)\n\nExample: Measuring exact height of students"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#events",
    "href": "files/lecture_notes/lecture4/lecture4.html#events",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Events",
    "text": "Events\n\n🎯 Definition An event is a subset of the sample space\n\nSimple event: Contains exactly one outcome (Ex: \\(A = \\{3\\}\\) (rolling a 3))\nCompound event: Contains multiple outcomes (Ex: \\(B = \\{2, 4, 6\\}\\) (rolling an even number))"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#event-notation",
    "href": "files/lecture_notes/lecture4/lecture4.html#event-notation",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Event Notation",
    "text": "Event Notation\nFor a die roll with \\(S = \\{1, 2, 3, 4, 5, 6\\}\\):\n\n\\(A = \\{1, 3, 5\\}\\) (rolling an odd number)\n\\(B = \\{4, 5, 6\\}\\) (rolling 4 or higher)\n\\(C = \\{6\\}\\) (rolling a six)\n\n\n\nWe can describe events in words or using set notation"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#set-operations-overview",
    "href": "files/lecture_notes/lecture4/lecture4.html#set-operations-overview",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Set Operations Overview",
    "text": "Set Operations Overview\n\n\n\n\n🎯 Definition:\n\nSet: Collection of distinct objects\nUnion: A OR B occurs\n\nIntersection: A AND B occurs\nComplement: A does NOT occur\nSample Space: All possible outcomes"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#what-is-a-set",
    "href": "files/lecture_notes/lecture4/lecture4.html#what-is-a-set",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "What is a Set?",
    "text": "What is a Set?\n\n\n\n\n🎯 Definition: A collection of things that share common characteristics. They can be elements, members, objects or similar terms.\nExamples:\n\nSet of even numbers:\n\n{2, 4, 6, 8, …}\n\nSet of vowels: {a, e, i, o, u}"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#union-a-b",
    "href": "files/lecture_notes/lecture4/lecture4.html#union-a-b",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Union: A ∪ B",
    "text": "Union: A ∪ B\n\n\n\n\n🎯 Definition: Contains all set elements, including intersections.\nIn Probability: The event that A OR B occurs (or both).\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#intersection-a-b",
    "href": "files/lecture_notes/lecture4/lecture4.html#intersection-a-b",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Intersection: A ∩ B",
    "text": "Intersection: A ∩ B\n\n\n\n\n🎯 Definition: Area where two or more sets overlap.\nIn Probability: The event that A AND B occurs.\nProperties:\n\nAlways smaller than or equal to individual sets\nCan be empty (disjoint sets)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#absolute-complement-ac",
    "href": "files/lecture_notes/lecture4/lecture4.html#absolute-complement-ac",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Absolute Complement: \\(A^c\\)",
    "text": "Absolute Complement: \\(A^c\\)\n\n\n\n🎯 Definition: All elements that do not belong to the set.\nIn Probability: The event that A does NOT occur.\n\n\\(P(A^c) = 1 - P(A)\\)\n\n\nKey Property:\n\\(A \\cup A^c = S\\) (Sample Space)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#summary-table",
    "href": "files/lecture_notes/lecture4/lecture4.html#summary-table",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\nOperation\nSymbol\nMeaning\nProbability\n\n\n\n\nUnion\n\\(A \\cup B\\)\nOccurs if \\(A\\) or \\(B\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nIntersection\n\\(A \\cap B\\)\nOccurs if \\(A\\) and \\(B\\)\n\\(P(A \\cap B)\\)\n\n\nComplement\n\\(A^c\\)\nOccurs if \\(A\\) does not occur\n\\(P(A^c) = 1 - P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-commutative",
    "href": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-commutative",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Commutative",
    "text": "Probability Axioms: Commutative\n\nCommutative\n\\(A \\cup B = B \\cup A\\)\n\\(A \\cap B = B \\cap A\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-associative",
    "href": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-associative",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Associative",
    "text": "Probability Axioms: Associative\n\nAssociative\n\\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\)\n\\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-distributive",
    "href": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-distributive",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Distributive",
    "text": "Probability Axioms: Distributive\n\nDistributive\n\\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\n\\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-de-morgans-laws",
    "href": "files/lecture_notes/lecture4/lecture4.html#probability-axioms-de-morgans-laws",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: De Morgan’s Laws",
    "text": "Probability Axioms: De Morgan’s Laws\n\nDe Morgan’s Laws\n\\((A \\cup B)^c = A^c \\cap B^c\\)\n\\((A \\cap B)^c = A^c \\cup B^c\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#practice-examples",
    "href": "files/lecture_notes/lecture4/lecture4.html#practice-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Examples",
    "text": "Practice Examples\n\nExample 1: In a class of students:\n\nSet A = Students who like Math\nSet B = Students who like Science\n\nQ: What does A ∪ B represent?\n\n\n\nSolution. Students who like Math OR Science (or both)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#example-set-operations",
    "href": "files/lecture_notes/lecture4/lecture4.html#example-set-operations",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Example: Set Operations",
    "text": "Example: Set Operations\n\n\nFor die roll \\(S = \\{1, 2, 3, 4, 5, 6\\}\\):\n\n\\(A = \\{1, 3, 5\\}\\) (odd numbers)\n\\(B = \\{4, 5, 6\\}\\) (4 or higher)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind:\n\n\\(A \\cup B\\)\n\\(A \\cap B\\)\n\\(A^c\\)\n\n\n\n\n\nSolution. \n\n\\(A \\cup B = \\{1, 3, 4, 5, 6\\}\\)\n\\(A \\cap B = \\{5\\}\\)\n\\(A^c = \\{2, 4, 6\\}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#mutually-exclusive-events",
    "href": "files/lecture_notes/lecture4/lecture4.html#mutually-exclusive-events",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive (or disjoint) if they cannot occur simultaneously\n\\[A \\cap B = \\emptyset\\]\n\n\n\n\n\n\n\n\n\n\n\n\nWhen rolling a die\n\n\\(A = \\{1, 3, 5\\}\\) (odd)\n\\(B = \\{2, 4, 6\\}\\) (even)\n\n\\(A\\) and \\(B\\) are mutually exclusive"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#the-classical-definition-of-probability",
    "href": "files/lecture_notes/lecture4/lecture4.html#the-classical-definition-of-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Classical Definition of Probability",
    "text": "The Classical Definition of Probability\n\n🎯 Definition: For equally likely outcomes:\n\\[P(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total number of outcomes in } S}\\]\n\n\nProbability of rolling an even number on a fair die\n\n\n\\[P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#properties-of-probability",
    "href": "files/lecture_notes/lecture4/lecture4.html#properties-of-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nNon-negativity: \\(P(A) \\geq 0\\) for any event \\(A\\)\nNormalization: \\(P(S) = 1\\)\nAdditivity: If \\(A\\) and \\(B\\) are mutually exclusive, then\n\n\n\\[P(A \\cup B) = P(A) + P(B)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#the-complement-rule",
    "href": "files/lecture_notes/lecture4/lecture4.html#the-complement-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Complement Rule",
    "text": "The Complement Rule\n\n\\[P(A) + P(A^c) = 1\\]\n\\[P(A^c) = 1 - P(A)\\]\n\n\n\nIf the probability of rain is 0.3, what’s the probability of no rain?\n\n\n\n\nSolution. \\[P(\\text{no rain}) = 1 - P(\\text{rain}) = 1 - 0.3 = 0.7\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#questions",
    "href": "files/lecture_notes/lecture4/lecture4.html#questions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Conditional Probabilities & Independence"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4.html#resources",
    "href": "files/lecture_notes/lecture4/lecture4.html#resources",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms\nProbability Animations"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html",
    "href": "files/lecture_notes/lecture2/lecture2.html",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "",
    "text": "Introduction to Descriptive Statistics (10 minutes)\n\nWhat is descriptive statistics?\nWhy it matters before any modeling\n\nTypes of Data and Measurement Scales (15 minutes)\nMeasures of Central Tendency (35 minutes)\n\nMean (12 minutes)\nMedian (12 minutes)\nMode (11 minutes)\n\nPython Implementation (15 minutes)\nReal-world Applications and Interpretation (5 minutes)\n\n\nQuick ice-breaker: ask students for one dataset they’ve looked at recently and what first question they asked about it."
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#todays-agenda",
    "href": "files/lecture_notes/lecture2/lecture2.html#todays-agenda",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Today’s Agenda",
    "text": "Today’s Agenda\n\nIntroduction to Descriptive Statistics (10 minutes)\n\nWhat is descriptive statistics?\nWhy it matters before any modeling\n\nTypes of Data and Measurement Scales (15 minutes)\nMeasures of Central Tendency (35 minutes)\n\nMean (12 minutes)\nMedian (12 minutes)\nMode (11 minutes)\n\nPython Implementation (15 minutes)\nReal-world Applications and Interpretation (5 minutes)\n\n\nQuick ice-breaker: ask students for one dataset they’ve looked at recently and what first question they asked about it."
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#learning-objectives-los",
    "href": "files/lecture_notes/lecture2/lecture2.html#learning-objectives-los",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Learning Objectives (LOs)",
    "text": "Learning Objectives (LOs)\nBy the end of this lecture you should be able to:\n\nDefine descriptive statistics and explain its importance in data analysis (Section 1)\nDistinguish between different types of data and measurement scales (Section 2)\nCalculate and interpret measures of central tendency (mean, median, mode)(Section 3)\nUnderstand when to use each measure of central tendency\nApply Python to compute descriptive statistics(Section 9)\nInterpret basic descriptive statistics in real-world contexts(Section 10)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#statistics",
    "href": "files/lecture_notes/lecture2/lecture2.html#statistics",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Statistics",
    "text": "Statistics\n\nFacts are stubborn, but statistics are more pliable.\nMark Twain\n\nStatistics refers to the mathematics and techniques with which we understand data. 1\n\n\n\n(source)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#what-are-descriptive-statistics",
    "href": "files/lecture_notes/lecture2/lecture2.html#what-are-descriptive-statistics",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "What are Descriptive Statistics?",
    "text": "What are Descriptive Statistics?\n\nDescriptive statistics are numerical and graphical methods used to summarize, organize, and describe data in a meaningful way.\n\nPurpose of Descriptive Statistics\n\nSummarize large datasets into manageable information\nIdentify patterns and trends in data\nCommunicate findings clearly to others\nPrepare data for further analysis\nMake initial assessments about data quality"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#descriptive-vs.-inferential-statistics",
    "href": "files/lecture_notes/lecture2/lecture2.html#descriptive-vs.-inferential-statistics",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics\n\n\n\n\n\n\n\nDescriptive Statistics\nInferential Statistics\n\n\n\n\nDescribes what the data shows\nMakes predictions about populations\n\n\nSummarizes sample data\nUses sample data to make generalizations\n\n\nNo conclusions beyond the data\nDraws conclusions beyond the immediate data\n\n\nExamples: mean, median, graphs\nExamples: hypothesis testing, confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#descriptive-vs.-inferential-statistics-1",
    "href": "files/lecture_notes/lecture2/lecture2.html#descriptive-vs.-inferential-statistics-1",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Descriptive vs. Inferential Statistics",
    "text": "Descriptive vs. Inferential Statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#why-data-types-matter",
    "href": "files/lecture_notes/lecture2/lecture2.html#why-data-types-matter",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Why Data Types Matter",
    "text": "Why Data Types Matter\nUnderstanding data types is crucial because:\n\n\nDifferent statistics are appropriate for different data types\nStatistical methods depend on the level of measurement\nMisapplying statistics can lead to incorrect conclusions"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#types-of-data",
    "href": "files/lecture_notes/lecture2/lecture2.html#types-of-data",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Types of Data",
    "text": "Types of Data\n\n\n\n\n\ngraph TD\n    A[Variable Types]\n    A --&gt; B[Categorical]\n    A --&gt; C[Numerical]\n    B --&gt; B1[\"Nominal&lt;br/&gt;e.g., gender, major\"]\n    B --&gt; B2[\"Ordinal&lt;br/&gt;e.g., rating, education level\"]\n    C --&gt; C1[\"Discrete&lt;br/&gt;e.g., count (# of students)\"]\n    C --&gt; C2[\"Continuous&lt;br/&gt;e.g., measurements like height, weight\"]\n    \n    classDef root fill:#e1f5fe,stroke:#01579b,stroke-width:3px\n    classDef categorical fill:#f3e5f5,stroke:#4a148c,stroke-width:2px\n    classDef numerical fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px\n    classDef nominal fill:#fce4ec,stroke:#880e4f,stroke-width:2px\n    classDef ordinal fill:#fff3e0,stroke:#e65100,stroke-width:2px\n    classDef discrete fill:#e0f2f1,stroke:#00695c,stroke-width:2px\n    classDef continuous fill:#f1f8e9,stroke:#33691e,stroke-width:2px\n    \n    class A root\n    class B categorical\n    class C numerical\n    class B1 nominal\n    class B2 ordinal\n    class C1 discrete\n    class C2 continuous\n\n\n\n\n\n\n\nPrompt: Which summary stat would you pick for “major”? For “gpa”?"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#categorical-data-qualitative",
    "href": "files/lecture_notes/lecture2/lecture2.html#categorical-data-qualitative",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Categorical Data (Qualitative)",
    "text": "Categorical Data (Qualitative)\n\n\nNominal Data\n\nCategories with no natural order\nExamples: gender, color, brand names, marital status\nAppropriate statistics: mode, frequency counts, proportions\n\n\nOrdinal Data\n\nCategories with a natural order or ranking\nExamples: education level, satisfaction ratings, letter grades\nAppropriate statistics: mode, median, percentiles"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#numerical-data-quantitative",
    "href": "files/lecture_notes/lecture2/lecture2.html#numerical-data-quantitative",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Numerical Data (Quantitative)",
    "text": "Numerical Data (Quantitative)\n\nDiscrete Data\n\nCountable values, often integers\nExamples: number of children, cars sold, defective items\nCan take on finite or countably infinite values\n\n\n\nContinuous Data\n\nCan take any value within a range\nExamples: height, weight, temperature, time\nMeasured rather than counted"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#measurement-scales-summary",
    "href": "files/lecture_notes/lecture2/lecture2.html#measurement-scales-summary",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Measurement Scales Summary",
    "text": "Measurement Scales Summary\n\n\n\n\n\n\n\n\n\n\nScale\nType\nProperties\nExamples\nAppropriate Statistics\n\n\n\n\nNominal\nCategorical\nCategories only\nGender, Color\nMode, Frequency\n\n\nOrdinal\nCategorical\nOrder matters\nRankings, Grades\nMode, Median\n\n\nInterval\nNumerical\nEqual intervals, no true zero\nTemperature (°C)\nMean, Median, Mode\n\n\nRatio\nNumerical\nEqual intervals, true zero\nHeight, Weight, Income\nAll statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#what-is-central-tendency",
    "href": "files/lecture_notes/lecture2/lecture2.html#what-is-central-tendency",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "What is Central Tendency?",
    "text": "What is Central Tendency?\nCentral tendency describes the center or typical value of a dataset.\nIt answers the question: “What is a representative value for this data?”"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#definition-and-formula",
    "href": "files/lecture_notes/lecture2/lecture2.html#definition-and-formula",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Definition and Formula",
    "text": "Definition and Formula\n\n🎯 Definition: The mean is the sum of all values divided by the number of values.\n\n\nFormula\n\nFor a sample: \\(\\bar{x} = \\frac{\\sum x}{n}\\)\nFor a population: \\(\\mu = \\frac{\\sum x}{N}\\)\n\nWhere:\n\n\\(\\bar{x}\\) (x-bar) = sample mean\n\\(\\mu\\) (mu) = population mean\n\\(\\sum x\\) = sum of all values\n\\(n\\) = sample size, \\(N\\) = population size"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#example-calculation",
    "href": "files/lecture_notes/lecture2/lecture2.html#example-calculation",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Example Calculation",
    "text": "Example Calculation\nStudent test scores: 85, 90, 78, 92, 88\n\nMean = \\(\\frac{85 + 90 + 78 + 92 + 88}{5} = \\frac{433}{5} = 86.6\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-mean",
    "href": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-mean",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Properties of the Mean",
    "text": "Properties of the Mean\n\nUses all data points - every value affects the mean\nSensitive to outliers - extreme values can distort the mean\nUnique - there is only one mean for a dataset\nCan be calculated for interval and ratio data\nBalancing point - sum of deviations from mean equals zero"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-mean",
    "href": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-mean",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "When to Use the Mean",
    "text": "When to Use the Mean\n✅ Use the mean when:\n\nData is approximately symmetric\nNo extreme outliers present\nWorking with interval or ratio data\nNeed to use the value in further calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#advantages-and-disadvantages",
    "href": "files/lecture_notes/lecture2/lecture2.html#advantages-and-disadvantages",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Advantages and Disadvantages",
    "text": "Advantages and Disadvantages\n\n\nAdvantages\n\nUses all information in the dataset\nAlgebraically defined and mathematically tractable\nWidely understood and accepted\n\n\nDisadvantages\n\nAffected by outliers and skewed distributions\nMay not represent a typical value in skewed data\nCannot be used with nominal or ordinal data"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#definition",
    "href": "files/lecture_notes/lecture2/lecture2.html#definition",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Definition",
    "text": "Definition\n\n🎯 Definition: The median is the middle value when data is arranged in ascending or descending order."
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#calculation-steps",
    "href": "files/lecture_notes/lecture2/lecture2.html#calculation-steps",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Calculation Steps",
    "text": "Calculation Steps\n\nArrange data in ascending order\nFind the middle position:\n\nIf n is odd: position = \\(\\frac{n + 1}{2}\\)\nIf n is even: average of positions \\(\\frac{n}{2}\\) and \\(\\frac{n}{2} + 1\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#examples",
    "href": "files/lecture_notes/lecture2/lecture2.html#examples",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Examples",
    "text": "Examples\n\nOdd number of values:\n\n\nData: 12, 15, 18, 20, 25\nWhat is the median here ?\nMedian = 18 (middle value)\n\n\n\n\nEven number of values:\n\n\nData: 10, 15, 20, 25, 30, 35\nWhat is the median here ?\nMedian = \\(\\frac{20 + 25}{2} = 22.5\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-median",
    "href": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-median",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Properties of the Median",
    "text": "Properties of the Median\n\n\nNot affected by outliers - resistant measure\nRepresents the 50th percentile\nMay not be an actual data value (when n is even)\nAppropriate for ordinal, interval, and ratio data\nDivides data into two equal halves"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-median",
    "href": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-median",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "When to Use the Median",
    "text": "When to Use the Median\n✅ Use the median when:\n\nData is skewed (not symmetric)\nOutliers are present\nWorking with ordinal data\nWant a robust measure of central tendency\nData represents income, housing prices, or similar distributions"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#median-vs-mean-with-outliers",
    "href": "files/lecture_notes/lecture2/lecture2.html#median-vs-mean-with-outliers",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Median vs Mean with Outliers",
    "text": "Median vs Mean with Outliers\nConsider household incomes: $30,000, $32,000, $35,000, $38,000, $2,000,000\n\n\nMean = $427,000 (not representative of typical household)\nMedian = $35,000 (better represents typical household)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#definition-1",
    "href": "files/lecture_notes/lecture2/lecture2.html#definition-1",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Definition",
    "text": "Definition\n\n🎯 Definition: The mode is the value that appears most frequently in a dataset.\n\n\n\n\n\n\n%%{init: {'flowchart': {'nodeSpacing': 100, 'rankSpacing': 100}, 'width': 600, 'height': 400}}%%\ngraph TD\n    A[\"Mode Types\"]\n    A --&gt; B[\"Unimodal&lt;br/&gt;One peak\"]\n    A --&gt; C[\"Bimodal&lt;br/&gt;Two peaks\"]\n    A --&gt; D[\"Multimodal&lt;br/&gt;Multiple peaks\"]\n    A --&gt; E[\"No Mode&lt;br/&gt;No repeated values\"]\n    \n    classDef main fill:#e1f5fe,stroke:#01579b,stroke-width:3px,font-size:16px\n    classDef types fill:#f5f5f5,stroke:#424242,stroke-width:2px,font-size:14px\n    \n    class A main\n    class B,C,D,E types"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#types-of-distributions-by-mode",
    "href": "files/lecture_notes/lecture2/lecture2.html#types-of-distributions-by-mode",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Types of Distributions by Mode",
    "text": "Types of Distributions by Mode\n\nUnimodal: One mode\nBimodal: Two modes\n\nMultimodal: More than two modes\nNo mode: All values appear with equal frequency"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#example-1",
    "href": "files/lecture_notes/lecture2/lecture2.html#example-1",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Example 1:",
    "text": "Example 1:\nData: 2, 3, 3, 4, 5, 5, 5, 6, 7\n\nAnalysis:\n\n\nCount each value: 2(1), 3(2), 4(1), 5(3), 6(1), 7(1)\nMost frequent value: 5 appears 3 times\nMode = 5\nType: Unimodal (one mode)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#example-2",
    "href": "files/lecture_notes/lecture2/lecture2.html#example-2",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Example 2:",
    "text": "Example 2:\nData: 1, 2, 2, 3, 4, 4, 5\n\nAnalysis:\n\n\nCount each value: 1(1), 2(2), 3(1), 4(2), 5(1)\nMost frequent values: 2 and 4 both appear twice\nModes = 2 and 4\nType: Bimodal (two modes)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#example-3",
    "href": "files/lecture_notes/lecture2/lecture2.html#example-3",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Example 3:",
    "text": "Example 3:\nData: 1, 2, 3, 4, 5\n\nAnalysis:\n\n\nCount each value: 1(1), 2(1), 3(1), 4(1), 5(1)\nAll values appear exactly once\nNo mode (no value repeats)\nType: No mode"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#mode-for-different-data-types",
    "href": "files/lecture_notes/lecture2/lecture2.html#mode-for-different-data-types",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Mode for Different Data Types",
    "text": "Mode for Different Data Types\nCategorical Data:\nFavorite colors: Red, Blue, Blue, Green, Blue, Red, Blue Mode = Blue\nContinuous Data:\nOften requires grouping into intervals or bins Example: Heights grouped into ranges"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-mode",
    "href": "files/lecture_notes/lecture2/lecture2.html#properties-of-the-mode",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Properties of the Mode",
    "text": "Properties of the Mode\n\nCan be used with any type of data (nominal, ordinal, interval, ratio)\nNot affected by outliers\nMay not exist or may not be unique\nRepresents the most common value\nEasy to identify in frequency distributions"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-mode",
    "href": "files/lecture_notes/lecture2/lecture2.html#when-to-use-the-mode",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "When to Use the Mode",
    "text": "When to Use the Mode\n✅ Use the mode when:\n\nWorking with categorical (nominal) data\nWant to identify the most popular or common choice\nData has clear peaks in frequency\nQuality control applications (most common defect type)\nBusiness applications (best-selling product, most common customer complaint)"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#comparing-measures-of-central-tendency",
    "href": "files/lecture_notes/lecture2/lecture2.html#comparing-measures-of-central-tendency",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Comparing Measures of Central Tendency",
    "text": "Comparing Measures of Central Tendency\n\n\n\n\n\n\n\n\n\n\nMeasure\nBest for Data Type\nStrengths\nWeaknesses\nAffected by Outliers?\n\n\n\n\nMean\nInterval, Ratio\nUses all data, mathematically tractable\nSensitive to outliers\nYes\n\n\nMedian\nOrdinal, Interval, Ratio\nRobust to outliers, represents middle\nIgnores extreme values\nNo\n\n\nMode\nAll types\nWorks with categorical, identifies most common\nMay not exist/be unique\nNo"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#shape-of-distribution-effects",
    "href": "files/lecture_notes/lecture2/lecture2.html#shape-of-distribution-effects",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Shape of Distribution Effects",
    "text": "Shape of Distribution Effects\n\nSymmetric distribution: Mean ≈ Median ≈ Mode\nRight-skewed (positively skewed): Mean &gt; Median &gt; Mode\n\nLeft-skewed (negatively skewed): Mode &gt; Median &gt; Mean"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#central-measures",
    "href": "files/lecture_notes/lecture2/lecture2.html#central-measures",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Central Measures",
    "text": "Central Measures\n\n\n\nMean CalculationMedian CalculationMode Calculation\n\n\n\n# Import numpy library for numeric operations\nimport numpy as np \n\n# Import pandas library for data structures\nimport pandas as pd\n\n# Define sample data as a list of test scores\ndata = [85, 90, 78, 92, 88, 91, 85, 87, 89, 86]\n\n# Compute the arithmetic mean using NumPy\nmean_np = np.mean(data)\nprint(f\"Mean (NumPy): {mean_np:.2f}\")\n\n# Convert the data list into a pandas DataFrame\ndf = pd.DataFrame({'scores': data})\n\n# Compute the arithmetic mean using Pandas\nmean_pd = df['scores'].mean()\nprint(f\"Mean (Pandas): {mean_pd:.2f}\")\n\n# Manually sum all scores and divide by count\nmanual_mean = sum(data) / len(data)\nprint(f\"Mean (Manual): {manual_mean:.2f}\")\n\nMean (NumPy): 87.10\nMean (Pandas): 87.10\nMean (Manual): 87.10\n\n\n\n\n\n# Compute the median value using NumPy\nmedian_np = np.median(data)\nprint(f\"Median (NumPy): {median_np:.2f}\")\n\n# Compute the median value using Pandas\nmedian_pd = df['scores'].median()\nprint(f\"Median (Pandas): {median_pd:.2f}\")\n\n# Sort the data list in ascending order\nsorted_data = sorted(data)\n\n# Determine the number of elements\nn = len(sorted_data)\n\n# If even count, average the two middle elements; else take middle element\nif n % 2 == 0:\n    manual_median = (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2\nelse:\n    manual_median = sorted_data[n//2]\n\nprint(f\"Median (Manual): {manual_median:.2f}\")\n\nMedian (NumPy): 87.50\nMedian (Pandas): 87.50\nMedian (Manual): 87.50\n\n\n\n\n\n# Import SciPy stats module to compute mode\nfrom scipy import stats\n\n# Use SciPy to find the most common value and its count\nmode_result = stats.mode(data, keepdims=True)\nprint(f\"Mode (SciPy): {mode_result.mode[0]}, Count: {mode_result.count[0]}\")\n\n# Use Pandas to get mode(s) from the DataFrame\nmode_pd = df['scores'].mode()\nprint(f\"Mode (Pandas): {mode_pd.values}\")\n\n# Import Counter for manual frequency counting\nfrom collections import Counter\n\n# Count occurrences of each score\ncounter = Counter(data)\n\n# Find the highest frequency\nmax_count = max(counter.values())\n\n# Identify all values that appear with that frequency\nmodes = [k for k, v in counter.items() if v == max_count]\n\nprint(f\"Mode (Manual): {modes}, Count: {max_count}\")\n\nMode (SciPy): 85, Count: 2\nMode (Pandas): [85]\nMode (Manual): [85], Count: 2"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#mean-mode-and-median-visualization",
    "href": "files/lecture_notes/lecture2/lecture2.html#mean-mode-and-median-visualization",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Mean, Mode and Median Visualization",
    "text": "Mean, Mode and Median Visualization\n\nCodeVisualization\n\n\n\n# import libraries\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n# define your data and compute statistics here\ndata = [85, 90, 78, 92, 88, 91, 85, 87, 89, 86]\nmean_np   = np.mean(data)\nmedian_np = np.median(data)\n# for mode, use Counter\ncounter   = Counter(data)\nmax_count = max(counter.values())\nmodes     = [k for k,v in counter.items() if v==max_count]\nmode_val  = modes[0]\n\n\n\n\n\n\n\n\nDistribution of Test Scores with Central Tendency Measures"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#business-applications",
    "href": "files/lecture_notes/lecture2/lecture2.html#business-applications",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Business Applications",
    "text": "Business Applications\n\nCustomer Satisfaction: Mean rating shows overall satisfaction, median shows typical experience\nSales Data: Mode identifies best-selling products, median shows typical sale amount\n\nEmployee Performance: Mean for overall team performance, median for typical employee"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#educational-applications",
    "href": "files/lecture_notes/lecture2/lecture2.html#educational-applications",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Educational Applications",
    "text": "Educational Applications\n\nTest Scores: Mean for class average, median for typical student performance\nGrade Distribution: Mode shows most common grade\nAttendance: Mean for overall attendance rate"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#healthcare-applications",
    "href": "files/lecture_notes/lecture2/lecture2.html#healthcare-applications",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Healthcare Applications",
    "text": "Healthcare Applications\n\nPatient Wait Times: Median often preferred due to skewed distributions\nTreatment Outcomes: Mean for overall effectiveness, mode for most common result\nVital Signs: All three measures provide different insights"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#interpretation-guidelines",
    "href": "files/lecture_notes/lecture2/lecture2.html#interpretation-guidelines",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Interpretation Guidelines",
    "text": "Interpretation Guidelines\n\nAlways consider the context of your data\nReport multiple measures when appropriate\n\nBe aware of data distribution shape\nConsider the presence of outliers\nChoose the most appropriate measure for your specific question"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#remember-these-points",
    "href": "files/lecture_notes/lecture2/lecture2.html#remember-these-points",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Remember These Points",
    "text": "Remember These Points\n\nData type determines which statistics are appropriate\nMean uses all data but is sensitive to outliers\nMedian is robust and represents the middle value\nMode identifies the most common value and works with all data types\nContext matters when choosing and interpreting measures\nPython provides powerful tools for calculating descriptive statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#next-lecture-preview",
    "href": "files/lecture_notes/lecture2/lecture2.html#next-lecture-preview",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Next Lecture Preview",
    "text": "Next Lecture Preview\nDescriptive Statistics Part II will cover:\n\nMeasures of variability (range, variance, standard deviation)\nMeasures of position (percentiles, quartiles, z-scores)\nShape of distributions (skewness and kurtosis)\nAdvanced Python visualization techniques"
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#try-these-problems",
    "href": "files/lecture_notes/lecture2/lecture2.html#try-these-problems",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Try These Problems",
    "text": "Try These Problems\n\nCalculate mean, median, and mode for: 12, 15, 18, 12, 20, 25, 12, 30\nA dataset has mean = 50 and median = 45. What does this tell you about the distribution?\nWhy might median be preferred over mean for reporting household income?\nCreate a Python function to identify the most appropriate measure of central tendency for a given dataset."
  },
  {
    "objectID": "files/lecture_notes/lecture2/lecture2.html#footnotes",
    "href": "files/lecture_notes/lecture2/lecture2.html#footnotes",
    "title": "Lecture 2: Descriptive Statistics I",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(source)↩︎"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html",
    "href": "files/resources/prob_cheat_sheet.html",
    "title": "Probability Rules Cheat Sheet",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#basic-probability-concepts",
    "href": "files/resources/prob_cheat_sheet.html#basic-probability-concepts",
    "title": "Probability Rules Cheat Sheet",
    "section": "Basic Probability Concepts",
    "text": "Basic Probability Concepts\n\nProbability Definition: \\[P(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of outcomes}}\\]\nProperties: - \\(0 \\leq P(A) \\leq 1\\) - \\(P(\\emptyset) = 0\\) (impossible event) - \\(P(S) = 1\\) (certain event, where \\(S\\) is sample space)\n\n\nExample: Rolling a fair die, probability of getting an even number:\n\\[P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2} = 0.5\\]\n\n\nPractice: What is the probability of drawing a face card from a standard deck?\nAnswer: \\(P(\\text{face card}) = \\frac{12}{52} = \\frac{3}{13}\\)"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#complement-rule",
    "href": "files/resources/prob_cheat_sheet.html#complement-rule",
    "title": "Probability Rules Cheat Sheet",
    "section": "Complement Rule",
    "text": "Complement Rule\n\nFormula: \\[P(A^c) = 1 - P(A)\\] Alternative notation: \\(P(A') = 1 - P(A)\\)\n\nExplanation: The probability that event \\(A\\) does not occur.\n\nIf the probability that Anya will graduate is 0.9, then the probability she will not graduate is:\n\\[P(\\text{not graduate}) = 1 - 0.9 = 0.1\\]\n\n\nIf \\(P(\\text{rain}) = 0.3\\), what is \\(P(\\text{no rain})\\)?\nAnswer: \\(P(\\text{no rain}) = 1 - 0.3 = 0.7\\)"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#addition-rules",
    "href": "files/resources/prob_cheat_sheet.html#addition-rules",
    "title": "Probability Rules Cheat Sheet",
    "section": "Addition Rules",
    "text": "Addition Rules\n\nGeneral Addition Rule (For Any Two Events)\n\nFormula: \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\nExplanation: We subtract \\(P(A \\cap B)\\) to avoid double-counting the overlap.\n\nIn a class of 24 students, 10 are girls, 11 are A students, and 6 are girls who are A students.\nProbability of selecting a girl or an A student:\n\\[P(\\text{girl or A}) = \\frac{10}{24} + \\frac{11}{24} - \\frac{6}{24} = \\frac{15}{24} = 0.625\\]\n\n\n\nAddition Rule for Mutually Exclusive Events\n\nFormula: \\[P(A \\cup B) = P(A) + P(B)\\] Condition: \\(P(A \\cap B) = 0\\) (events cannot occur simultaneously)\n\n\nProbability of rolling a 2 or 6 on a die:\n\\[P(2 \\text{ or } 6) = \\frac{1}{6} + \\frac{1}{6} = \\frac{2}{6} = 0.333\\]\n\n\nA bag contains 4 red, 3 blue, and 2 green marbles. What’s the probability of drawing a red or green marble?\nAnswer: \\(P(\\text{red or green}) = \\frac{4}{9} + \\frac{2}{9} = \\frac{6}{9} = \\frac{2}{3}\\)"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#multiplication-rules",
    "href": "files/resources/prob_cheat_sheet.html#multiplication-rules",
    "title": "Probability Rules Cheat Sheet",
    "section": "Multiplication Rules",
    "text": "Multiplication Rules\n\nMultiplication Rule for Dependent Events\n\nFormula: \\[P(A \\cap B) = P(A) \\times P(B|A)\\] Alternative: \\(P(A \\cap B) = P(B) \\times P(A|B)\\)\n\n\nDrawing two red cards without replacement from a standard deck:\n\\[P(\\text{red and red}) = \\frac{26}{52} \\times \\frac{25}{51} = 0.245\\]\n\n\n\nMultiplication Rule for Independent Events\n\nFormula: \\[P(A \\cap B) = P(A) \\times P(B)\\] Condition: Events are independent if \\(P(A|B) = P(A)\\)\n\n\nDrawing two red cards with replacement:\n\\[P(\\text{red and red}) = \\frac{26}{52} \\times \\frac{26}{52} = 0.25\\]\n\n\nTwo fair coins are flipped. What’s the probability of getting two heads?\nAnswer: \\(P(HH) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\\)"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#conditional-probability",
    "href": "files/resources/prob_cheat_sheet.html#conditional-probability",
    "title": "Probability Rules Cheat Sheet",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nFormula: \\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\] Condition: \\(P(B) &gt; 0\\)\n\nExplanation: The probability of event \\(A\\) occurring given that event \\(B\\) has occurred.\n\nIn a group of 100 people, 60 are employed and 40 are unemployed. Of the employed, 45 are satisfied with their job.\nWhat’s the probability someone is satisfied given they are employed?\n\\[P(\\text{satisfied} | \\text{employed}) = \\frac{45}{60} = 0.75\\]\n\n\nA card is drawn from a deck. Given that it’s red, what’s the probability it’s a heart?\nAnswer: \\(P(\\text{heart} | \\text{red}) = \\frac{13}{26} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "files/resources/prob_cheat_sheet.html#set-operations-and-probability",
    "href": "files/resources/prob_cheat_sheet.html#set-operations-and-probability",
    "title": "Probability Rules Cheat Sheet",
    "section": "Set Operations and Probability",
    "text": "Set Operations and Probability\n\nUnion (OR): - Symbol: \\(A \\cup B\\) - Meaning: Event \\(A\\) OR event \\(B\\) (or both) occurs - Formula: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nIntersection (AND): - Symbol: \\(A \\cap B\\) - Meaning: Both events \\(A\\) AND \\(B\\) occur - Formula: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\n\n\nComplement (NOT): - Symbol: \\(A^c\\) or \\(A'\\) - Meaning: Event \\(A\\) does NOT occur - Formula: \\(P(A^c) = 1 - P(A)\\)"
  },
  {
    "objectID": "files/worksheets/worksheet2.html",
    "href": "files/worksheets/worksheet2.html",
    "title": "PSTAT 5A Practice Worksheet 2",
    "section": "",
    "text": "Download PDF"
  },
  {
    "objectID": "files/worksheets/worksheet2.html#understanding-variance-population-vs-sample",
    "href": "files/worksheets/worksheet2.html#understanding-variance-population-vs-sample",
    "title": "PSTAT 5A Practice Worksheet 2",
    "section": "4.1 Understanding Variance: Population vs Sample",
    "text": "4.1 Understanding Variance: Population vs Sample\n\n🎯 Key Variance Concepts:\nPopulation Variance (when you have ALL data): \\[\\sigma^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}\\]\nSample Variance (when you have a sample): \\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\]\nWhy (n-1)? Using the sample mean to estimate deviations “uses up” one degree of freedom.\n\n\nProblem C1: Basic Variance Calculations\nThe following data represents the number of customer complaints per day for a small business over 8 days:\nData: 3, 7, 2, 8, 5, 6, 4, 9\nPart (a) : Calculate the sample mean \\(\\bar{x}\\).\nPart (b) : Calculate the sample variance \\(s^2\\) using the formula with \\((n-1)\\) in the denominator.\nPart (c) : Calculate the sample standard deviation \\(s\\).\nPart (d) : If this were treated as a complete population, what would the population variance \\(\\sigma^2\\) be?\nPart (e) : Explain why we divide by \\((n-1)\\) for sample variance instead of \\(n\\).\nAnswer:\n\n\n\nProblem C2: Comparing Variability\nConsider two data sets:\n\nSet A: 10, 12, 14, 16, 18\nSet B: 5, 10, 14, 18, 23\n\n\nCalculate the mean for each set.\nCalculate the sample variance for each set.\nWhich set has greater variability?\nCalculate the coefficient of variation \\((CV = s/x̄)\\) for each set. Which has greater relative variability?\n\nAnswer:"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#introduction",
    "href": "files/labs/lab2/lab2.html#introduction",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "1 Introduction",
    "text": "1 Introduction\nLast week, we were introduced to the notion of data types. Recall that “data type” can be thought of as the category (or type) of data i.e. integer, float, character, etc.\nIn Python, however, we often need to aggregate data into larger structures, often referred to as data classes."
  },
  {
    "objectID": "files/labs/lab2/lab2.html#lists",
    "href": "files/labs/lab2/lab2.html#lists",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "2 Lists",
    "text": "2 Lists\nPerhaps the most fundamental data structure in Python is that of a list. Just like lists in real life or in mathematics, Python lists are just collections of items enclosed in square brackets:\n[&lt;item 1&gt;, &lt;item 2&gt;, ..., &lt;item n&gt;]\nAgain, the items in a list can be of any data type; we can even mix and match data types!\n\n2.1 Task 1\nCreate a list containing the elements 1, \"hi\", 3.4, and \"PSTAT 5A\". Assign this list to a variable called list1.\n\n# Your code here\n\nJust as we were able to use a Python function (type()) to check the type of a particular piece of data, we can also use Python to check the structure or class of a piece of data. It turns out that we use the same function as before- namely, type()!\n\n\n2.2 Task 1 (cont’d)\nRun the code type(list1).\n\n# Your code here"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#indexing",
    "href": "files/labs/lab2/lab2.html#indexing",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "3 Indexing",
    "text": "3 Indexing\nAlright, now that we can store data in lists, how can we access elements in a list? The answer is to use what is known as indexing.\nGiven a list x, we access the ith element using the code:\nx[i]\nThe reason we call this “indexing” is because the number that goes between the brackets is the index of the element that we want.\n\n\n\n\n\n\nCaution\n\n\n\nPython begins indexing at 0.\n\n\nWhat does this mean? Well, let’s see by way of an example.\n\n3.1 Task 2\nCreate a list with the numbers 1 through 10, inclusive, and assign this to a variable called x.\n\n# Your code here\n\nRun the code x[1].\n\n# Your code here\n\nRun the code x[0].\n\n# Your code here\n\nSo, what we would colloquially call the first element of a list, Python calls the zeroeth element.\n\n\n3.2 Task 3\nCreate a list called x that contains the elements 1, \"two\", 3.5, \"four\", and \"five five\". Answer the following questions WITHOUT running any code, writing your answers as a comment in a code cell:\n\nWhat would be the output of type(x)?\nWhat would be the output of type(x[1])?\nWhat would be the output of x[0]?\n\n\n# Create the list\nx = [1, \"two\", 3.5, \"four\", \"five five\"]\n\n# Your predictions as comments:\n# 1. type(x) would output: \n# 2. type(x[1]) would output: \n# 3. x[0] would output: \n\nNow, run code to verify your answers to the above three questions.\n\n# Verify your answers here"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#tables",
    "href": "files/labs/lab2/lab2.html#tables",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "4 Tables",
    "text": "4 Tables\nAnother very useful data structure in Python is that of a table. Python tables behave pretty much the same as the tables we’ve used in, say, math- they are a grid of values arranged sequentially.\nTables can be created using the Table() function in Python, which itself comes from the datascience module. The general syntax of creating a table with the Table() function is:\nTable().with_columns(\n  \"\", [, , ... ],\n  \"\", [, , ... ],\n  ...\n)\nFor example:\n\n# Install datascience if needed (uncomment if necessary)\n# !pip install datascience\nfrom datascience import *\n\nTable().with_columns(\n  \"Course\", [\"PSTAT 5A\", \"PSTAT 120A\", \"PSTAT 130\"],\n  \"Units\", [4, 4, 4],\n  \"Quarter\", [\"Summer\", \"Fall\", \"Winter\"]\n)\n\n/Users/narjesmathlouthi/Desktop/PSTAT5A/web/PSTAT5A/.venv/lib/python3.13/site-packages/datascience/maps.py:13: UserWarning:\n\npkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools&lt;81.\n\n\n\n\n\n\nCourse\nUnits\nQuarter\n\n\n\n\nPSTAT 5A\n4\nSummer\n\n\nPSTAT 120A\n4\nFall\n\n\nPSTAT 130\n4\nWinter\n\n\n\n\n\nThere is nothing stopping us from assigning a table to a variable! For example, after running:\n\ntable1 = Table().with_columns(\n  \"Course\", [\"PSTAT 5A\", \"PSTAT 120A\", \"PSTAT 130\"],\n  \"Units\", [4, 4, 4],\n  \"Instructor\", [\"Mathlouthi\", \"Johnson\", \"Smith\"]\n)\n\ntable1\n\n\n\n\nCourse\nUnits\nInstructor\n\n\n\n\nPSTAT 5A\n4\nMathlouthi\n\n\nPSTAT 120A\n4\nJohnson\n\n\nPSTAT 130\n4\nSmith\n\n\n\n\n\n\n4.1 Terminology\nSometimes in Python we will encounter expressions of the form:\n&lt;object type&gt;.&lt;function name&gt;()\nIn this syntax, the function &lt;function name&gt; is said to be a method. For example, the function with_columns() is a method for the Table object.\nThe datascience module contains a plethora of methods we can use to manage tables. For example, the select() method can be used to select columns by name:\n\ntable1.select(\"Units\")\n\n\n\n\nUnits\n\n\n\n\n4\n\n\n4\n\n\n4\n\n\n\n\n\n\n\n\n\n\n\nSyntax\n\n\n\nMethods are always appended to either a function that creates a blank object type (like Table()) or a variable of the correct type.\n\n\n\n\n4.2 Task 4\nRead the list of methods for Table objects at http://data8.org/datascience/tables.html, and write down (in a code cell, using comments) at least three different methods, including a short description of what each method does. For example:\n\n# .with_columns(): adds specified columns to a table.\n# Your three methods here:\n\n\n\n4.3 Task 5\nCreate the following table, and assign it to a variable called profs:\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Swenson\nSouth Hall\nPSTAT 130\n\n\nDr. Wainwright\nOld Gym\nPSTAT 120A\n\n\nDr. Mouti\nOld Gym\nPSTAT 126\n\n\n\n\n# Your code here\n\nRun a cell containing only the code profs to make sure (visually) that your table looks correct.\n\n# Your code here\n\nSelect the column called Course from profs.\n\n# Your code here\n\nCreate a new table called profs_new that contains the same rows as the profs table, but with the following additional row:\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Ravat\nSouth Hall\nPSTAT 120B\n\n\n\n\n# Your code here\n\nRun a cell containing only the code profs_new to make sure (visually) that the appending was successful.\n\n\n\n\n\n\nTip\n\n\n\nThink about how you can use our discussion on updating variable values from last lab. Also, the method .with_row() may be useful; see the help file at http://data8.org/datascience/tables.html for more information.\n\n\n\n# Your code here\n\n\n\n4.4 Filtering Tables\nSuppose we want to select rows of a table that satisfy a given condition. For example, if we wanted to find the information of only courses taught by Mathlouthi in the table1 table above, we would call:\n\ntable1.where(\"Instructor\", \"Mathlouthi\")\n\n\n\n\nCourse\nUnits\nInstructor\n\n\n\n\nPSTAT 5A\n4\nMathlouthi\n\n\n\n\n\nWhat would happen if we tried to select the rows of table1 with “Wilson” in the Instructor column? Well, since there is nobody in table1 named Wilson, we should hope that Python returns an empty table.\n\ntable1.where(\"Instructor\", \"Wilson\")\n\n\n\n\nCourse\nUnits\nInstructor\n\n\n\n\n\n\n\nSure enough, Python has returned an empty table!"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#arrays",
    "href": "files/labs/lab2/lab2.html#arrays",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "5 Arrays",
    "text": "5 Arrays\nThe final Data Structure we will examine in this class is that of an array. Arrays behave very similarly to Tables, with a few differences. For one, the syntax used to create an array is slightly different:\nmake_array(&lt;item 1&gt;, &lt;item 2&gt;, &lt;item 3&gt;, ...)\nFor example:\n\nmake_array(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\n\narray(['Spring', 'Summer', 'Autumn', 'Winter'],\n      dtype='&lt;U6')\n\n\nYou may ask- what’s that dtype='&lt;U6' symbol at the end of the output? For now, don’t worry about it, as we will revisit this later.\n\n5.1 Lists vs. Arrays\nSo, we now know about three different data classes in Python: lists, tables, and arrays. At first glance, lists and arrays may seem somewhat similar. However, there are a few key differences between them:\n\n\n5.2 Task 6\nMake a list called my_list containing the elements 1, 2, and 3, and make an array called my_array also containing the elements 1, 2, and 3. Run the following commands in separate code cells:\n\n# Create list and array\nmy_list = [1, 2, 3]\nmy_array = make_array(1, 2, 3)\n\n\nsum(my_list)\n\n6\n\n\n\nsum(my_array)\n\n6\n\n\n\n# my_list + 2  # This will cause an error\n\n\nmy_array + 2\n\narray([3, 4, 5])\n\n\nWhat the previous Task illustrates is the fact that arrays lend themselves to element-wise operations, whereas lists do not. One important limitation about arrays, though, is that the elements in an array must all be of the same data type. If you try to make an array consisting of elements that are different data types Python will still run, however it will not run in the way you expect it to!"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#comparisons",
    "href": "files/labs/lab2/lab2.html#comparisons",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "6 Comparisons",
    "text": "6 Comparisons\nHere’s a question: is 2 less than 3? Well, yes it is! If we wanted to confirm this, we could simply ask Python whether 2 is less than 3 by running:\n\n2 &lt; 3\n\nTrue\n\n\nNotice, however, how Python answered this question: it simply returned True. Let’s see what the data type of True is:\n\ntype(True)\n\nbool\n\n\nTrue is of the type bool, which is short for boolean. There are only two boolean quantities in Python: True and False. Let’s see how we can generate a False value:\n\n3 &lt; 2\n\nFalse\n\n\nHere is a list of comparison operators, taken from the Inferential Thinking textbook:\n\n\n\nComparison\nOperator\nTrue Example\nFalse Example\n\n\n\n\nLess than\n&lt;\n2 &lt; 3\n2 &lt; 2\n\n\nGreater than\n&gt;\n3 &gt; 2\n3 &gt; 3\n\n\nLess than or equal\n&lt;=\n2 &lt;= 2\n3 &lt;= 2\n\n\nGreater than or equal\n&gt;=\n3 &gt;= 3\n2 &gt;= 3\n\n\nEqual\n==\n3 == 3\n3 == 2\n\n\nNot equal\n!=\n3 != 2\n2 != 2\n\n\n\nOne nice thing about Python is that it allows for multiple simultaneous comparisons. For example:\n\n2 &lt; 3 &lt; 4\n\nTrue\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a multiple comparison, Python will only return True when all of the included comparisons are true.\n\n\nFor instance, 2 &lt; 3 &lt; 1 would return False, because even though 2 is less than 3 it is not true that 3 is less than 1.\nBelieve it or not, you can compare strings as well! Python compares strings alphabetically; that is, letters at the beginning of the alphabet are considered to have smaller ordinal value than letters at the end of the alphabet. For example:\n\n\"apple\" &lt; \"banana\"\n\nTrue\n\n\n\n\"zebra\" &lt; \"zanzibar\"\n\nFalse\n\n\n\n\"cat\" &lt;= \"catenary\"\n\nTrue\n\n\n\n6.1 Task 7\nCheck how \"statistics\" and \"Statistics\" (note the capitalization!) compare. Use this to answer the question: when Python is comparing strings, does it give precedence to capital letters or not? If so, which (lowercase or capital) is given a “higher” value?\n\n# Your code here\n\n\n\n6.2 Comparing Lists and Arrays\nFinally, we discuss how comparisons work in the context of lists and arrays. The way Python compares lists is by what is known as lexicographical order. From the official Python help documentation, this means:\n\nfirst the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted.\n\nFor instance, [1, 2, 3] &lt; [2, 1, 1] would return True since 1 (the first element of the first list) is less than 2 (the first element of the second list).\nThe comparison of arrays is a little more straightforward, except:\n\n\n\n\n\n\nImportant\n\n\n\nWhen comparing two arrays, the arrays must be of the same length.\n\n\nTo see exactly how comparison of arrays works, let’s work through a Task:\n\n\n6.3 Task 8\nMake an array with the elements 1, 2, and 3, and call this x. Make another array with the elements 2, 3, 1, and call this y. Run x &lt; y, and comment on the result.\n\n# Your code here\n\nWhat the previous task illustrates is that Python compares arrays element-wise."
  },
  {
    "objectID": "files/labs/lab2/lab2.html#conditionals",
    "href": "files/labs/lab2/lab2.html#conditionals",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "7 Conditionals",
    "text": "7 Conditionals\nNow, we can use comparisons for much more than verifying simple arithmetic relationships. One of the main areas in which comparisons arise is the area of conditional expressions.\nSimply put, conditional expressions are how we can convey a set of choices to Python. As an example, let’s consider finding someone’s city based on their zip code. To simplify, let’s assume the only zip codes we consider are 93117, 93120, and 93150. From postal data, we know that:\n\na zip code of 93117 corresponds to Goleta\na zip code of 93120 corresponds to Santa Barbara\n\na zip code of 93150 corresponds to Montecito\n\nWe can rephrase this information in terms of “if” statements:\n\nIf a person has a zip code of 93117, then they are in Goleta\nOtherwise, if they have a zip code of 93120, then they are in Santa Barbara\nOtherwise, if they have a zip code of 93150, then they are in Montecito\n\nThis is precisely the syntax we would use when translating this experiment into Python syntax:\n\n# Example of conditional statements with zip codes\n# First, we need to define a zip_code variable\nzip_code = 93117  # Example zip code\n\nif zip_code == 93117:\n    location = \"Goleta\"\nelif zip_code == 93120:\n    location = \"Santa Barbara\"\nelif zip_code == 93150:\n    location = \"Montecito\"\nelse:\n    location = \"Unknown location\"\n\nprint(f\"Zip code {zip_code} corresponds to: {location}\")\n\n# Test with different zip codes\nfor test_zip in [93117, 93120, 93150, 99999]:\n    if test_zip == 93117:\n        loc = \"Goleta\"\n    elif test_zip == 93120:\n        loc = \"Santa Barbara\"\n    elif test_zip == 93150:\n        loc = \"Montecito\"\n    else:\n        loc = \"Unknown location\"\n    print(f\"Zip code {test_zip}: {loc}\")\n\nZip code 93117 corresponds to: Goleta\nZip code 93117: Goleta\nZip code 93120: Santa Barbara\nZip code 93150: Montecito\nZip code 99999: Unknown location\n\n\nBy the way: elif is an abbreviation for else if, which itself can be thought of as equivalent to otherwise, if.\nHere’s the general syntax of a conditional expression in Python:\n\nif &lt;condition 1&gt;:\n    &lt;task 1&gt;\nelif &lt;condition 2&gt;:\n    &lt;task 2&gt;\n...\nelse:\n    &lt;final task&gt;\n\nWhen executing the above conditional statement, Python first checks whether &lt;condition 1&gt; returns a value of True or False. If it returns a value of True, then &lt;task 1&gt; is executed and the statement ends. Otherwise, Python checks whether &lt;condition 2&gt; is True or False; if it is True then &lt;task 2&gt; is executed, etc.\n\n\n\n\n\n\nImportant\n\n\n\nIn the example code above: if &lt;condition 1&gt; is True, then no tasks beyond &lt;task 1&gt; are evaluated. If &lt;condition 2&gt; is True, then no tasks beyond &lt;task 2&gt; are evaluated. And so on and so forth.\n\n\n\n7.1 Task 9\nConsider the code:\n\nx = 2\n\nif x &lt; 2:\n    x = \"hello\"\nelif x &lt; 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nBefore running any code, write down what you think the result of executing x would be. Then, run the loop, execute x, and check whether your answer was correct or not.\n\n# Your prediction: \n\n# Now run the code:\nx = 2\n\nif x &lt; 2:\n    x = \"hello\"\nelif x &lt; 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nprint(x)\n\ngoodbye\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIndentation is very important in Python.\n\n\nFor example, if instead of the conditional expression in Task 9 we had instead put:\nx = 2\n\nif x &lt; 2:\nx = \"hello\"\nelif x &lt; 3:\nx = \"goodbye\"\nelse:\nx = \"take care\"\nthen we would have received an error!"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#functions",
    "href": "files/labs/lab2/lab2.html#functions",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "8 Functions",
    "text": "8 Functions\nFinally, let’s quickly discuss Python functions. We’ve already been using quite a few functions:\n\n8.1 Task 10\nIn a Markdown cell, write down three functions we’ve used in this Lab thus far.\nYour answer here:\nIf you recall, the general syntax for calling a function is:\n&lt;function name&gt;(&lt;arg1&gt;, &lt;arg2&gt;, ... )\nwhere &lt;function name&gt; denotes the function name and &lt;arg1&gt;, &lt;arg2&gt;, etc. denote the arguments of the function.\nCreating your own function in Python is actually fairly simple! Here is the syntax we use:\n\ndef &lt;function name&gt;(&lt;list out the argument names&gt;):\n    \"\"\"include a 'docstring' here\"\"\"\n    &lt;body of the function&gt;\n    return &lt;what you want the function to output&gt;\n\nFor example:\n\ndef f(x, y):\n    \"\"\"returns x^2 + y^2\"\"\"\n    return x**2 + y**2\n\ncreates a function f that can be called on two arguments, x and y, and returns the sum of squares of the arguments; e.g.\n\nf(3, 4)  # should return 3^2 + 4^2 = 25\n\n25\n\n\nBy the way, the docstring referenced above is a verbal description of what the function does. (Recall from Lab1 that it is just a multi-line comment, since it is enclosed in triple quotation marks!). All functions should include a docstring to convey to the user what the function does.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t include a return statement in the definition of a function, then your function will never return anything.\n\n\nFor instance:\n\ndef g(x, y):\n    \"\"\"should return x^2 + y^2\"\"\"\n    x**2 + y**2\n\nprint(g(3, 4))  # This will print None!\n\nNone\n\n\n\n\n8.2 Task 11\nWrite a function called cent_to_far() which takes in a single temperature c as measured in degrees Celsius and returns the corresponding temperature in degrees Fahrenheit. Check that cent_to_far(0) correctly returns 32 and cent_to_far(20) correctly returns 68.\nAs a reminder, the conversion formula is: F = (9/5) × C + 32\n\n# Your code here\ndef cent_to_far(c):\n    \"\"\"Convert Celsius to Fahrenheit\"\"\"\n    # Your implementation here\n    pass\n\n# Test your function\nprint(cent_to_far(0))   # Should return 32\nprint(cent_to_far(20))  # Should return 68\n\nFinally, let’s combine some things by way of a concluding Task:\n\n\n8.3 Task 12\nWrite a function called parity() that returns the parity (i.e. whether a number is even or odd) of an input x. Call your parity() function on 2 and then 3 to make sure your function behaves as expected. Some hints:\n\nRecall that % is the modulus operator in Python. Specifically, x % y returns the remainder of performing x divided by y.\nRecall that even numbers are divisible by 2 (so what does this mean about the remainder of dividing x by 2 if x is even?)\n\n\n# Your code here\ndef parity(x):\n    \"\"\"Returns 'even' if x is even, 'odd' if x is odd\"\"\"\n    # Your implementation here\n    pass\n\n# Test your function\nprint(parity(2))  # Should return 'even'\nprint(parity(3))  # Should return 'odd'"
  },
  {
    "objectID": "files/labs/lab2/lab2.html#summary",
    "href": "files/labs/lab2/lab2.html#summary",
    "title": "Lab 2: Data Classes and Programming Fundamentals",
    "section": "9 Summary",
    "text": "9 Summary\nIn this lab, you’ve learned about:\n\nLists: Ordered collections that can contain mixed data types\nIndexing: Accessing elements using zero-based indexing\nTables: Structured data with named columns using the datascience module\nArrays: Homogeneous collections that support element-wise operations\nComparisons: Boolean operations and comparison operators\nConditionals: if/elif/else statements for decision making\nFunctions: Creating reusable code blocks with def statements\n\nThese are fundamental building blocks that will be essential for the rest of the course!"
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#introduction",
    "href": "files/labs/lab2/lab2_sln.html#introduction",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Introduction",
    "text": "Introduction\nThis document contains the complete solutions to Lab 2: Data Classes and Programming Fundamentals. Each task is solved with explanations to help you understand the concepts."
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#lists",
    "href": "files/labs/lab2/lab2_sln.html#lists",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Lists",
    "text": "Lists\n\nTask 1 Solution\nCreate a list containing the elements 1, \"hi\", 3.4, and \"PSTAT 5A\". Assign this list to a variable called list1.\n\n# Solution\nlist1 = [1, \"hi\", 3.4, \"PSTAT 5A\"]\nprint(list1)\n\n[1, 'hi', 3.4, 'PSTAT 5A']\n\n\n\n\nTask 1 (cont’d) Solution\nRun the code type(list1).\n\n# Solution\nprint(type(list1))\n\n&lt;class 'list'&gt;\n\n\nExplanation: The output shows &lt;class 'list'&gt;, confirming that list1 is indeed a list object."
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#indexing",
    "href": "files/labs/lab2/lab2_sln.html#indexing",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Indexing",
    "text": "Indexing\n\nTask 2 Solution\nCreate a list with the numbers 1 through 10, inclusive, and assign this to a variable called x.\n\n# Solution\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n# Alternative using range:\n# x = list(range(1, 11))\nprint(x)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\nRun the code x[1].\n\n# Solution\nprint(x[1])\n\n2\n\n\nRun the code x[0].\n\n# Solution\nprint(x[0])\n\n1\n\n\nExplanation: - x[1] returns 2 (the second element, since indexing starts at 0) - x[0] returns 1 (the first element)\n\n\nTask 3 Solution\nCreate a list called x that contains the elements 1, \"two\", 3.5, \"four\", and \"five five\". Answer the questions and verify.\n\n# Create the list\nx = [1, \"two\", 3.5, \"four\", \"five five\"]\n\n# Predictions as comments:\n# 1. type(x) would output: &lt;class 'list'&gt;\n# 2. type(x[1]) would output: &lt;class 'str'&gt;\n# 3. x[0] would output: 1\n\nNow verify the answers:\n\n# Verify answers\nprint(\"1. type(x):\", type(x))\nprint(\"2. type(x[1]):\", type(x[1]))\nprint(\"3. x[0]:\", x[0])\n\n1. type(x): &lt;class 'list'&gt;\n2. type(x[1]): &lt;class 'str'&gt;\n3. x[0]: 1\n\n\nExplanation: 1. type(x) returns &lt;class 'list'&gt; because x is a list 2. type(x[1]) returns &lt;class 'str'&gt; because x[1] is “two”, which is a string 3. x[0] returns 1, the first element of the list"
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#tables",
    "href": "files/labs/lab2/lab2_sln.html#tables",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Tables",
    "text": "Tables\nFirst, let’s install and import the datascience module:\n\n# Install datascience if needed (uncomment if necessary)\n# !pip install datascience\nfrom datascience import *\n\n\nTask 4 Solution\nRead the list of methods for Table objects and write down three different methods:\n\n# Three Table methods with descriptions:\n# .with_columns(): adds specified columns to a table\n# .select(): selects specific columns from a table and returns a new table\n# .where(): filters rows based on a condition and returns a new table with matching rows\n# .num_rows: returns the number of rows in the table\n# .num_columns: returns the number of columns in the table\n\n\n\nTask 5 Solution\nCreate the professor table:\n\n# Solution\nprofs = Table().with_columns(\n    \"Professor\", [\"Dr. Swenson\", \"Dr. Wainwright\", \"Dr. Mouti\"],\n    \"Office\", [\"South Hall\", \"Old Gym\", \"Old Gym\"],\n    \"Course\", [\"PSTAT 130\", \"PSTAT 120A\", \"PSTAT 126\"]\n)\n\nprofs\n\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Swenson\nSouth Hall\nPSTAT 130\n\n\nDr. Wainwright\nOld Gym\nPSTAT 120A\n\n\nDr. Mouti\nOld Gym\nPSTAT 126\n\n\n\n\n\nSelect the column called Course from profs:\n\n# Solution\nprofs.select(\"Course\")\n\n\n\n\nCourse\n\n\n\n\nPSTAT 130\n\n\nPSTAT 120A\n\n\nPSTAT 126\n\n\n\n\n\nCreate a new table called profs_new with an additional row:\n\n# Solution\nprofs_new = profs.with_row([\"Dr. Ravat\", \"South Hall\", \"PSTAT 120B\"])\nprofs_new\n\n\n\n\nProfessor\nOffice\nCourse\n\n\n\n\nDr. Swenson\nSouth Hall\nPSTAT 130\n\n\nDr. Wainwright\nOld Gym\nPSTAT 120A\n\n\nDr. Mouti\nOld Gym\nPSTAT 126\n\n\nDr. Ravat\nSouth Hall\nPSTAT 120B\n\n\n\n\n\nExplanation: The .with_row() method adds a new row to the existing table. We provide the values in the same order as the columns.\n\n\nFiltering Tables Example\n\n# Create example table for filtering\ntable1 = Table().with_columns(\n    \"Course\", [\"PSTAT 5A\", \"PSTAT 120A\", \"PSTAT 130\"],\n    \"Units\", [4, 4, 4],\n    \"Instructor\", [\"Mathlouthi\", \"Johnson\", \"Smith\"]\n)\n\n# Filter for courses taught by Mathlouthi\ntable1.where(\"Instructor\", \"Mathlouthi\")\n\n\n\n\nCourse\nUnits\nInstructor\n\n\n\n\nPSTAT 5A\n4\nMathlouthi\n\n\n\n\n\n\n# Try filtering for an instructor that doesn't exist\ntable1.where(\"Instructor\", \"Wilson\")\n\n\n\n\nCourse\nUnits\nInstructor"
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#arrays",
    "href": "files/labs/lab2/lab2_sln.html#arrays",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Arrays",
    "text": "Arrays\n\nTask 6 Solution\nMake a list and array with the same elements and compare operations:\n\n# Create list and array\nmy_list = [1, 2, 3]\nmy_array = make_array(1, 2, 3)\n\nprint(\"List:\", my_list)\nprint(\"Array:\", my_array)\n\nList: [1, 2, 3]\nArray: [1 2 3]\n\n\n\n# Sum operations\nprint(\"sum(my_list):\", sum(my_list))\nprint(\"sum(my_array):\", sum(my_array))\n\nsum(my_list): 6\nsum(my_array): 6\n\n\n\n# Addition with scalar - this will cause an error for lists\ntry:\n    result = my_list + 2\n    print(\"my_list + 2:\", result)\nexcept TypeError as e:\n    print(\"Error with my_list + 2:\", e)\n\nError with my_list + 2: can only concatenate list (not \"int\") to list\n\n\n\n# Addition with scalar works for arrays\nprint(\"my_array + 2:\", my_array + 2)\n\nmy_array + 2: [3 4 5]\n\n\nExplanation: Arrays support element-wise operations (like adding 2 to each element), while lists do not. Lists use + for concatenation, not arithmetic."
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#comparisons",
    "href": "files/labs/lab2/lab2_sln.html#comparisons",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Comparisons",
    "text": "Comparisons\n\nTask 7 Solution\nCompare \"statistics\" and \"Statistics\":\n\n# Solution\nprint('\"statistics\" &lt; \"Statistics\":', \"statistics\" &lt; \"Statistics\")\nprint('\"Statistics\" &lt; \"statistics\":', \"Statistics\" &lt; \"statistics\")\n\n# Additional comparisons to understand the pattern\nprint('ord(\"S\"):', ord(\"S\"))\nprint('ord(\"s\"):', ord(\"s\"))\n\n\"statistics\" &lt; \"Statistics\": False\n\"Statistics\" &lt; \"statistics\": True\nord(\"S\"): 83\nord(\"s\"): 115\n\n\nAnswer: When Python compares strings, capital letters are given precedence (they have “lower” ASCII values). Capital letters come before lowercase letters in ASCII ordering, so \"Statistics\" &lt; \"statistics\" returns True.\n\n\nTask 8 Solution\nCreate arrays and compare element-wise:\n\n# Solution\nx = make_array(1, 2, 3)\ny = make_array(2, 3, 1)\n\nprint(\"x:\", x)\nprint(\"y:\", y)\nprint(\"x &lt; y:\", x &lt; y)\n\nx: [1 2 3]\ny: [2 3 1]\nx &lt; y: [ True  True False]\n\n\nExplanation: Python compares arrays element-wise, returning an array of boolean values: - 1 &lt; 2 → True - 2 &lt; 3 → True\n- 3 &lt; 1 → False"
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#conditionals",
    "href": "files/labs/lab2/lab2_sln.html#conditionals",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Conditionals",
    "text": "Conditionals\n\nTask 9 Solution\nPredict and verify the conditional statement:\n\n# Prediction: x will be \"goodbye\"\n# Reasoning: x = 2, so x &lt; 2 is False, but x &lt; 3 is True\n\n# Run the code:\nx = 2\n\nif x &lt; 2:\n    x = \"hello\"\nelif x &lt; 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nprint(\"Result:\", x)\n\nResult: goodbye\n\n\nExplanation: Since x = 2: - x &lt; 2 is False (2 is not less than 2) - x &lt; 3 is True (2 is less than 3) - So the elif condition executes, setting x = \"goodbye\""
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#functions",
    "href": "files/labs/lab2/lab2_sln.html#functions",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Functions",
    "text": "Functions\n\nTask 10 Solution\nThree functions we’ve used in this Lab:\n# Three functions used in this lab:\n# 1. type() - returns the data type of an object\n# 2. print() - displays output to the screen\n# 3. make_array() - creates an array from the given elements\n# Additional: sum(), len(), range()\n\n\nTask 11 Solution\nWrite a Celsius to Fahrenheit conversion function:\n\n# Solution\ndef cent_to_far(c):\n    \"\"\"Convert Celsius to Fahrenheit using the formula F = (9/5) * C + 32\"\"\"\n    fahrenheit = (9/5) * c + 32\n    return fahrenheit\n\n# Test the function\nprint(\"cent_to_far(0):\", cent_to_far(0))    # Should return 32\nprint(\"cent_to_far(20):\", cent_to_far(20))  # Should return 68\n\ncent_to_far(0): 32.0\ncent_to_far(20): 68.0\n\n\nExplanation: The conversion formula is F = (9/5) × C + 32. Our function correctly implements this formula.\n\n\nTask 12 Solution\nWrite a parity function to determine if a number is even or odd:\n\n# Solution\ndef parity(x):\n    \"\"\"Returns 'even' if x is even, 'odd' if x is odd\"\"\"\n    if x % 2 == 0:\n        return \"even\"\n    else:\n        return \"odd\"\n\n# Test the function\nprint(\"parity(2):\", parity(2))  # Should return 'even'\nprint(\"parity(3):\", parity(3))  # Should return 'odd'\n\n# Additional tests\nprint(\"parity(10):\", parity(10))\nprint(\"parity(15):\", parity(15))\n\nparity(2): even\nparity(3): odd\nparity(10): even\nparity(15): odd\n\n\nExplanation: The modulus operator % returns the remainder of division. If x % 2 == 0, then x is divisible by 2 (even). Otherwise, it’s odd."
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#complete-examples-and-additional-practice",
    "href": "files/labs/lab2/lab2_sln.html#complete-examples-and-additional-practice",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Complete Examples and Additional Practice",
    "text": "Complete Examples and Additional Practice\nHere are some additional examples that demonstrate the concepts:\n\nAdvanced List Operations\n\n# List slicing examples\nnumbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(\"First 5 elements:\", numbers[:5])\nprint(\"Last 3 elements:\", numbers[-3:])\nprint(\"Every other element:\", numbers[::2])\n\nFirst 5 elements: [0, 1, 2, 3, 4]\nLast 3 elements: [7, 8, 9]\nEvery other element: [0, 2, 4, 6, 8]\n\n\n\n\nTable Operations\n\n# More complex table operations\nstudents = Table().with_columns(\n    \"Name\", [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"Grade\", [85, 92, 78, 96],\n    \"Year\", [\"Sophomore\", \"Junior\", \"Freshman\", \"Senior\"]\n)\n\n# Multiple operations\nhigh_performers = students.where(\"Grade\", are.above(90))\nprint(\"High performers:\")\nhigh_performers.show()\n\n# Sort by grade\nsorted_students = students.sort(\"Grade\", descending=True)\nprint(\"\\nStudents sorted by grade:\")\nsorted_students.show()\n\nHigh performers:\n\n\n\n\n\nName\nGrade\nYear\n\n\n\n\nBob\n92\nJunior\n\n\nDiana\n96\nSenior\n\n\n\n\n\n\nStudents sorted by grade:\n\n\n\n\n\nName\nGrade\nYear\n\n\n\n\nDiana\n96\nSenior\n\n\nBob\n92\nJunior\n\n\nAlice\n85\nSophomore\n\n\nCharlie\n78\nFreshman\n\n\n\n\n\n\n\nArray Operations\n\nimport numpy as np\n\n# Array mathematical operations\nscores = make_array(85, 92, 78, 96, 89)\nprint(\"Original scores:\", scores)\nprint(\"Curved scores (+5):\", scores + 5)\nprint(\"Squared scores:\", scores ** 2)\nprint(\"Average score:\", np.mean(scores))\n\nOriginal scores: [85 92 78 96 89]\nCurved scores (+5): [ 90  97  83 101  94]\nSquared scores: [7225 8464 6084 9216 7921]\nAverage score: 88.0"
  },
  {
    "objectID": "files/labs/lab2/lab2_sln.html#summary",
    "href": "files/labs/lab2/lab2_sln.html#summary",
    "title": "Lab 2 Solutions: Data Classes and Programming Fundamentals",
    "section": "Summary",
    "text": "Summary\nThis lab covered fundamental Python data structures and programming concepts:\n\nLists: Mutable, mixed-type collections with zero-based indexing\nTables: Structured data with named columns for data analysis\nArrays: Homogeneous collections supporting element-wise operations\nComparisons: Boolean logic and various comparison operators\nConditionals: Decision-making with if/elif/else statements\nFunctions: Creating reusable code blocks with proper documentation\n\nThese concepts form the foundation for data analysis and programming in Python!"
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html",
    "href": "files/worksheets/worksheet2_sln.html",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "",
    "text": "Given:\n\n24 students: average = 74, standard deviation = 8.9\n1 makeup student: score = 64\n\n\n\nSolution: DECREASE\nSince 64 &lt; 74 (the current average), adding this score will pull the average down.\n\n\n\nSolution:\nNew average = (Sum of all 25 scores) / 25\nSum of first 24 scores = 24 × 74 = 1,776\nTotal sum = 1,776 + 64 = 1,840\nNew average = 1,840 / 25 = 73.6 points\n\n\n\nSolution: INCREASE\nThe score of 64 is more than one standard deviation below the original mean (74 - 8.9 = 65.1). This adds more variability to the dataset, increasing the standard deviation.\n\n\n\n\nGiven: - TV watching hours per week - Mean = 4.71 hours - Standard deviation = 4.18 hours\nQuestion: Is the distribution symmetric? What shape? Explain reasoning.\nSolution: NOT SYMMETRIC - RIGHT-SKEWED\nReasoning: 1. The standard deviation (4.18) is nearly as large as the mean (4.71)\n\nSince hours cannot be negative, there’s a natural lower bound at 0\nSome students likely watch much more TV than others, creating a long right tail\nThe large standard deviation relative to the mean suggests high variability\nIn a right-skewed distribution, a few high values (heavy TV watchers) pull the mean higher"
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-a1-mean-and-standard-deviation",
    "href": "files/worksheets/worksheet2_sln.html#problem-a1-mean-and-standard-deviation",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "",
    "text": "Given:\n\n24 students: average = 74, standard deviation = 8.9\n1 makeup student: score = 64\n\n\n\nSolution: DECREASE\nSince 64 &lt; 74 (the current average), adding this score will pull the average down.\n\n\n\nSolution:\nNew average = (Sum of all 25 scores) / 25\nSum of first 24 scores = 24 × 74 = 1,776\nTotal sum = 1,776 + 64 = 1,840\nNew average = 1,840 / 25 = 73.6 points\n\n\n\nSolution: INCREASE\nThe score of 64 is more than one standard deviation below the original mean (74 - 8.9 = 65.1). This adds more variability to the dataset, increasing the standard deviation."
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-a2-distribution-shape-analysis",
    "href": "files/worksheets/worksheet2_sln.html#problem-a2-distribution-shape-analysis",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "",
    "text": "Given: - TV watching hours per week - Mean = 4.71 hours - Standard deviation = 4.18 hours\nQuestion: Is the distribution symmetric? What shape? Explain reasoning.\nSolution: NOT SYMMETRIC - RIGHT-SKEWED\nReasoning: 1. The standard deviation (4.18) is nearly as large as the mean (4.71)\n\nSince hours cannot be negative, there’s a natural lower bound at 0\nSome students likely watch much more TV than others, creating a long right tail\nThe large standard deviation relative to the mean suggests high variability\nIn a right-skewed distribution, a few high values (heavy TV watchers) pull the mean higher"
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-b1-interpreting-histograms",
    "href": "files/worksheets/worksheet2_sln.html#problem-b1-interpreting-histograms",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "2.1 Problem B1: Interpreting Histograms",
    "text": "2.1 Problem B1: Interpreting Histograms\nContext: Infant mortality histogram shows right-skewed distribution with:\n\nHighest bar at 0-10 range (about 38% of countries)\nDecreasing bars: 10-20 (23%), 20-30 (11%)\nLong right tail with few countries having high rates\n\n\n2.1.1 Part (a): Estimate Q1, the median, and Q3 from the histogram.\nSolution: Looking at cumulative percentages:\n\nQ1 (25th percentile) ≈ 8 deaths per 1,000 live births\nMedian (50th percentile) ≈ 15 deaths per 1,000 live births\nQ3 (75th percentile) ≈ 35 deaths per 1,000 live births\n\n\n\n2.1.2 Part (b): Would you expect the mean to be smaller or larger than the median? Explain.\nSolution: MEAN &gt; MEDIAN\nReasoning: - The distribution is right-skewed\n\nThe long right tail contains countries with very high infant mortality rates\nThese extreme values pull the mean higher than the median\nIn right-skewed distributions, the mean is always greater than the median\nThe median is resistant to outliers, but the mean is affected by them"
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-b2-comparing-distributions",
    "href": "files/worksheets/worksheet2_sln.html#problem-b2-comparing-distributions",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "2.2 Problem B2: Comparing Distributions",
    "text": "2.2 Problem B2: Comparing Distributions\nBased on the plots showing Gain vs No Gain counties:\n\n2.2.1 Center:\n\nGain group has higher median household income (~$55,000)\nNo Gain group has lower median household income (~$45,000)\n\n\n\n2.2.2 Variability:\n\nGain group shows less variability (tighter distribution)\nNo Gain group shows greater variability (wider spread)\n\n\n\n2.2.3 Shape:\n\nBoth groups are right-skewed\nShape is relatively consistent between groups\nBoth have longer right tails\n\n\n\n2.2.4 Modes:\n\nEach group has one prominent mode\nGain group: mode around $50,000-$55,000\nNo Gain group: mode around $40,000-$45,000"
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-c1-basic-variance-calculations",
    "href": "files/worksheets/worksheet2_sln.html#problem-c1-basic-variance-calculations",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "3.1 Problem C1: Basic Variance Calculations",
    "text": "3.1 Problem C1: Basic Variance Calculations\nData: 3, 7, 2, 8, 5, 6, 4, 9\n\n3.1.1 Part (a): Calculate the sample mean x̄.\nSolution:\nx̄ = (3 + 7 + 2 + 8 + 5 + 6 + 4 + 9) / 8\nx̄ = 44 / 8 = 5.5\n\n\n3.1.2 Part (b): Calculate the sample variance s² using (n-1).\nSolution:\ns² = Σ(xi - x̄)² / (n-1)\n\nDeviations from mean:\n(3-5.5)² = (-2.5)² = 6.25\n(7-5.5)² = (1.5)² = 2.25  \n(2-5.5)² = (-3.5)² = 12.25\n(8-5.5)² = (2.5)² = 6.25\n(5-5.5)² = (-0.5)² = 0.25\n(6-5.5)² = (0.5)² = 0.25\n(4-5.5)² = (-1.5)² = 2.25\n(9-5.5)² = (3.5)² = 12.25\n\nSum = 6.25 + 2.25 + 12.25 + 6.25 + 0.25 + 0.25 + 2.25 + 12.25 = 42\n\ns² = 42 / (8-1) = 42 / 7 = 6.0\n\n\n3.1.3 Part (c): Calculate the sample standard deviation s.\nSolution:\ns = √s² = √6 = 2.4495\n\n\n3.1.4 Part (d): Population variance σ² if treated as complete population.\nSolution:\nσ² = Σ(xi - μ)² / N\nσ² = 42 / 8 = 5.25\n\n\n3.1.5 Part (e): Why divide by (n-1) for sample variance instead of n?\nSolution: We use (n-1) because of degrees of freedom. When we use the sample mean x̄ to calculate deviations, we “use up” one degree of freedom. The sample mean constrains the data - if we know (n-1) deviations and the sample mean, the last deviation is determined. This makes s² an unbiased estimator of the population variance σ²."
  },
  {
    "objectID": "files/worksheets/worksheet2_sln.html#problem-c2-comparing-variability",
    "href": "files/worksheets/worksheet2_sln.html#problem-c2-comparing-variability",
    "title": "PSTAT 5A Practice Worksheet 2 Solutions",
    "section": "3.2 Problem C2: Comparing Variability",
    "text": "3.2 Problem C2: Comparing Variability\nData Sets: - Set A: 10, 12, 14, 16, 18 - Set B: 5, 10, 14, 18, 23\n\n3.2.1 Part (a): Calculate the mean for each set.\nSolution:\nSet A: x̄_A = (10 + 12 + 14 + 16 + 18) / 5 = 70 / 5 = 14\nSet B: x̄_B = (5 + 10 + 14 + 18 + 23) / 5 = 70 / 5 = 14\n\n\n3.2.2 Part (b): Calculate the sample variance for each set.\nSolution:\nSet A:\nDeviations: (10-14)²=16, (12-14)²=4, (14-14)²=0, (16-14)²=4, (18-14)²=16\nSum = 16 + 4 + 0 + 4 + 16 = 40\ns²_A = 40 / (5-1) = 40 / 4 = 10\nSet B:\nDeviations: (5-14)²=81, (10-14)²=16, (14-14)²=0, (18-14)²=16, (23-14)²=81  \nSum = 81 + 16 + 0 + 16 + 81 = 194\ns²_B = 194 / (5-1) = 194 / 4 = 48.5\n\n\n3.2.3 Part (c): Which set has greater variability?\nSolution: SET B has greater variability\nSet B has variance = 48.5 vs Set A variance = 10\n\n\n3.2.4 Part (d): Calculate coefficient of variation for each set. Which has greater relative variability?\nSolution:\nCV_A = s_A / x̄_A = √10 / 14 = 3.162 / 14 = 0.2259\nCV_B = s_B / x̄_B = √48.5 / 14 = 6.964 / 14 = 0.4974\nSET B has greater relative variability (CV_B = 0.4974 &gt; CV_A = 0.2259)\nNote: The coefficient of variation measures variability relative to the mean, making it useful for comparing datasets with different units or scales."
  },
  {
    "objectID": "files/lecture_notes/Lecture_1/lecture1.html",
    "href": "files/lecture_notes/Lecture_1/lecture1.html",
    "title": "Lecture 1: Introduction to Data Science",
    "section": "",
    "text": "Welcome to PSTAT5A!\nInstructor\n\nNarjes Mathlouthi (nmathlouthi@ucsb.edu)\n\nOffice Hours (Zoom):\n\nThursdays 11 AM–12 PM\n\n\nTeaching Assistants\n\nSummer Lee (sle@ucsb.edu)\nMingzhu He (mingzhuhe@ucsb.edu)\n\n\n\n\n\nCourse Resources\n\nCanvas: Grades & Announcements\nGradescope: Quizzes & Labs\n\nEntry code: WJ4XR7\n\n\nCourse Website: bit.ly/3Ga8CSK\n\nAll lecture slides, labs, and code will be posted here\n\n\n\n\n\n\n\n\n\n\n\n\nCourse Resources\nLabs: Interactive, hands‐on Python computing sessions hosted on JupyterHub.\nAccess the lab environment at: https://pstat5a.lsit.ucsb.edu\n\n\n\n\n\n\n\n\n\nAll relevant course material will be posted on the website. Quizzes are the only exception and will be administered via Gradescope. Please read the syllabus fully and carefully!\n\n\n\n\n\nCommunication & Email\n\nPriority: Bring non‐urgent questions to office hours or after lecture rather than emailing.\n\nEmail Subject: Always include [PSTAT 5A] to help us sort and reply efficiently.\n\nResponse Time: Please allow 24–48 hours for replies; avoid sending emails over weekends.\n\n\n\n\n\n\n\n\n\n\n\nWhat is Data Science?\n\nNo single agreed-upon definition\n\nA cross-disciplinary field:\n\nStatistics: theory of modeling & randomness\n\nComputer Science: computation & data handling\n\n\n\n\n\n\n\n\n\n\n\nWhy Theory Matters\n\nData today is huge computation alone isn’t enough\n\nTheory guides how and why we apply tools\n\nEmployers need analysts who understand and apply\n\n\n\nPath Forward: Course Outline\n\nDescriptive Statistics: Summarize & visualize data\n\nProbability: Random variables & distributions\n\nInferential Statistics: Confidence intervals & hypothesis tests\n\nRegression: Modeling relationships\n\nData Collection: Sampling & study design\n\n\n\nWhy Should I Care?\n\nData is everywhere, any field dealing with data needs these skills\n\nCompanies seek insightful analysts, not just code runners\n\nThis course equips you with both theory and practice\n\n\n\nLet’s Get Started!\nReady to dive into Descriptive Statistics?"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html",
    "href": "files/lecture_notes/lecture5/lecture5.html",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#sec-objectives",
    "href": "files/lecture_notes/lecture5/lecture5.html#sec-objectives",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#practice-problem-1",
    "href": "files/lecture_notes/lecture5/lecture5.html#practice-problem-1",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\n\n\n\nA standard deck has 52 cards. What is the probability of drawing:\n\nA heart \\(\\heartsuit\\)?\nA face card (Jack, Queen, King)?\nThe ace of spades \\(\\spadesuit\\)?\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{heart}) = \\frac{13}{52} = \\frac{1}{4}\\)\n\\(P(\\text{face card}) = \\frac{12}{52} = \\frac{3}{13}\\)\n\\(P(\\text{ace of spades}) = \\frac{1}{52}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#the-addition-rule",
    "href": "files/lecture_notes/lecture5/lecture5.html#the-addition-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Addition Rule",
    "text": "The Addition Rule\nFor any two events \\(A\\) and \\(B\\):\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\n\nWhy subtract \\(P(A \\cap B)\\)?\n\n\n\nSolution. We don’t want to double-count outcomes that are in both events"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#addition-rule-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#addition-rule-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Addition Rule Example",
    "text": "Addition Rule Example\n\n\n\n\nDrawing from a standard deck:\n\n\\(A\\): Drawing a heart \\(\\heartsuit\\) (\\(P(A) = \\frac{13}{52}\\))\n\\(B\\): Drawing a face card (\\(P(B) = \\frac{12}{52}\\))\nWhat’s \\(P(A \\cup B)\\) (heart OR face card)?"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#conditional-probability",
    "href": "files/lecture_notes/lecture5/lecture5.html#conditional-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n🎯Conditional probability is the probability of event \\(A\\) given that event \\(B\\) has occurred\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nprovided \\(P(B) &gt; 0\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#conditional-probability-interpretation",
    "href": "files/lecture_notes/lecture5/lecture5.html#conditional-probability-interpretation",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability Interpretation",
    "text": "Conditional Probability Interpretation\n\\(P(A|B)\\) means:\n\nWe know event  \\(B\\) has occurred \nWhat’s the probability that \\(A\\) also occurred?\nWe “restrict” our sample space to only outcomes in \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#conditional-probability-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#conditional-probability-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability Example",
    "text": "Conditional Probability Example\n\n\n\nDrawing a card from a standard deck:\n\n\\(A\\): Card is a heart \\(\\heartsuit\\)\n\\(B\\): Card is red\nQ: Find \\(P(A|B)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. \\(P(A \\cap B) = P(\\text{heart}) = \\frac{13}{52}\\)\n\\(P(B) = P(\\text{red}) = \\frac{26}{52}\\)\n\\(P(A|B) = \\frac{13/52}{26/52} = \\frac{13}{26} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#independence",
    "href": "files/lecture_notes/lecture5/lecture5.html#independence",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\n🎯 Definition Events \\(A\\) and \\(B\\) are independent if:\n\\[P(A|B) = P(A)\\]\nor equivalently:\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\nKnowing that \\(B\\) occurred doesn’t change the probability of \\(A\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#independence-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#independence-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Independence Example",
    "text": "Independence Example\n\nTwo coin flips:\n\n\\(A\\): First flip is heads\n\\(B\\): Second flip is heads\n\nQ: Are \\(A\\) and \\(B\\) independent?\n\n\n\nSolution. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\)\n\\(P(A \\cap B) = P(\\text{HH}) = \\frac{1}{4}\\)\n\\(P(A) \\times P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\\)\nYes, they are independent!"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#questions",
    "href": "files/lecture_notes/lecture5/lecture5.html#questions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Conditional Probability & Bayes Theorem"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#resources",
    "href": "files/lecture_notes/lecture5/lecture5.html#resources",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms\nInteractive Set Theory & Conditional Probability"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "",
    "text": "Introduction to Probability\nUnderstanding uncertainty through statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#welcome-to-lecture-4",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#welcome-to-lecture-4",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "",
    "text": "Introduction to Probability\nUnderstanding uncertainty through statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#sec-objectives",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#sec-objectives",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#sec-prob",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#sec-prob",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "What is Probability?",
    "text": "What is Probability?\n\n🎯 Definition\nProbability is a measure of the likelihood that an event will occur"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-range",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-range",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Range",
    "text": "Probability Range\n\n\n\nRanges from 0 to 1 (or 0% to 100%)\n0: Event will never occur (impossible)\n1: Event will certainly occur (certain)\n0.5: Event has equal chance of occurring or not"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#three-ways-to-express-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#three-ways-to-express-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Three Ways to Express Probability",
    "text": "Three Ways to Express Probability\n\nFraction: \\(\\frac{1}{2}\\), \\(\\frac{3}{4}\\), \\(\\frac{2}{6}\\)\nDecimal: 0.5, 0.75, 0.33\nPercentage: 50%, 75%, 33%"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Example",
    "text": "Example\nWhen we roll a die, there are six possible outcomes:\n1, 2, 3, 4, 5, 6.\nThe probability of any of them turning up is 1/6 or 16%."
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#why-study-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#why-study-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Why Study Probability?",
    "text": "Why Study Probability?\nProbability helps us:\n\nMake decisions under uncertainty\nUnderstand random processes\nAnalyze data and draw conclusions\nModel real-world phenomena\nAssess risk and likelihood\n\n\n\nApplications: Weather forecasting, medical diagnosis, finance, quality control, gaming, insurance"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#random-experiments",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#random-experiments",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Random Experiments",
    "text": "Random Experiments\nA random experiment is a process that:\n\nCan be repeated under similar conditions\nHas multiple possible outcomes\nThe outcome cannot be predicted with certainty\n\n\n\nExamples\n\n🪙 Flipping a coin\n🎲 Rolling a die\n🃏 Drawing a card from a deck\n💡 Measuring the lifetime of a light bulb"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#sample-space",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#sample-space",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Sample Space",
    "text": "Sample Space\n\n🎯 Definition The sample space (denoted \\(S\\) or \\(\\Omega\\)) is the set of all possible outcomes of a random experiment\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a blank canvas with a rectangle representing the sample space\nfig, ax = plt.subplots(figsize=(6, 4))\nrect = patches.Rectangle(\n    (0.1, 0.1), 0.8, 0.8,\n    linewidth=3, edgecolor='black', facecolor='none'\n)\nax.add_patch(rect)\n\n# Label the sample space inside the rectangle\nax.text(0.5, 0.88, r'$\\Omega$', fontsize=24, fontweight='bold', ha='center', va='top')\nax.text(0.5, 0.5, 'All possible outcomes', fontsize=14, ha='center')\n\n# Clean up axes\nax.set_xticks([])\nax.set_yticks([])\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\nplt.title('Sample Space (Ω)', fontsize=16, pad=20)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#sample-space-examples",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#sample-space-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Sample Space Examples",
    "text": "Sample Space Examples\n\nCoin flip: \\(S = \\{H, T\\}\\)\nTwo coin flips: \\(S = \\{HH, HT, TH, TT\\}\\)\n\n\n\n\n\n\n\n\n🎲 Die roll: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\)\nTwo die rolls \n\n\n\n\\(S = \\{A\\heartsuit,\\ 2\\heartsuit,\\ \\dots,\\ K\\heartsuit,\\\\\n\\phantom{S = \\{}A\\diamondsuit,\\ 2\\diamondsuit,\\ \\dots,\\ K\\diamondsuit,\\\\\n\\phantom{S = \\{}A\\clubsuit,\\ 2\\clubsuit,\\ \\dots,\\ K\\clubsuit,\\\\\n\\phantom{S = \\{}A\\spadesuit,\\ 2\\spadesuit,\\ \\dots,\\ K\\spadesuit\\}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#types-of-sample-spaces",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#types-of-sample-spaces",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Types of Sample Spaces",
    "text": "Types of Sample Spaces\n\n\nFinite Sample Space\n\nLimited number of outcomes\n\nExample: Rolling a die\n\n\n\n\n\n\n\n\n\n\n\n\nInfinite Sample Space\n\nUnlimited outcomes (countable or uncountable)\n\nExample: Measuring exact height of students"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#events",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#events",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Events",
    "text": "Events\n\n🎯 Definition An event is a subset of the sample space\n\nSimple event: Contains exactly one outcome (Ex: \\(A = \\{3\\}\\) (rolling a 3))\nCompound event: Contains multiple outcomes (Ex: \\(B = \\{2, 4, 6\\}\\) (rolling an even number))\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as mlines\n\n# Coordinates and styling\noutcomes = list(range(1, 7))\ny = 0.5\nradius = 0.3\ndot_y = y\nfig, ax = plt.subplots(figsize=(8, 4))\nax.axis('off')\n\n# Rectangle bounds to fit all circles\nmin_x = min(outcomes) - radius\nmax_x = max(outcomes) + radius\nmin_y = dot_y - radius\nmax_y = dot_y + radius\nrect = patches.Rectangle((min_x, min_y), max_x-min_x, max_y-min_y,\n                         linewidth=2, edgecolor='black', facecolor='none')\nax.add_patch(rect)\n\n# Label the sample space inside\nax.text((min_x+max_x)/2, max_y + 0.05, r'$\\Omega$', fontsize=20, fontweight='bold', ha='center')\n\n# Plot all outcomes marked by gray crosses\nax.scatter(outcomes, [dot_y]*6, s=200, color='gray', marker='x', linewidths=3, zorder=2)\n\n# Highlight simple event {3} in red\ncircle_simple = patches.Circle((3, dot_y), radius, facecolor='red', alpha=0.3,\n                               edgecolor='black', linewidth=2, zorder=1)\nax.add_patch(circle_simple)\n\n# Highlight compound event {2,4,6} in blue\nfor x in [2, 4, 6]:\n    circle = patches.Circle((x, dot_y), radius, facecolor='blue', alpha=0.3,\n                            edgecolor='black', linewidth=2, zorder=1)\n    ax.add_patch(circle)\n\n# Legend for events\nlegend_handles = [\n    mlines.Line2D([], [], color='red', marker='o', linestyle='None',\n                  markersize=15, label='Simple event {3}', alpha=0.3),\n    mlines.Line2D([], [], color='blue', marker='o', linestyle='None',\n                  markersize=15, label='Compound event {2,4,6}', alpha=0.3)\n]\nax.legend(handles=legend_handles, loc='lower center', ncol=2, frameon=False, bbox_to_anchor=(0.5, -0.3))\n\n# Adjust limits\nax.set_xlim(min_x - 0.5, max_x + 0.5)\nax.set_ylim(min_y - 0.2, max_y + 0.2)\n\nplt.title('Sample Space and Events', fontsize=16, pad=20)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#event-notation",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#event-notation",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Event Notation",
    "text": "Event Notation\nFor a die roll with \\(S = \\{1, 2, 3, 4, 5, 6\\}\\):\n\n\\(A = \\{1, 3, 5\\}\\) (rolling an odd number)\n\\(B = \\{4, 5, 6\\}\\) (rolling 4 or higher)\n\\(C = \\{6\\}\\) (rolling a six)\n\n\n\n\n\n\n\nWe can describe events in words or using set notation"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#set-operations-overview",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#set-operations-overview",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Set Operations Overview",
    "text": "Set Operations Overview\n\n\n\n\n🎯 Definition:\n\nSet: Collection of distinct objects\nUnion: A OR B occurs\n\nIntersection: A AND B occurs\nComplement: A does NOT occur\nSample Space: All possible outcomes\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# 1) Draw the Venn, but format subset labels to be blank\nv = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    ax=ax,\n    subset_label_formatter=lambda x: \"\"     # removes the \"1\" labels\n)\n\n# 2) Color each region (optional)\nv.get_patch_by_id('10').set_color('lightblue')\nv.get_patch_by_id('01').set_color('lightcoral')\nv.get_patch_by_id('11').set_color('lightgreen')\n\n# 3) Add a dashed universe rectangle around S, with high z‐order\nrect = patches.Rectangle(\n    (-1, -1),    # lower‐left corner\n    2, 2,        # width, height\n    linewidth=2,\n    edgecolor='black',\n    facecolor='none',\n    linestyle='--',\n    zorder=10     # put on top of the circles\n)\nax.add_patch(rect)\n\n# 4) Label the universe \"S\"\nax.text(-.95,  0.95, 'S', fontsize=14, fontweight='bold',\n        va='top', ha='left', zorder=11)\n\n# 5) Tidy up\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.set_axis_off()\n\nplt.title('Sample Space $S$ with Events $A$ and $B$')\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#what-is-a-set",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#what-is-a-set",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "What is a Set?",
    "text": "What is a Set?\n\n\n\n\n🎯 Definition: A collection of things that share common characteristics. They can be elements, members, objects or similar terms.\nExamples:\n\nSet of even numbers:\n\n{2, 4, 6, 8, …}\n\nSet of vowels: {a, e, i, o, u}\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Draw the Venn with no default set labels and blank subset labels\nv = venn2(\n    subsets=(1, 0, 0),\n    set_labels=('', ''),                       # no set labels\n    subset_label_formatter=lambda x: \"\",       # hide subset‐size labels\n    ax=ax\n)\n\n# Color and shade the A region\npatch = v.get_patch_by_id('10')\npatch.set_color('lightblue')\npatch.set_alpha(0.7)\n\n# Add the dashed universe rectangle for S\nrect = patches.Rectangle(\n    (-1, -1),   # lower‐left corner\n    2,          # width\n    2,          # height\n    linewidth=2,\n    edgecolor='black',\n    facecolor='none',\n    linestyle='--',\n    zorder=2     # above the circles\n)\nax.add_patch(rect)\n\n# Label the universe \"S\"\nax.text(\n    -0.95, 0.95, 'S',\n    fontsize=14, fontweight='bold',\n    va='top', ha='left',\n    zorder=3\n)\n\n# Label set A inside the circle\nax.text(\n    0, 0, 'SET A',\n    fontsize=14, fontweight='bold',\n    ha='center',\n    zorder=3\n)\n\n# Adjust limits and styling\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\n\nplt.title('A Single Set $A$ within Sample Space $S$')\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#union-a-b",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#union-a-b",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Union: A ∪ B",
    "text": "Union: A ∪ B\n\n\n\n\n🎯 Definition: Contains all set elements, including intersections.\nIn Probability: The event that A OR B occurs (or both).\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\n# Create figure and axes\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Draw Venn without default numeric labels\nv = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    subset_label_formatter=lambda x: \"\",\n    ax=ax\n)\n\n# Fill entire circles for A and B with semi-transparent colors\nfor region in ('10', '11'):  # parts of A\n    patch = v.get_patch_by_id(region)\n    patch.set_facecolor('skyblue')\n    patch.set_alpha(0.5)\n    patch.set_edgecolor('black')\n    patch.set_linewidth(2)\nfor region in ('01', '11'):  # parts of B\n    patch = v.get_patch_by_id(region)\n    patch.set_facecolor('lightcoral')\n    patch.set_alpha(0.5)\n    patch.set_edgecolor('black')\n    patch.set_linewidth(2)\n\n# Union label at center\nax.text(\n    0, 0, r'$\\mathbf{A \\cup B}$',\n    fontsize=18, fontweight='bold', ha='center', va='center'\n)\n\n# Add the universe rectangle (sample space)\nrect = patches.Rectangle(\n    (-1.1, -1.1), 2.2, 2.2,\n    linewidth=2, edgecolor='black', facecolor='none', linestyle='--'\n)\nax.add_patch(rect)\n\n# Label the universe \"Ω\"\nax.text(\n    -1.05, 1.05, 'Ω',\n    fontsize=16, fontweight='bold', va='top', ha='left'\n)\n\n# Increase set label font size\nv.get_label_by_id('A').set_fontsize(14)\nv.get_label_by_id('B').set_fontsize(14)\n\n# Title and styling\nax.set_title('Union of Events: $A \\\\cup B$', fontsize=16, pad=20)\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\n\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#intersection-a-b",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#intersection-a-b",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Intersection: A ∩ B",
    "text": "Intersection: A ∩ B\n\n\n\n\n🎯 Definition: Area where two or more sets overlap.\nIn Probability: The event that A AND B occurs.\nProperties:\n\nAlways smaller than or equal to individual sets\nCan be empty (disjoint sets)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\n# Create figure and axes\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Draw Venn without subset‐size labels\nv = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    subset_label_formatter=lambda x: \"\",  # hide the \"1\" labels\n    ax=ax\n)\n\n# Shade the non‐intersection regions light gray\nfor region in ('10', '01'):\n    patch = v.get_patch_by_id(region)\n    patch.set_color('lightgray')\n    patch.set_alpha(0.8)\n\n# Shade the intersection region red\npatch = v.get_patch_by_id('11')\npatch.set_color('red')\npatch.set_alpha(0.8)\n\n# Add the universe rectangle\nrect = patches.Rectangle(\n    (-1, -1),    # lower‐left corner\n    2,           # width\n    2,           # height\n    linewidth=2,\n    edgecolor='black',\n    facecolor='none',\n    linestyle='--',\n    zorder=2      # above circles\n)\nax.add_patch(rect)\n\n# Label the universe \"S\"\nax.text(\n    -0.95, 0.95, 'S',\n    fontsize=14, fontweight='bold',\n    va='top', ha='left',\n    zorder=3\n)\n\n# Label the intersection inside\nax.text(\n    0, 0, 'A ∩ B',\n    fontsize=14, fontweight='bold',\n    ha='center', va='center',\n    zorder=3\n)\n\n# Tidy up\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\n\nplt.title('Intersection: $A \\\\cap B$ (A AND B)')\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#absolute-complement-ac",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#absolute-complement-ac",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Absolute Complement: \\(A^c\\)",
    "text": "Absolute Complement: \\(A^c\\)\n\n\n\n🎯 Definition: All elements that do not belong to the set.\nIn Probability: The event that A does NOT occur.\n\n\\(P(A^c) = 1 - P(A)\\)\n\nKey Property:\n\n\\(A \\cup A^c = S\\) (Sample Space)\n\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2, venn2_circles\n\n# 1) Create figure and axes\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# 2) Draw the universe rectangle (purple background)\nrect = patches.Rectangle(\n    (-1, -1),  # lower-left corner\n    2, 2,      # width, height\n    linewidth=2,\n    edgecolor='black',\n    facecolor='purple',\n    alpha=0.3,\n    zorder=1\n)\nax.add_patch(rect)\n\n# 3) Draw Venn diagram with blank subset labels\nv = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    subset_label_formatter=lambda x: \"\",\n    ax=ax\n)\n\n# 4) Mask out A (both A‐only and intersection) with white\nfor region in ('10', '11'):\n    p = v.get_patch_by_id(region)\n    p.set_facecolor('white')\n    p.set_edgecolor('none')\n    p.set_alpha(1)\n\n# 5) Hide the B‐only region so purple shows through\np_b = v.get_patch_by_id('01')\np_b.set_facecolor('none')\np_b.set_edgecolor('none')\np_b.set_alpha(1)\n\n# 6) Draw crisp circle outlines on top\nvenn2_circles((1, 1, 1), ax=ax, linestyle='solid', linewidth=2, color='black')\n\n# 7) Label universe and complement\nax.text(-0.95, 0.95, 'S', fontsize=14, fontweight='bold', va='top', ha='left', zorder=3)\nax.text(-.6, 0.75, r'$A^c$', fontsize=14, fontweight='bold', ha='center', va='center', zorder=3)\n\n# 8) Final styling\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\nplt.title('Complement: $A^c$ (NOT A)')\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#summary-table",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#summary-table",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Summary Table",
    "text": "Summary Table\n\n\n\n\n\n\n\n\n\nOperation\nSymbol\nMeaning\nProbability\n\n\n\n\nUnion\n\\(A \\cup B\\)\nOccurs if \\(A\\) or \\(B\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nIntersection\n\\(A \\cap B\\)\nOccurs if \\(A\\) and \\(B\\)\n\\(P(A \\cap B)\\)\n\n\nComplement\n\\(A^c\\)\nOccurs if \\(A\\) does not occur\n\\(P(A^c) = 1 - P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-commutative",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-commutative",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Commutative",
    "text": "Probability Axioms: Commutative\n\nCommutative\n\\(A \\cup B = B \\cup A\\)\n\\(A \\cap B = B \\cap A\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-associative",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-associative",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Associative",
    "text": "Probability Axioms: Associative\n\nAssociative\n\\((A \\cup B) \\cup C = A \\cup (B \\cup C)\\)\n\\((A \\cap B) \\cap C = A \\cap (B \\cap C)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-distributive",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-distributive",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: Distributive",
    "text": "Probability Axioms: Distributive\n\nDistributive\n\\(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\n\\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-de-morgans-laws",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-axioms-de-morgans-laws",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability Axioms: De Morgan’s Laws",
    "text": "Probability Axioms: De Morgan’s Laws\n\nDe Morgan’s Laws\n\\((A \\cup B)^c = A^c \\cap B^c\\)\n\\((A \\cap B)^c = A^c \\cup B^c\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-examples",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Examples",
    "text": "Practice Examples\n\nExample 1: In a class of students:\n\nSet A = Students who like Math\nSet B = Students who like Science\n\nQ: What does A ∪ B represent?\n\n\n\nSolution. Students who like Math OR Science (or both)\n\n\n\n\nExample 2: What does A ∩ B represent?\n\n\n\nSolution. Students who like BOTH Math AND Science\n\n\n\n\nExample 3: What does \\(A^c\\) represent?\n\n\n\nSolution. Students who do NOT like Math"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#example-set-operations",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#example-set-operations",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Example: Set Operations",
    "text": "Example: Set Operations\n\n\nFor die roll \\(S = \\{1, 2, 3, 4, 5, 6\\}\\):\n\n\\(A = \\{1, 3, 5\\}\\) (odd numbers)\n\\(B = \\{4, 5, 6\\}\\) (4 or higher)\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Correct subset sizes: A only = 2, B only = 2, A ∩ B = 1\nv = venn2(\n    subsets=(2, 2, 1),\n    set_labels=('A', 'B'),\n    subset_label_formatter=lambda x: \"\",  # hide the count labels\n    ax=ax\n)\n\n# Label each region\nv.get_label_by_id('10').set_text('A ∖ B\\n{1, 3}')\nv.get_label_by_id('01').set_text('B ∖ A\\n{4, 6}')\nv.get_label_by_id('11').set_text('A ∩ B\\n{5}')\n\n# Draw the universe rectangle for S\nrect = patches.Rectangle(\n    (-1, -1),   # lower-left corner\n    2, 2,       # width, height\n    linewidth=2,\n    edgecolor='black',\n    facecolor='none',\n    linestyle='--',\n    zorder=1\n)\nax.add_patch(rect)\n\n# Label the universe\nax.text(\n    -0.95, 0.95, 'S = {1,2,3,4,5,6}',\n    fontsize=12, fontweight='bold',\n    va='top', ha='left',\n    zorder=2\n)\n\n# Final styling\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\n\nplt.title('Odd Numbers vs. 4 or Higher')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFind:\n\n\\(A \\cup B\\)\n\\(A \\cap B\\)\n\\(A^c\\)\n\n\n\n\n\nSolution. \n\n\\(A \\cup B = \\{1, 3, 4, 5, 6\\}\\)\n\\(A \\cap B = \\{5\\}\\)\n\\(A^c = \\{2, 4, 6\\}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-events",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-events",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive Events",
    "text": "Mutually Exclusive Events\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive (or disjoint) if they cannot occur simultaneously\n\\[A \\cap B = \\emptyset\\]\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib_venn import venn2\n\n# Create figure and axes\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Draw Venn diagram with blank subset labels\nv = venn2(\n    subsets=(3, 3, 0),\n    set_labels=('A', 'B'),\n    subset_label_formatter=lambda x: \"\",\n    ax=ax\n)\n\n# Color and shade regions\nv.get_patch_by_id('10').set_color('lightblue')\nv.get_patch_by_id('01').set_color('lightcoral')\nv.get_patch_by_id('10').set_alpha(0.7)\nv.get_patch_by_id('01').set_alpha(0.7)\n\n# Label each region explicitly\nv.get_label_by_id('10').set_text('A: {1,3,5}')\nv.get_label_by_id('01').set_text('B: {2,4,6}')\n\n# Add universe rectangle for S\nrect = patches.Rectangle(\n    (-1, -1), 2, 2,\n    linewidth=2,\n    edgecolor='black',\n    facecolor='none',\n    linestyle='--',\n    zorder=1\n)\nax.add_patch(rect)\n\n# Label the universe\nax.text(\n    -0.95, 0.95, 'S = {1, 2, 3, 4, 5, 6}',\n    fontsize=12, fontweight='bold',\n    va='top', ha='left',\n    zorder=2\n)\n\n# Final styling\nax.set_xlim(-1.2, 1.2)\nax.set_ylim(-1.2, 1.2)\nax.set_aspect('equal')\nax.axis('off')\n\nplt.title('Mutually Exclusive: Odd vs. Even')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhen rolling a die\n\n\\(A = \\{1, 3, 5\\}\\) (odd)\n\\(B = \\{2, 4, 6\\}\\) (even)\n\n\\(A\\) and \\(B\\) are mutually exclusive"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#the-classical-definition-of-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#the-classical-definition-of-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Classical Definition of Probability",
    "text": "The Classical Definition of Probability\n\n🎯 Definition: For equally likely outcomes:\n\\[P(A) = \\frac{\\text{Number of outcomes in } A}{\\text{Total number of outcomes in } S}\\]\n\n\nProbability of rolling an even number on a fair die\n\n\n\\[P(\\text{even}) = \\frac{3}{6} = \\frac{1}{2}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#properties-of-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#properties-of-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Properties of Probability",
    "text": "Properties of Probability\n\nNon-negativity: \\(P(A) \\geq 0\\) for any event \\(A\\)\nNormalization: \\(P(S) = 1\\)\nAdditivity: If \\(A\\) and \\(B\\) are mutually exclusive, then\n\n\n\\[P(A \\cup B) = P(A) + P(B)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#the-complement-rule",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#the-complement-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Complement Rule",
    "text": "The Complement Rule\n\n\\[P(A) + P(A^c) = 1\\]\n\\[P(A^c) = 1 - P(A)\\]\n\n\n\nIf the probability of rain is 0.3, what’s the probability of no rain?\n\n\n\n\nSolution. \\[P(\\text{no rain}) = 1 - P(\\text{rain}) = 1 - 0.3 = 0.7\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-1",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-1",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\n\n\n\nA standard deck has 52 cards. What is the probability of drawing:\n\nA heart \\(\\heartsuit\\)?\nA face card (Jack, Queen, King)?\nThe ace of spades \\(\\spadesuit\\)?\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{heart}) = \\frac{13}{52} = \\frac{1}{4}\\)\n\\(P(\\text{face card}) = \\frac{12}{52} = \\frac{3}{13}\\)\n\\(P(\\text{ace of spades}) = \\frac{1}{52}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#the-addition-rule",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#the-addition-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "The Addition Rule",
    "text": "The Addition Rule\nFor any two events \\(A\\) and \\(B\\):\n\n\\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\]\n\n\nWhy subtract \\(P(A \\cap B)\\)?\n\n\n\nSolution. We don’t want to double-count outcomes that are in both events"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#addition-rule-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#addition-rule-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Addition Rule Example",
    "text": "Addition Rule Example\n\n\n\n\nDrawing from a standard deck:\n\n\\(A\\): Drawing a heart \\(\\heartsuit\\) (\\(P(A) = \\frac{13}{52}\\))\n\\(B\\): Drawing a face card (\\(P(B) = \\frac{12}{52}\\))\nWhat’s \\(P(A \\cup B)\\) (heart OR face card)?\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\nplt.figure(figsize=(6,4))\nv = venn2(subsets=(10, 9, 3))\nv.get_label_by_id('10').set_text('A: Hearts')\nv.get_label_by_id('01').set_text('B: Face Cards')\nv.get_label_by_id('11').set_text('A ∩ B')\nplt.title('Hearts vs. Face Cards')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Compute region sizes\nonly_hearts = 13 - 3\nonly_face = 12 - 3\nintersection = 3\n\nplt.figure(figsize=(6,4))\nvenn2(subsets=(only_hearts, only_face, intersection), set_labels=('Hearts', 'Face Cards'))\nplt.title('Hearts vs. Face Cards')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSolution. \\(P(A \\cap B) = \\frac{3}{52}\\) (face cards that are hearts)\n\\(P(A \\cup B) = \\frac{13}{52} + \\frac{12}{52} - \\frac{3}{52} = \\frac{22}{52} = \\frac{11}{26}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\n\n\n🎯Conditional probability is the probability of event \\(A\\) given that event \\(B\\) has occurred\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nprovided \\(P(B) &gt; 0\\)\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Define colors and edge style\ncolor_intersection = '#1f77b4'  # blue\ncolor_B = '#ff7f0e'             # orange\nedge_color = 'black'\nlinewidth = 2\ntext_fontsize = 20\nlabel_fontsize = 20\ntitle_fontsize = 20\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- First plot: Intersection Only ---\nv1 = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    ax=axes[0],\n    subset_label_formatter=lambda x: ''\n)\n# Style regions\nfor region in ('10', '01'):\n    patch = v1.get_patch_by_id(region)\n    patch.set_facecolor('none')\n    patch.set_edgecolor(edge_color)\n    patch.set_linewidth(linewidth)\npatch_int1 = v1.get_patch_by_id('11')\npatch_int1.set_facecolor(color_intersection)\npatch_int1.set_alpha(0.5)\npatch_int1.set_edgecolor(edge_color)\npatch_int1.set_linewidth(linewidth)\n# Increase label font sizes\nv1.get_label_by_id('A').set_fontsize(label_fontsize)\nv1.get_label_by_id('B').set_fontsize(label_fontsize)\n# Add bolded P(A∩B) text\nx_int, y_int = v1.get_label_by_id('11').get_position()\naxes[0].text(x_int, y_int, r'$\\mathbf{P(A\\cap B)}$', \n             ha='center', va='center', fontsize=text_fontsize)\naxes[0].set_title('Intersection Only', fontsize=title_fontsize)\n\n# --- Second plot: B Only (including intersection) ---\nv2 = venn2(\n    subsets=(1, 1, 1),\n    set_labels=('A', 'B'),\n    ax=axes[1],\n    subset_label_formatter=lambda x: ''\n)\n# Style regions\npatch_A2 = v2.get_patch_by_id('10')\npatch_A2.set_facecolor('none')\npatch_A2.set_edgecolor(edge_color)\npatch_A2.set_linewidth(linewidth)\nfor region in ('01', '11'):\n    patch = v2.get_patch_by_id(region)\n    patch.set_facecolor(color_B)\n    patch.set_alpha(0.5)\n    patch.set_edgecolor(edge_color)\n    patch.set_linewidth(linewidth)\n# Increase label font sizes\nv2.get_label_by_id('A').set_fontsize(label_fontsize)\nv2.get_label_by_id('B').set_fontsize(label_fontsize)\n# Add bolded B text inside B circle\nx_B, y_B = v2.get_label_by_id('01').get_position()\naxes[1].text(x_B, y_B, r'$\\mathbf{B}$', \n             ha='center', va='center', fontsize=text_fontsize)\naxes[1].set_title('B Only', fontsize=title_fontsize)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability-interpretation",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability-interpretation",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability Interpretation",
    "text": "Conditional Probability Interpretation\n\\(P(A|B)\\) means:\n\nWe know event  \\(B\\) has occurred \nWhat’s the probability that \\(A\\) also occurred?\nWe “restrict” our sample space to only outcomes in \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#conditional-probability-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Conditional Probability Example",
    "text": "Conditional Probability Example\n\n\n\nDrawing a card from a standard deck:\n\n\\(A\\): Card is a heart \\(\\heartsuit\\)\n\\(B\\): Card is red\nQ: Find \\(P(A|B)\\)\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\nplt.figure(figsize=(6,4))\nv = venn2(subsets=(13, 13, 0))\nv.get_label_by_id('10').set_text('A: Hearts')\nv.get_label_by_id('01').set_text('B: Red Cards')\nplt.title('Hearts vs. Red Cards')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSolution. \\(P(A \\cap B) = P(\\text{heart}) = \\frac{13}{52}\\)\n\\(P(B) = P(\\text{red}) = \\frac{26}{52}\\)\n\\(P(A|B) = \\frac{13/52}{26/52} = \\frac{13}{26} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#independence",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#independence",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Independence",
    "text": "Independence\n\n\n\n🎯 Definition Events \\(A\\) and \\(B\\) are independent if:\n\\[P(A|B) = P(A)\\]\nor equivalently:\n\\[P(A \\cap B) = P(A) \\times P(B)\\]\n\n\nKnowing that \\(B\\) occurred doesn’t change the probability of \\(A\\)\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Define probabilities for independent events\nP_A = 0.4\nP_B = 0.5\nP_AB = P_A * P_B  # 0.2\n\n# Subset sizes: only A, only B, intersection\nsubsets = (P_A - P_AB, P_B - P_AB, P_AB)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nv = venn2(subsets=subsets, set_labels=('A', 'B'), ax=ax)\n\n# Style regions\nv.get_patch_by_id('10').set_color('skyblue')    # A only\nv.get_patch_by_id('10').set_alpha(0.5)\nv.get_patch_by_id('01').set_color('lightgreen') # B only\nv.get_patch_by_id('01').set_alpha(0.5)\nv.get_patch_by_id('11').set_color('orange')     # Intersection\nv.get_patch_by_id('11').set_alpha(0.7)\n\n# Annotate margins and intersection inside\nv.get_label_by_id('10').set_text(f'{subsets[0]:.1f}')\nv.get_label_by_id('01').set_text(f'{subsets[1]:.1f}')\nv.get_label_by_id('11').set_text(f'{subsets[2]:.1f}')\n\n# Print P(A) and P(B) to the sides\nax.text(-0.8, 0.6, f'$P(A)={P_A}$', fontsize=14, fontweight='bold')\nax.text(0.8, 0.6, f'$P(B)={P_B}$', fontsize=14, fontweight='bold')\n\nplt.title('Independent Events\\n$P(A\\\\cap B)=P(A)P(B)$', fontsize=16)\nax.axis('equal')\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#independence-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#independence-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Independence Example",
    "text": "Independence Example\n\nTwo coin flips:\n\n\\(A\\): First flip is heads\n\\(B\\): Second flip is heads\n\nQ: Are \\(A\\) and \\(B\\) independent?\n\n\n\nSolution. \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{1}{2}\\)\n\\(P(A \\cap B) = P(\\text{HH}) = \\frac{1}{4}\\)\n\\(P(A) \\times P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\\)\nYes, they are independent!"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-vs.-independent",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-vs.-independent",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive vs. Independent",
    "text": "Mutually Exclusive vs. Independent\n\nMutually Exclusive (left): the circles A and B do not overlap, so \\(P(A\\cap B)=0\\).\nIndependent (right): the circles overlap, and we’ve sized the intersection so that \\(P(A\\cap B)=P(A)\\,P(B)\\).\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Create side-by-side Venn diagrams\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# 1) Mutually Exclusive: no overlap\nvenn2(\n    subsets=(1, 1, 0),\n    set_labels=('A', 'B'),\n    ax=axes[0],\n    subset_label_formatter=lambda x: ''\n)\naxes[0].set_title('Mutually Exclusive\\nP(A∩B) = 0')\n\n# 2) Independent: overlap equals product of areas (e.g., 0.5 * 0.5 = 0.25)\n# Scale counts arbitrarily (25, 25, 25) to represent proportions\nvenn2(\n    subsets=(25, 25, 25),\n    set_labels=('A', 'B'),\n    ax=axes[1],\n    subset_label_formatter=lambda x: ''\n)\naxes[1].set_title('Independent\\nP(A∩B) = P(A)P(B)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-vs.-independent-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#mutually-exclusive-vs.-independent-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive vs. Independent Example",
    "text": "Mutually Exclusive vs. Independent Example\n\n\n\nDraw a single card from a 52-card deck:\n\nLet A={“draw an Ace”}, so P(A)=4/52.\nLet B={“draw a King”}, so P(B)=4/52.\n\nQ: What is \\(P(A\\cap B)\\) ?\n\n\n\n\n\nSolution. They’re disjoint (you can’t draw an Ace and a King), so \\(P(A\\cap B) = 0\\).\nBut \\(P(A)\\,P(B) = \\frac{4}{52}\\times\\frac{4}{52} = \\frac{16}{2704} \\neq 0\\).\nHence, \\(P(A\\cap B)\\neq P(A)P(B)\\), so they’re not independent."
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#multiplication-rule",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#multiplication-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nGeneral case: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nIndependent events: \\(P(A \\cap B) = P(A) \\times P(B)\\)\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Define scaled subset sizes for illustration\n# General case: P(A)=40 (10+30), P(B|A)=0.75 so intersection=30, B-only=20 (if P(B)=50)\n# Independent case: P(A)=40, P(B)=50, intersection=P(A)*P(B)=20\ngeneral_subsets = (10, 20, 30)   # (A-only, B-only, intersection)\nindep_subsets = (20, 30, 20)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- General Multiplication Rule ---\nvenn2(subsets=general_subsets, set_labels=('A', 'B'), ax=axes[0])\naxes[0].set_title('General: P(A∩B) = P(A)·P(B|A)', fontsize=14)\n\n# --- Independent Events ---\nvenn2(subsets=indep_subsets, set_labels=('A', 'B'), ax=axes[1])\naxes[1].set_title('Independent: P(A∩B) = P(A)·P(B)', fontsize=14)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#tree-diagrams",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#tree-diagrams",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\n\n🎯 Definition Tree diagrams help visualize sequential events and calculate probabilities.\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes\ncoords = {\n    'root': (0.1, 0.5),\n    'A': (0.4, 0.7),\n    'B': (0.4, 0.3),\n    'A_Red': (0.7, 0.8),\n    'A_Blue': (0.7, 0.6),\n    'B_Red': (0.7, 0.4),\n    'B_Blue': (0.7, 0.2),\n}\n\n# Probabilities\np_A = 0.7\np_B = 0.3\np_R_A = 0.6\np_B_A = 0.4\np_R_B = 0.3\np_B_B = 0.7\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\n# Draw nodes\nfor node, (x, y) in coords.items():\n    circle = Circle((x, y), 0.05, edgecolor='black', facecolor='white', linewidth=2)\n    ax.add_patch(circle)\n    label = node.replace('_', '\\n')\n    if node == 'root':\n        label = 'Start'\n    ax.text(x, y, label, ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw arrows and annotate probabilities\ndef draw_branch(start, end, text):\n    x1, y1 = coords[start]\n    x2, y2 = coords[end]\n    arrow = FancyArrowPatch((x1+0.05, y1), (x2-0.05, y2), arrowstyle='-&gt;', mutation_scale=20)\n    ax.add_patch(arrow)\n    ax.text((x1+x2)/2, (y1+y2)/2, text, fontsize=12, backgroundcolor='white', ha='center')\n\ndraw_branch('root', 'A', f'{p_A}')\ndraw_branch('root', 'B', f'{p_B}')\ndraw_branch('A', 'A_Red', f'{p_R_A}')\ndraw_branch('A', 'A_Blue', f'{p_B_A}')\ndraw_branch('B', 'B_Red', f'{p_R_B}')\ndraw_branch('B', 'B_Blue', f'{p_B_B}')\n\n# Title\nax.set_title('Tree Diagram: Drawing from Urn A (70%) or B (30%)', fontsize=14, pad=20)\n\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#tree-diagram-examples",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#tree-diagram-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Tree Diagram Examples",
    "text": "Tree Diagram Examples\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes in the full tree\ncoords = {\n    'root':       (0.1, 0.5),\n    'A':          (0.35, 0.75),\n    'NotA':       (0.35, 0.25),\n    'A_B':        (0.7, 0.8),\n    'A_notB':     (0.7, 0.65),\n    'NotA_B':     (0.7, 0.35),\n    'NotA_notB':  (0.7, 0.2)\n}\n\n# Probabilities (example values)\npA     = 0.4\npNotA  = 0.6\npB_A   = 0.75\npNotB_A= 0.25\npB_notA= 0.333\npNotB_notA=0.667\n\n# Colors\ncolor_intersection = '#1f77b4'  # blue\ncolor_B            = '#ff7f0e'  # orange\ngray               = 'lightgray'\n\ndef draw_tree(highlight_branches, branch_color, node_to_color):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n\n    # Draw nodes\n    node_patches = {}\n    for node, (x, y) in coords.items():\n        circ = Circle((x, y), 0.035, edgecolor='black', facecolor='white', lw=2)\n        ax.add_patch(circ)\n        node_patches[node] = circ\n        label = {\n            'root': 'Start',\n            'A': 'A',\n            'NotA': '¬A',\n            'A_B': 'B',\n            'A_notB': '¬B',\n            'NotA_B': 'B',\n            'NotA_notB': '¬B'\n        }[node]\n        ax.text(x, y, label, ha='center', va='center', fontsize=12)\n\n    # Function to draw a branch\n    def draw_branch(start, end, text, color, lw=1.5):\n        x1, y1 = coords[start]\n        x2, y2 = coords[end]\n        arr = FancyArrowPatch((x1+0.035, y1), (x2-0.035, y2),\n                              arrowstyle='-|&gt;', mutation_scale=15,\n                              lw=lw, color=color)\n        ax.add_patch(arr)\n        ax.text((x1+x2)/2, (y1+y2)/2, text, ha='center', va='center',\n                backgroundcolor='white', fontsize=10)\n\n    # Draw all branches in gray\n    branches = [\n        ('root','A', f'{pA}'),\n        ('root','NotA', f'{pNotA}'),\n        ('A','A_B', f'{pB_A}'),\n        ('A','A_notB', f'{pNotB_A}'),\n        ('NotA','NotA_B', f'{pB_notA:.3f}'),\n        ('NotA','NotA_notB', f'{pNotB_notA:.3f}')\n    ]\n    for b in branches:\n        draw_branch(*b, color=gray)\n\n    # Highlight requested branches\n    for b in highlight_branches:\n        # find text label for branch\n        prob = {\n            ('root','A'): f'{pA}',\n            ('root','NotA'): f'{pNotA}',\n            ('A','A_B'): f'{pB_A}',\n            ('A','A_notB'): f'{pNotB_A}',\n            ('NotA','NotA_B'): f'{pB_notA:.3f}',\n            ('NotA','NotA_notB'): f'{pNotB_notA:.3f}'\n        }[b]\n        draw_branch(b[0], b[1], prob, color=branch_color, lw=3)\n    \n    # Color the end-node if desired\n    for node in node_to_color:\n        node_patches[node].set_facecolor(branch_color)\n\n    return fig, ax\n\n# Tree 1: highlight only the intersection branch root-&gt;A-&gt;A_B in blue\nfig1, ax1 = draw_tree(highlight_branches=[('root','A'),('A','A_B')],\n                      branch_color=color_intersection,\n                      node_to_color=['A_B'])\nax1.set_title('Intersection Only (P(A∩B))', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Tree 2: highlight both B branches (A-&gt;B and ¬A-&gt;B) in orange\nfig2, ax2 = draw_tree(highlight_branches=[('root','A'),('A','A_B'),\n                                          ('root','NotA'),('NotA','NotA_B')],\n                      branch_color=color_B,\n                      node_to_color=['A_B','NotA_B'])\nax2.set_title('Event B Only (P(B))', fontsize=14)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-2",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-2",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\n\n\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement.\n\nWhat’s the probability both balls are red?\nWhat’s the probability the first is red and second is blue?\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.lines import Line2D\n\n# Set up figure\nfig, ax = plt.subplots(figsize=(10, 6))\nax.axis('off')\n\n# Node positions\npositions = {\n    'root': (0.1, 0.5),\n    'R': (0.35, 0.7),\n    'B': (0.35, 0.3),\n    'RR': (0.7, 0.8),\n    'RB': (0.7, 0.6),\n    'BR': (0.7, 0.4),\n    'BB': (0.7, 0.2),\n}\n\n# Define colors\ncolor_root = '#ecf0f1'\ncolor_R = '#e74c3c'       # red for R or RR\ncolor_B = '#3498db'       # blue for B or BB\ncolor_mixed = '#ff7f0e'   # orange for mixed (RB, BR)\nedge_color = 'black'\nlinewidth = 2\ntext_fs = 12\n\n# Draw nodes with colored backgrounds and bold labels\nnode_colors = {\n    'root': color_root,\n    'R': color_R,\n    'B': color_B,\n    'RR': color_R,\n    'RB': color_mixed,\n    'BR': color_mixed,\n    'BB': color_B\n}\n\nfor name, (x, y) in positions.items():\n    circ = Circle((x, y), 0.05, facecolor=node_colors[name],\n                  edgecolor=edge_color, linewidth=linewidth, zorder=2)\n    ax.add_patch(circ)\n    ax.text(x, y, name, ha='center', va='center', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Function to draw an arrow and label\ndef draw_arrow(src, dst, label):\n    x1, y1 = positions[src]\n    x2, y2 = positions[dst]\n    ax.annotate('', xy=(x2-0.05, y2), xytext=(x1+0.05, y1),\n                arrowprops=dict(arrowstyle='-&gt;', color=node_colors[dst], lw=linewidth), zorder=1)\n    ax.text((x1+x2)/2, (y1+y2)/2, label, ha='center', va='center',\n            backgroundcolor='white', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Draw branches with probabilities\ndraw_arrow('root', 'R', '5/8')\ndraw_arrow('root', 'B', '3/8')\ndraw_arrow('R', 'RR', '4/7')\ndraw_arrow('R', 'RB', '3/7')\ndraw_arrow('B', 'BR', '5/7')\ndraw_arrow('B', 'BB', '2/7')\n\n# Label leaves with final probabilities in bold\nleaf_probs = {\n    'RR': '20/56',\n    'RB': '15/56',\n    'BR': '15/56',\n    'BB': '6/56'\n}\nfor leaf, prob in leaf_probs.items():\n    x, y = positions[leaf]\n    ax.text(x+0.12, y, prob, ha='left', va='center', fontsize=text_fs, fontweight='bold')\n\n# Legend\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', label='RR / R',\n           markerfacecolor=color_R, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='BB / B',\n           markerfacecolor=color_B, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='RB / BR',\n           markerfacecolor=color_mixed, markersize=12, markeredgecolor=edge_color),\n]\nax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.02, 0.95))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{both red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)\n\\(P(\\text{red then blue}) = \\frac{5}{8} \\times \\frac{3}{7} = \\frac{15}{56}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#law-of-total-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#law-of-total-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n🎯 Definition\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space, then:\n\\[P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\\]\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Ellipse, Rectangle\n\n# Example probabilities\nP_B = [0.3, 0.5, 0.2]\nP_A_given_B = [0.2, 0.6, 0.5]\nP_A_and_B = [P_B[i] * P_A_given_B[i] for i in range(3)]\n\n# Create figure\nfig, ax = plt.subplots(figsize=(9, 5))\nax.set_aspect('equal')\nax.axis('off')\n\n# Draw sample space rectangle\nrect = Rectangle((-1, -2), width=8, height=4, edgecolor='black', facecolor='none', linewidth=2)\nax.add_patch(rect)\nax.text(-0.9, 1.7, r'$\\Omega$', fontsize=14, fontweight='bold', va='top', ha='left')\n\n# Draw B partitions as circles\npositions_B = [(1, 0), (3, 0), (5, 0)]\ncolors_B = ['#ffcc00', '#ff6600', '#ff0066']\n\nfor i, (x, y) in enumerate(positions_B):\n    circle = Circle((x, y), radius=1, facecolor=colors_B[i], edgecolor='black', alpha=0.3)\n    ax.add_patch(circle)\n    ax.text(x, y-1.3, f'$P(B_{i+1})={P_B[i]:.2f}$', ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw A as a large ellipse overlapping all B's\nellipse = Ellipse((3, 0), width=8, height=3, angle=0, facecolor='#1f77b4', edgecolor='black', alpha=0.2)\nax.add_patch(ellipse)\nax.text(3, 1.2, '$A$', ha='center', va='center', fontsize=14, fontweight='bold')\n\n# Annotate intersections P(A ∩ Bi)\nfor i, (x, y) in enumerate(positions_B):\n    ax.text(x, 0, f'$P(A\\\\cap B_{i+1})={P_A_and_B[i]:.2f}$', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n\nax.set_xlim(-1, 7)\nax.set_ylim(-2, 2)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#law-of-total-probability-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\n\nA factory has two machines:\n\nMachine 1: Produces 60% of items, 5% defective\nMachine 2: Produces 40% of items, 3% defective\n\nQ: What’s the overall probability an item is defective?\n\n\n\nSolution. \\(P(\\text{defective}) = P(D|M_1)P(M_1) + P(D|M_2)P(M_2)\\)\n\\(= 0.05 \\times 0.6 + 0.03 \\times 0.4 = 0.03 + 0.012 = 0.042\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n🎯 Definition \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nThis allows us to “reverse” conditional probabilities\nNamed after Thomas Bayes (1701-1761)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-components",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-components",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\n\\(A,B\\): Events\n\\(P(A|B)\\): Posterior probability - what we want to find\n\\(P(B|A)\\): Likelihood - given \\(A\\), probability of observing \\(B\\)\n\\(P(A)\\): Prior probability - initial probability of \\(A\\)\n\\(P(B)\\): Marginal probability - total probability of \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Example",
    "text": "Bayes’ Theorem Example\n\n\n\nMedical test for a disease: 1\n\nDisease affects 1% of population\nTest is 95% accurate for sick people\nTest is 90% accurate for healthy people\n\nQ:If someone tests positive, what’s the probability they have the disease?\n\n\n\nimport plotly.express as px\nimport pandas as pd\n\n# Given probabilities\nP_D = 0.01\nP_notD = 0.99\nP_pos_given_D = 0.95\nP_pos_given_notD = 0.10\n\n# Joint probabilities\nP_D_and_pos = P_D * P_pos_given_D       # True positives\nP_notD_and_pos = P_notD * P_pos_given_notD  # False positives\nP_D_and_neg = P_D * (1 - P_pos_given_D)     # False negatives\nP_notD_and_neg = P_notD * (1 - P_pos_given_notD)  # True negatives\n\n# Prepare DataFrame for treemap (mosaic)\ndf = pd.DataFrame({\n    'Test Result': ['Positive', 'Positive', 'Negative', 'Negative'],\n    'Condition':   ['Disease',   'No Disease', 'Disease',   'No Disease'],\n    'Probability': [P_D_and_pos, P_notD_and_pos, P_D_and_neg, P_notD_and_neg]\n})\n\n# Create treemap mosaic plot\nfig = px.treemap(\n    df,\n    path=['Test Result', 'Condition'],\n    values='Probability',\n    color='Condition',\n    color_discrete_map={'Disease':'tomato', 'No Disease':'skyblue'}\n)\nfig.update_traces(textinfo='label+percent entry')\nfig.update_layout(\n    title='Bayes’ Theorem: Distribution by Test Result and Condition',\n    margin=dict(t=50, l=25, r=25, b=25)\n)\nfig.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-solution",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-solution",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Solution",
    "text": "Bayes’ Theorem Solution\nLet:\n\n\\(D\\): Person has disease\n\\(T^+\\): Test is positive\n\nGiven:\n\n\\(P(D) = 0.01\\)\n\\(P(T^+|D) = 0.95\\)\n\\(P(T^-|D^c) = 0.90\\), so \\(P(T^+|D^c) = 0.10\\)\n\n\n\nSolution. \\(P(T^+) = P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\)\n\\(= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.1085\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-solution-cont.",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#bayes-theorem-solution-cont.",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Solution (cont.)",
    "text": "Bayes’ Theorem Solution (cont.)\n\\[P(D|T^+) = \\frac{P(T^+|D) \\times P(D)}{P(T^+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\]\n\nimport matplotlib.pyplot as plt\n\n# Posterior probabilities given a positive test\nP_D_and_pos = 0.0095\nP_notD_and_pos = 0.099\nP_positive = P_D_and_pos + P_notD_and_pos\n\nP_D_given_pos = P_D_and_pos / P_positive\nP_notD_given_pos = P_notD_and_pos / P_positive\n\n# Pie chart\nlabels = ['Disease (P≈8.8%)', 'No Disease (P≈91.2%)']\nsizes = [P_D_given_pos, P_notD_given_pos]\ncolors = ['tomato', 'skyblue']\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(\n    sizes, labels=labels, colors=colors, autopct='%.1f%%',\n    startangle=90, textprops={'fontsize': 14, 'fontweight': 'bold'}\n)\nax.set_title('Posterior Probability Given Positive Test', fontsize=16, fontweight='bold')\nax.axis('equal')  # Equal aspect ensures pie is circular.\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSurprising result: Even with a positive test, there’s only an 8.8% chance of having the disease!\nThis is due to the low base rate of the disease"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#common-probability-mistakes",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#common-probability-mistakes",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nConfusing \\(P(A|B)\\) with \\(P(B|A)\\)\n\n\nProsecutor’s fallacy is a specific error in interpreting conditional probabilities. Confusing\n\\(P(\\text{Evidence}\\mid\\text{Innocent})\n\\quad\\text{with}\\quad\nP(\\text{Innocent}\\mid\\text{Evidence})\\).\nEx: OJ Simpson Case 2"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#common-probability-mistakes-1",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#common-probability-mistakes-1",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nAssuming independence when events are dependent\nIgnoring base rates (as in the medical test example)\n\n\nBase rate fallacy is when you ignore or underweight the prior probability \\(P(H)\\) of a hypothesis, focusing only on the new evidence \\(E\\).\n\n\nDouble counting in union calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-3",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-3",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nTwo fair dice are rolled. Find:\n\n\\(P(\\text{sum} = 7)\\)\n\\(P(\\text{sum} = 7 | \\text{first die shows 3})\\)\nAre these events independent?\n\n\n\n\nSolution. \n\n6 ways out of 36: \\(P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\nGiven first die is 3, need second die to be 4: \\(P(\\text{sum} = 7 | \\text{first} = 3) = \\frac{1}{6}\\)\nYes, they’re independent since \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#counting-and-probability",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#counting-and-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\nSometimes we need to count outcomes:\n\nMultiplication Principle: If task 1 can be done in \\(m\\) ways and task 2 in \\(n\\) ways, both can be done in \\(m \\times n\\) ways\n\n\nPermutations: Arrangements where order matters \\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\nCombinations: Selections where order doesn’t matter \\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#counting-example",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#counting-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Counting Example",
    "text": "Counting Example\n\nQ: How many ways can you arrange 5 people in a row?\n\n\n\nSolution. This is a permutation: \\(P(5,5) = 5! = 120\\) ways\n\n\n\n\n\nQ:How many ways can you choose 3 people from 5 for a committee?\n\n\n\n\nSolution. This is a combination: \\(C(5,3) = \\binom{5}{3} = \\frac{5!}{3!2!} = 10\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-with-counting",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#probability-with-counting",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability with Counting",
    "text": "Probability with Counting\n\nExample: A committee of 3 people is chosen from 8 people (5 women, 3 men). What’s the probability all 3 are women?\n\n\n\nSolution. Total ways to choose 3 from 8: \\(\\binom{8}{3} = 56\\)\nWays to choose 3 women from 5: \\(\\binom{5}{3} = 10\\)\nProbability: \\(\\frac{10}{56} = \\frac{5}{28}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#real-world-applications",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#real-world-applications",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Diagnosis: Using Bayes’ theorem for test interpretation\nQuality Control: Probability of defective items\nFinance: Risk assessment and portfolio theory\nSports: Probability of wins, fantasy sports\nInsurance: Calculating premiums based on risk"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#key-formulas-summary",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\nBasic probability: \\(P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\)\nComplement: \\(P(A^c) = 1 - P(A)\\)\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nConditional: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nIndependence: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nBayes’: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#problem-solving-strategy",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\nIdentify the sample space and events\nDetermine if events are independent or mutually exclusive\nChoose the appropriate rule or formula\nCalculate step by step\nCheck if your answer makes sense"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-4",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-4",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA bag contains 4 red, 3 blue, and 2 green marbles. Three marbles are drawn without replacement.\nFind the probability that: a) All three are red b) No two are the same color c) At least one is blue"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-4-solutions",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#practice-problem-4-solutions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 4 Solutions",
    "text": "Practice Problem 4 Solutions\n\nSolution. \n\nAll red: \\(\\frac{4}{9} \\times \\frac{3}{8} \\times \\frac{2}{7} = \\frac{24}{504} = \\frac{1}{21}\\)\nDifferent colors: \\(\\frac{4 \\times 3 \\times 2}{9 \\times 8 \\times 7} \\times 3! = \\frac{24 \\times 6}{504} = \\frac{144}{504} = \\frac{2}{7}\\)\nAt least one blue: \\(1 - P(\\text{no blue}) = 1 - \\frac{6 \\times 5 \\times 4}{9 \\times 8 \\times 7} = 1 - \\frac{120}{504} = \\frac{384}{504} = \\frac{16}{21}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#common-questions",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#common-questions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Questions",
    "text": "Common Questions\n\nQ1.: “Why isn’t \\(P(A \\cup B) = P(A) + P(B)\\) always?”\nA: We’d double-count outcomes in both events\nQ2.: “How do I know if events are independent?”\nA: Check if \\(P(A|B) = P(A)\\) or if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nQ3.: “When do I use Bayes’ theorem?”\nA: When you want to “reverse” a conditional probability\n\n\nQ3 note (Bayes Example)\n\nForward: I know my test picks up disease 95% of the time ⇒ \\(P(+\\mid D)=0.95\\).\nReverse: I want the chance I really have the disease when the test is positive ⇒ \\(P(D\\mid +)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#looking-ahead",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#looking-ahead",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture:\n\nRandom Variables and Probability Distributions\nDiscrete vs. continuous random variables\nExpected value and variance"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#final-thoughts",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#final-thoughts",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nProbability is the foundation of statistics:\n\nHelps us quantify uncertainty\nProvides tools for making decisions with incomplete information\nEssential for understanding statistical inference\n\n\n\nPractice: The key to mastering probability is working through many problems!"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#questions",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#questions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Random Variables and Distributions"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#resources",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#resources",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms"
  },
  {
    "objectID": "files/lecture_notes/lecture4/lecture4-5-6.html#footnotes",
    "href": "files/lecture_notes/lecture4/lecture4-5-6.html#footnotes",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)↩︎\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent.↩︎"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html",
    "href": "files/lecture_notes/lecture3/lecture3.html",
    "title": "Descriptive Statistics Part II",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nCalculate and interpret measures of variability (range, variance, standard deviation)\nUnderstand and compute measures of position (percentiles, quartiles, z-scores)\nAssess distribution shape using skewness and kurtosis\nCreate and interpret histograms with appropriate bin widths\nConstruct and analyze boxplots for data exploration\nIdentify trends, patterns, and outliers in data visualizations\nApply Python for comprehensive descriptive analysis and visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#learning-objectives",
    "href": "files/lecture_notes/lecture3/lecture3.html#learning-objectives",
    "title": "Descriptive Statistics Part II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this lecture, you will be able to:\n\nCalculate and interpret measures of variability (range, variance, standard deviation)\nUnderstand and compute measures of position (percentiles, quartiles, z-scores)\nAssess distribution shape using skewness and kurtosis\nCreate and interpret histograms with appropriate bin widths\nConstruct and analyze boxplots for data exploration\nIdentify trends, patterns, and outliers in data visualizations\nApply Python for comprehensive descriptive analysis and visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#lecture-outline",
    "href": "files/lecture_notes/lecture3/lecture3.html#lecture-outline",
    "title": "Descriptive Statistics Part II",
    "section": "Lecture Outline",
    "text": "Lecture Outline\n\n\nPart I: Measures of Variability (25 min)\n\nRange, Variance, Standard Deviation\nCoefficient of Variation\nPython Implementation\n\nPart II: Measures of Position (20 min)\n\nPercentiles and Quartiles\nZ-scores and Standardization\n\n\n\nPart III: Distribution Shape (10 min)\n\nSkewness and Kurtosis\n\nPart IV: Data Visualization (20 min)\n\nHistograms and Bin Width Selection\nBoxplots and Interpretation\n\nPart V: Identifying Patterns (5 min)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#what-is-variability",
    "href": "files/lecture_notes/lecture3/lecture3.html#what-is-variability",
    "title": "Descriptive Statistics Part II",
    "section": "What is Variability?",
    "text": "What is Variability?\n\n🎯 Definition: Variability (or dispersion) measures how spread out or scattered the data points are around the center.\n\nWhy Variability Matters\n\nTwo datasets can have the same mean but very different spreads\nVariability indicates consistency and predictability\nEssential for risk assessment and quality control\nHelps determine confidence in our central tendency measures"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#range",
    "href": "files/lecture_notes/lecture3/lecture3.html#range",
    "title": "Descriptive Statistics Part II",
    "section": "Range",
    "text": "Range\n\nRange = Maximum value - Minimum value\n\nExample\nData: 12, 15, 18, 22, 25, 30, 35\n\nRange = 35 - 12 = 23"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#when-to-use-range",
    "href": "files/lecture_notes/lecture3/lecture3.html#when-to-use-range",
    "title": "Descriptive Statistics Part II",
    "section": "When to Use Range",
    "text": "When to Use Range\n✅ Use range when:\n\nNeed a quick, simple measure of spread\nWorking with small datasets\nCommunicating to non-technical audiences\n\n❌ Avoid range when:\n\nOutliers are present\nNeed detailed information about variability\nWorking with large datasets"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#variance-definition",
    "href": "files/lecture_notes/lecture3/lecture3.html#variance-definition",
    "title": "Descriptive Statistics Part II",
    "section": "Variance Definition",
    "text": "Variance Definition\n\n🎯 Definition: Variance measures the average squared deviation from the mean."
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#recall",
    "href": "files/lecture_notes/lecture3/lecture3.html#recall",
    "title": "Descriptive Statistics Part II",
    "section": "Recall",
    "text": "Recall"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#side-by-side-comparison",
    "href": "files/lecture_notes/lecture3/lecture3.html#side-by-side-comparison",
    "title": "Descriptive Statistics Part II",
    "section": "Side-by-Side Comparison",
    "text": "Side-by-Side Comparison\n\n\n\nPopulation Variance\n\n\n\\[\\sigma^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}\\]\n\n\\(\\sigma^2\\) = population variance\n\\(x_i\\) = value of \\(i^{th}\\) element\n\\(\\mu\\) = population mean\n\\(N\\) = population size\n\n\n\nSample Variance\n\n\n\\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\]\n\n\\(s^2\\) = sample variance\n\\(x_i\\) = value of \\(i^{th}\\) element\n\\(\\bar{x}\\) = sample mean\n\\(n\\) = sample size"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#key-differences",
    "href": "files/lecture_notes/lecture3/lecture3.html#key-differences",
    "title": "Descriptive Statistics Part II",
    "section": "Key Differences",
    "text": "Key Differences\n\nKey Difference: Sample variance uses \\((n-1)\\) instead of \\(N\\) in the denominator\n\nWhy \\((n-1)\\)?\n\nWhen we use sample mean \\(\\bar{x}\\) to estimate population mean \\(\\mu\\)\nWe lose one degree of freedom\nCalled Bessel’s correction\nMakes sample variance an unbiased estimator"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#when-to-use-each-formula",
    "href": "files/lecture_notes/lecture3/lecture3.html#when-to-use-each-formula",
    "title": "Descriptive Statistics Part II",
    "section": "When to Use Each Formula",
    "text": "When to Use Each Formula\n\nPopulation Variance (\\(\\sigma^2\\))\n\nYou have data for the entire population\nYou know the true population mean \\(\\mu\\)\nExample: Test scores for all students in a small class\n\n\n\nSample Variance (\\(s^2\\))\n\nYou have data from a sample only\nWant to estimate population variance\nExample: Survey responses from 100 people out of 10,000"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#understanding-degrees-of-freedom",
    "href": "files/lecture_notes/lecture3/lecture3.html#understanding-degrees-of-freedom",
    "title": "Descriptive Statistics Part II",
    "section": "Understanding Degrees of Freedom",
    "text": "Understanding Degrees of Freedom\n\n\n📊 Population Case\nAll observations are independent\n\nWe know the true population mean \\(\\mu\\)\nEach of the \\(N\\) observations provides independent information\nNo constraints on the data\n\n\n\\[\\text{Degrees of Freedom} = N\\]\n\n\n\n📈 Sample Case\nConstraint introduced by sample mean\n\nWe must estimate \\(\\mu\\) using \\(\\bar{x}\\)\nOnce we know \\(\\bar{x}\\) and \\((n-1)\\) observations, the last one is determined\nWe “lose” one degree of freedom\n\n\n\\[\\text{Degrees of Freedom} = n-1\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#section",
    "href": "files/lecture_notes/lecture3/lecture3.html#section",
    "title": "Descriptive Statistics Part II",
    "section": "",
    "text": "Sample Mean Constraint\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\]\nRearranging: \\[x_n = n\\bar{x} - (x_1 + x_2 + \\cdots + x_{n-1})\\]\nOnce we know \\(\\bar{x}\\) and the first \\((n-1)\\) values, \\(x_n\\) is completely determined!"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#example-calculation",
    "href": "files/lecture_notes/lecture3/lecture3.html#example-calculation",
    "title": "Descriptive Statistics Part II",
    "section": "Example Calculation",
    "text": "Example Calculation\nData: 3, 7, 2, 8, 5\nIf this is the entire population:\n\n\\(\\mu = \\frac{3+7+2+8+5}{5} = 5\\)\n\\(\\sigma^2 = \\frac{(3-5)^2+(7-5)^2+(2-5)^2+(8-5)^2+(5-5)^2}{5} = \\frac{22}{5} = 4.4\\)\n\nIf this is a sample:\n\n\\(\\bar{x} = 5\\) (same calculation)\n\\(s^2 = \\frac{22}{5-1} = \\frac{22}{4} = 5.5\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#variance-properties",
    "href": "files/lecture_notes/lecture3/lecture3.html#variance-properties",
    "title": "Descriptive Statistics Part II",
    "section": "Variance Properties",
    "text": "Variance Properties\nVariance measures:\n\nAverage squared deviation from the mean\nAlways non-negative: \\(\\sigma^2 \\geq 0\\), \\(s^2 \\geq 0\\)\nUnits: (original units)²\n\nStandard Deviation:\n\n\\(\\sigma = \\sqrt{\\sigma^2}\\) (population)\n\\(s = \\sqrt{s^2}\\) (sample)\nSame units as original data"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#bias-and-unbiasedness",
    "href": "files/lecture_notes/lecture3/lecture3.html#bias-and-unbiasedness",
    "title": "Descriptive Statistics Part II",
    "section": "Bias and Unbiasedness",
    "text": "Bias and Unbiasedness\nPopulation variance:\n\nTrue parameter value\nNo estimation involved\n\nSample variance with \\((n-1)\\):\n\n\\(E[s^2] = \\sigma^2\\) (unbiased)\nOn average, equals population variance\n\nSample variance with \\(n\\):\n\n\\(E\\left[\\frac{\\sum(x_i - \\bar{x})^2}{n}\\right] = \\frac{n-1}{n}\\sigma^2\\) (biased)\nSystematically underestimates population variance"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#implementation",
    "href": "files/lecture_notes/lecture3/lecture3.html#implementation",
    "title": "Descriptive Statistics Part II",
    "section": "Implementation",
    "text": "Implementation\nCalculators:\n\nMost use \\((n-1)\\) by default for sample standard deviation\nCheck your calculator’s documentation\n\nSoftware:\n\nR: var() uses \\((n-1)\\), sd() uses \\((n-1)\\)\nExcel: VAR.S() uses \\((n-1)\\), VAR.P() uses \\(n\\)\nPython: np.var(ddof=1) uses \\((n-1)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#practice-problem",
    "href": "files/lecture_notes/lecture3/lecture3.html#practice-problem",
    "title": "Descriptive Statistics Part II",
    "section": "Practice Problem",
    "text": "Practice Problem\nDataset: Number of hours studied by 6 students: 2, 4, 3, 5, 6, 4\nCalculate both:\n\nPopulation variance (assuming this is the entire population)\nSample variance (assuming this is a sample)\n\n\nSolution:\n\nMean: \\(\\bar{x} = \\frac{24}{6} = 4\\)\nPopulation variance: \\(\\sigma^2 = \\frac{10}{6} = 1.67\\)\nSample variance: \\(s^2 = \\frac{10}{5} = 2.0\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#summary",
    "href": "files/lecture_notes/lecture3/lecture3.html#summary",
    "title": "Descriptive Statistics Part II",
    "section": "Summary",
    "text": "Summary\n\nKey Takeaways\n\nPopulation variance uses \\(N\\) (entire population)\nSample variance uses \\((n-1)\\) (Bessel’s correction)\nSample variance is unbiased estimator of population variance\nDifference matters more for small samples\nAlways check which formula your software uses!"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#example-1",
    "href": "files/lecture_notes/lecture3/lecture3.html#example-1",
    "title": "Descriptive Statistics Part II",
    "section": "Example",
    "text": "Example\n\n\n📊 Complete Population Data (Test Scores)\nWe have test scores from 100 students arranged in a grid:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nT\n\n\n\n\n1\n24\n96\n30\n69\n85\n60\n55\n18\n30\n66\n64\n99\n92\n95\n84\n55\n72\n38\n86\n32\n\n\n2\n53\n81\n30\n89\n42\n94\n31\n26\n53\n78\n38\n60\n93\n90\n82\n85\n89\n54\n30\n58\n\n\n3\n62\n67\n75\n47\n99\n25\n32\n63\n49\n45\n30\n97\n57\n32\n37\n62\n33\n16\n11\n41\n\n\n4\n95\n74\n28\n73\n82\n97\n65\n88\n56\n95\n85\n44\n70\n65\n34\n85\n58\n15\n64\n84\n\n\n5\n76\n46\n83\n56\n98\n16\n76\n77\n35\n19\n97\n42\n90\n79\n73\n28\n82\n92\n90\n22\n\n\n\n\n\n\n🎯 Random Sample Selection\nWe randomly select 5 scores from different positions in our population:\nOur Sample: 82, 95, 83, 60, 92"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#final-calculations",
    "href": "files/lecture_notes/lecture3/lecture3.html#final-calculations",
    "title": "Descriptive Statistics Part II",
    "section": "Final Calculations",
    "text": "Final Calculations\n\n\n\nSample Variance\n\n\n\\[s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\] \\[s^2 = \\frac{753.2}{5-1} = \\frac{753.2}{4} = 188.3\\]\n\n\n\n\nSample Standard Deviation\n\n\n\\[s = \\sqrt{s^2}\\] \\[s = \\sqrt{188.3} = 13.72\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#properties-of-standard-deviation",
    "href": "files/lecture_notes/lecture3/lecture3.html#properties-of-standard-deviation",
    "title": "Descriptive Statistics Part II",
    "section": "Properties of Standard Deviation",
    "text": "Properties of Standard Deviation\n\nSame units as the original data\nAlways non-negative\nZero only when all values are identical\nLarger values indicate more variability\nApproximately 68% of data within 1 SD of mean (for normal distributions)\nApproximately 95% of data within 2 SD of mean"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#empirical-rule-68-95-99.7-rule",
    "href": "files/lecture_notes/lecture3/lecture3.html#empirical-rule-68-95-99.7-rule",
    "title": "Descriptive Statistics Part II",
    "section": "Empirical Rule (68-95-99.7 Rule)",
    "text": "Empirical Rule (68-95-99.7 Rule)\nFor approximately normal distributions:\n\n68% of data falls within 1 standard deviation of the mean\n95% of data falls within 2 standard deviations of the mean\n99.7% of data falls within 3 standard deviations of the mean\n\nThis rule helps us understand what constitutes “typical” vs “unusual” values."
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#definition-and-purpose",
    "href": "files/lecture_notes/lecture3/lecture3.html#definition-and-purpose",
    "title": "Descriptive Statistics Part II",
    "section": "Definition and Purpose",
    "text": "Definition and Purpose\n\n🎯 Definition: Coefficient of Variation (CV) = \\(\\frac{\\text{Standard Deviation}}{\\text{Mean}} \\times 100\\%\\)\n\n\nWhy Use CV?\n\nCompares variability across different units or scales\nRelative measure of variability\nUseful when means differ substantially"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#python-implementation---variability",
    "href": "files/lecture_notes/lecture3/lecture3.html#python-implementation---variability",
    "title": "Descriptive Statistics Part II",
    "section": "Python Implementation - Variability",
    "text": "Python Implementation - Variability\n\nCodeVisualization\n\n\n\nimport numpy as np\nimport pandas as pd\n\n# Sample data\ndata = [10, 12, 14, 16, 18, 22, 25]\n\n# Calculate measures of variability\nrange_val = np.max(data) - np.min(data)\nvariance_sample = np.var(data, ddof=1)  # Sample variance\nstd_sample = np.std(data, ddof=1)       # Sample standard deviation\ncv = (std_sample / np.mean(data)) * 100\n\nprint(f\"Range: {range_val}\")\nprint(f\"Variance: {variance_sample:.2f}\")\nprint(f\"Standard Deviation: {std_sample:.2f}\")\nprint(f\"Coefficient of Variation: {cv:.1f}%\")\n\nRange: 15\nVariance: 28.90\nStandard Deviation: 5.38\nCoefficient of Variation: 32.2%"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#what-are-measures-of-position",
    "href": "files/lecture_notes/lecture3/lecture3.html#what-are-measures-of-position",
    "title": "Descriptive Statistics Part II",
    "section": "What are Measures of Position?",
    "text": "What are Measures of Position?\n\n\nMeasures of position tell us where a particular value stands relative to the rest of the data.\nThey answer questions like:\n\n“What percentage of students scored below 85?”\n“Is this value typical or unusual?”\n“How does this observation compare to others?”"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#percentiles-definition",
    "href": "files/lecture_notes/lecture3/lecture3.html#percentiles-definition",
    "title": "Descriptive Statistics Part II",
    "section": "Percentiles Definition",
    "text": "Percentiles Definition\nThe k-th percentile is the value below which k% of the data falls.\nExamples:\n\n50th percentile = Median (50% of data below this value)\n90th percentile = 90% of data falls below this value\n25th percentile = 25% of data falls below this value"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#quartiles",
    "href": "files/lecture_notes/lecture3/lecture3.html#quartiles",
    "title": "Descriptive Statistics Part II",
    "section": "Quartiles",
    "text": "Quartiles\nQuartiles divide the data into four equal parts:\n\nQ1 (First Quartile) = 25th percentile\nQ2 (Second Quartile) = 50th percentile = Median\nQ3 (Third Quartile) = 75th percentile"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#interquartile-range-iqr",
    "href": "files/lecture_notes/lecture3/lecture3.html#interquartile-range-iqr",
    "title": "Descriptive Statistics Part II",
    "section": "Interquartile Range (IQR)",
    "text": "Interquartile Range (IQR)\nIQR = Q3 - Q1\n\n\n\n\nProperties of IQR:\n\nContains the middle 50% of the data\nResistant to outliers\nUsed in boxplot construction\nUseful for outlier detection"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#z-score-definition",
    "href": "files/lecture_notes/lecture3/lecture3.html#z-score-definition",
    "title": "Descriptive Statistics Part II",
    "section": "Z-score Definition",
    "text": "Z-score Definition\n\n\n\n🎯 Definition\nZ-score tells us how many standard deviations a value is from the mean.\n\n\n\\[z = \\frac{x - \\mu}{\\sigma} \\text{ or } z = \\frac{x - \\bar{x}}{s}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#z-score-example",
    "href": "files/lecture_notes/lecture3/lecture3.html#z-score-example",
    "title": "Descriptive Statistics Part II",
    "section": "Z-score Example",
    "text": "Z-score Example\nStudent’s test score: 85 Class mean: 78, Class standard deviation: 6\n\n\\[z = \\frac{85 - 78}{6} = \\frac{7}{6} = 1.17\\]\n\nInterpretation: This student scored 1.17 standard deviations above the class average."
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#benefits-of-standardization",
    "href": "files/lecture_notes/lecture3/lecture3.html#benefits-of-standardization",
    "title": "Descriptive Statistics Part II",
    "section": "Benefits of Standardization",
    "text": "Benefits of Standardization\n\nCompare across different scales (test scores vs income)\nIdentify outliers systematically\n\nCombine different variables meaningfully\nPrepare data for certain statistical methods"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#skewness",
    "href": "files/lecture_notes/lecture3/lecture3.html#skewness",
    "title": "Descriptive Statistics Part II",
    "section": "Skewness",
    "text": "Skewness\nSkewness measures the asymmetry of a distribution.\nTypes of Skewness:\n\n\n\n\nSymmetric (Skewness ≈ 0): Mean ≈ Median ≈ Mode\nRight-skewed (Positive skewness): Mean &gt; Median, long tail to the right\nLeft-skewed (Negative skewness): Mean &lt; Median, long tail to the left"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#examples-of-skewness",
    "href": "files/lecture_notes/lecture3/lecture3.html#examples-of-skewness",
    "title": "Descriptive Statistics Part II",
    "section": "Examples of Skewness",
    "text": "Examples of Skewness\nRight-skewed (Income data):\n\nMost people earn moderate amounts\nFew people earn very high amounts\nMean &gt; Median\n\nLeft-skewed (Test scores with ceiling effect):\n\nMost students score high\nFew students score very low\nMean &lt; Median"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#kurtosis",
    "href": "files/lecture_notes/lecture3/lecture3.html#kurtosis",
    "title": "Descriptive Statistics Part II",
    "section": "Kurtosis",
    "text": "Kurtosis\nKurtosis measures the “tailedness” of a distribution. It measures the degree of peaked Ness or flatness of a distribution compared to the normal distribution.\nTypes:\n\nMesokurtic (Normal-like): Kurtosis ≈ 3\nLeptokurtic (Heavy tails): Kurtosis &gt; 3, more peaked\nPlatykurtic (Light tails): Kurtosis &lt; 3, flatter\n\nExcess Kurtosis = Kurtosis - 3 (makes normal distributions have excess kurtosis of 0)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#python-implementation---position-shape",
    "href": "files/lecture_notes/lecture3/lecture3.html#python-implementation---position-shape",
    "title": "Descriptive Statistics Part II",
    "section": "Python Implementation - Position & Shape",
    "text": "Python Implementation - Position & Shape\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\ndata = [12, 15, 18, 22, 25, 28, 30, 35, 40, 45]\n\n# Percentiles and quartiles\nq1 = np.percentile(data, 25)\nmedian = np.percentile(data, 50)\nq3 = np.percentile(data, 75)\niqr = q3 - q1\n\n# Z-scores\nz_scores = stats.zscore(data)\n\n# Shape measures\nskewness = stats.skew(data)\nkurt = stats.kurtosis(data)\n\nprint(f\"Q1: {q1}, Median: {median}, Q3: {q3}\")\nprint(f\"IQR: {iqr}\")\nprint(f\"Skewness: {skewness:.3f}\")\nprint(f\"Kurtosis: {kurt:.3f}\")\n\nQ1: 19.0, Median: 26.5, Q3: 33.75\nIQR: 14.75\nSkewness: 0.243\nKurtosis: -1.023"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#what-is-a-histogram",
    "href": "files/lecture_notes/lecture3/lecture3.html#what-is-a-histogram",
    "title": "Descriptive Statistics Part II",
    "section": "What is a Histogram?",
    "text": "What is a Histogram?\nA histogram displays the distribution of a continuous variable by dividing data into bins and showing the frequency of observations in each bin.\nKey Components:\n\nX-axis: Variable values (continuous)\nY-axis: Frequency or density\nBins: Intervals that group the data\nBars: Height represents frequency in each bin"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#choosing-bin-width-critical-decision",
    "href": "files/lecture_notes/lecture3/lecture3.html#choosing-bin-width-critical-decision",
    "title": "Descriptive Statistics Part II",
    "section": "Choosing Bin Width: Critical Decision",
    "text": "Choosing Bin Width: Critical Decision\nBin width dramatically affects histogram interpretation!\nToo Few Bins (Wide bins):\n\nOversmoothing - lose important details\nMay hide multimodality\nDistribution appears simpler than it is\n\nToo Many Bins (Narrow bins):\n\nUndersmoothing - too much noise\nMay create artificial gaps\nHard to see overall pattern"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#bin-width-guidelines",
    "href": "files/lecture_notes/lecture3/lecture3.html#bin-width-guidelines",
    "title": "Descriptive Statistics Part II",
    "section": "Bin Width Guidelines",
    "text": "Bin Width Guidelines\nRule of Thumb Methods:\n\nSquare Root Rule: Number of bins ≈ \\(\\sqrt{n}\\)\nSturges’ Rule: Number of bins = \\(1 + \\log_2(n)\\)\nScott’s Rule: Bin width = \\(\\frac{3.5 \\times \\text{SD}}{n^{1/3}}\\)\nFreedman-Diaconis Rule: Bin width = \\(\\frac{2 \\times \\text{IQR}}{n^{1/3}}\\)\n\nBest practice: Try multiple bin widths and choose based on the story your data tells!"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#python-histogram-examples",
    "href": "files/lecture_notes/lecture3/lecture3.html#python-histogram-examples",
    "title": "Descriptive Statistics Part II",
    "section": "Python Histogram Examples",
    "text": "Python Histogram Examples\n\nCodeVisualization\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\nnp.random.seed(42)\ndata = np.random.normal(100, 15, 1000)"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#interpreting-histograms",
    "href": "files/lecture_notes/lecture3/lecture3.html#interpreting-histograms",
    "title": "Descriptive Statistics Part II",
    "section": "Interpreting Histograms",
    "text": "Interpreting Histograms\nWhat to Look For:\n\nShape: Normal, skewed, uniform, bimodal?\nCenter: Where is the “typical” value?\nSpread: How variable is the data?\nOutliers: Any unusual values?\nGaps: Are there missing values in certain ranges?\nMultiple peaks: Suggests multiple subgroups"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#anatomy-of-a-boxplot",
    "href": "files/lecture_notes/lecture3/lecture3.html#anatomy-of-a-boxplot",
    "title": "Descriptive Statistics Part II",
    "section": "Anatomy of a Boxplot",
    "text": "Anatomy of a Boxplot"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#boxplot-components-explained",
    "href": "files/lecture_notes/lecture3/lecture3.html#boxplot-components-explained",
    "title": "Descriptive Statistics Part II",
    "section": "Boxplot Components Explained",
    "text": "Boxplot Components Explained\nThe Box:\n\nLeft edge: Q1 (25th percentile)\nMiddle line: Median (Q2, 50th percentile)\n\nRight edge: Q3 (75th percentile)\nBox width: IQR (contains middle 50% of data)\n\nThe Whiskers:\n\nExtend to: Most extreme values within 1.5 × IQR from box edges\nLower whisker: Minimum value within Q1 - 1.5×IQR\nUpper whisker: Maximum value within Q3 + 1.5×IQR"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#what-boxplots-tell-us",
    "href": "files/lecture_notes/lecture3/lecture3.html#what-boxplots-tell-us",
    "title": "Descriptive Statistics Part II",
    "section": "What Boxplots Tell Us",
    "text": "What Boxplots Tell Us\nDistribution Shape:\n\nSymmetric: Median in center of box, whiskers equal length\nRight-skewed: Median closer to Q1, longer upper whisker\nLeft-skewed: Median closer to Q3, longer lower whisker\n\nVariability:\n\nWide box: High variability in middle 50%\nLong whiskers: High overall variability\nMany outliers: Extreme variability"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#comparing-groups-with-boxplots",
    "href": "files/lecture_notes/lecture3/lecture3.html#comparing-groups-with-boxplots",
    "title": "Descriptive Statistics Part II",
    "section": "Comparing Groups with Boxplots",
    "text": "Comparing Groups with Boxplots"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#advanced-boxplot-interpretations",
    "href": "files/lecture_notes/lecture3/lecture3.html#advanced-boxplot-interpretations",
    "title": "Descriptive Statistics Part II",
    "section": "Advanced Boxplot Interpretations",
    "text": "Advanced Boxplot Interpretations"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#comparing-boxplots",
    "href": "files/lecture_notes/lecture3/lecture3.html#comparing-boxplots",
    "title": "Descriptive Statistics Part II",
    "section": "Comparing Boxplots:",
    "text": "Comparing Boxplots:\n\nMedian differences: Which group has higher typical values?\nIQR differences: Which group is more consistent?\nOutlier patterns: Which group has more extreme values?\nOverlap: Do the groups have similar ranges?"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#business-applications",
    "href": "files/lecture_notes/lecture3/lecture3.html#business-applications",
    "title": "Descriptive Statistics Part II",
    "section": "Business Applications:",
    "text": "Business Applications:\n\nQuality control: Compare product batches\nPerformance analysis: Compare team/department performance\n\nCustomer segmentation: Compare customer groups"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#common-patterns-in-data",
    "href": "files/lecture_notes/lecture3/lecture3.html#common-patterns-in-data",
    "title": "Descriptive Statistics Part II",
    "section": "Common Patterns in Data",
    "text": "Common Patterns in Data\nDistribution Patterns:\n\nNormal/Bell-shaped: Symmetric, single peak\nUniform: All values equally likely\nBimodal: Two distinct peaks (suggests subgroups)\nMultimodal: Multiple peaks\nU-shaped: High values at extremes, low in middle\n\nOutlier Patterns:\n\nIndividual outliers: Data entry errors, measurement errors\nClustered outliers: Distinct subpopulation\nSystematic outliers: May indicate process changes"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#red-flags-in-data-visualization",
    "href": "files/lecture_notes/lecture3/lecture3.html#red-flags-in-data-visualization",
    "title": "Descriptive Statistics Part II",
    "section": "Red Flags in Data Visualization",
    "text": "Red Flags in Data Visualization\nWarning Signs:\n\nGaps in histograms: Missing data or measurement limitations\nHeaping: Values cluster at round numbers (10, 50, 100)\nTruncation: Data cut off at certain values\nDigit preference: People prefer certain ending digits\nMultiple modes: Hidden subgroups in your data"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#essential-concepts-to-remember",
    "href": "files/lecture_notes/lecture3/lecture3.html#essential-concepts-to-remember",
    "title": "Descriptive Statistics Part II",
    "section": "Essential Concepts to Remember",
    "text": "Essential Concepts to Remember\nVariability:\n\nStandard deviation is preferred over range for most analyses\nCV allows comparison across different scales\nIQR is resistant to outliers\n\nPosition:\n\nPercentiles and quartiles provide relative position\nZ-scores standardize across different distributions\nFive-number summary gives complete overview"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#practical-guidelines",
    "href": "files/lecture_notes/lecture3/lecture3.html#practical-guidelines",
    "title": "Descriptive Statistics Part II",
    "section": "Practical Guidelines",
    "text": "Practical Guidelines\n\nAlways visualize before calculating statistics\nUse multiple measures - no single statistic tells the whole story\nConsider the context - what makes sense for your data?\nCheck for outliers - they can drastically affect your analysis\nCompare distributions using standardized measures when appropriate"
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#try-these-exercises",
    "href": "files/lecture_notes/lecture3/lecture3.html#try-these-exercises",
    "title": "Descriptive Statistics Part II",
    "section": "Try These Exercises",
    "text": "Try These Exercises\n\nCalculate the five-number summary for: 12, 15, 18, 22, 25, 28, 30, 35, 40, 45\nCreate histograms with 5, 15, and 50 bins for the same dataset. What patterns do you observe?\nInterpret this scenario: Dataset A has mean=50, SD=5. Dataset B has mean=100, SD=10. Which is more variable?\nA student scores 85 on a test where the class mean is 78 and SD is 6. Calculate and interpret the z-score.\nDesign a boxplot comparison for three different customer segments in your business."
  },
  {
    "objectID": "files/lecture_notes/lecture3/lecture3.html#additional-resources",
    "href": "files/lecture_notes/lecture3/lecture3.html#additional-resources",
    "title": "Descriptive Statistics Part II",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMatplotlib Gallery: Histogram and Boxplot Examples\nExplore examples of histograms, boxplots, and other visualizations using Matplotlib.\nSeaborn Documentation: Statistical Visualizations\nFind examples and documentation for statistical visualizations, including distribution plots, categorical plots, and regression plots.\nNumPy Statistical Functions Reference\nOfficial reference for NumPy’s statistical functions such as mean, median, variance, and standard deviation.\nSciPy Statistical Functions Reference\nComprehensive documentation for statistical functions in scipy.stats, including probability distributions, hypothesis tests, and descriptive statistics.\nRecommended reading: Continue reading Chapter 2 in course textbook"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html",
    "href": "files/lecture_notes/lecture8/lecture8.html",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "",
    "text": "Continuous Random Variables\nFrom discrete jumps to smooth curves: modeling the continuous world"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#welcome-to-lecture-6",
    "href": "files/lecture_notes/lecture8/lecture8.html#welcome-to-lecture-6",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Welcome to Lecture 6",
    "text": "Welcome to Lecture 6\nConditional Probabilities\nUnderstanding probability when information changes the game"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture8/lecture8.html#todays-learning-objectives",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDistinguish between discrete and continuous random variables (Section 3)\nUnderstand probability density functions (PDFs) and their interpretation (Section 4)\nWork with cumulative distribution functions (CDFs) for continuous variables\nCalculate probabilities using areas under curves\nCompute expected values and variances for continuous distributions\nWork with common continuous distributions (Uniform, Normal, Exponential)\nApply the Central Limit Theorem\nUse python to work with continuous distributions"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#motivation-why-conditional-probability",
    "href": "files/lecture_notes/lecture8/lecture8.html#motivation-why-conditional-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Motivation: Why Conditional Probability?",
    "text": "Motivation: Why Conditional Probability?\nIn real life, we rarely make decisions with no information\nExamples: - Medical diagnosis with test results - Weather forecast with current conditions\n- Investment decisions with market data - Sports betting with team statistics - Insurance premiums based on risk factors\n\nConditional probability helps us update our beliefs when we gain new information"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#what-is-conditional-probability",
    "href": "files/lecture_notes/lecture8/lecture8.html#what-is-conditional-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "What is Conditional Probability?",
    "text": "What is Conditional Probability?\nConditional Probability is the probability of an event occurring, given that another event has already occurred\nNotation: \\(P(A|B)\\) read as “probability of A given B”\n\nKey insight: When we know B has occurred, our sample space effectively “shrinks” to only outcomes where B is true"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#intuitive-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#intuitive-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Intuitive Example",
    "text": "Intuitive Example\nYou roll a fair six-sided die, but before revealing the result, someone tells you “the number is even”\nWhat’s the probability it’s a 4?\n\nWithout information: \\(P(\\text{rolling 4}) = \\frac{1}{6}\\)\nWith information: \\(P(\\text{4 | even}) = ?\\)\nGiven it’s even, possible outcomes: \\(\\{2, 4, 6\\}\\) So \\(P(\\text{4 | even}) = \\frac{1}{3}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#formal-definition",
    "href": "files/lecture_notes/lecture8/lecture8.html#formal-definition",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Formal Definition",
    "text": "Formal Definition\nFor events A and B where \\(P(B) &gt; 0\\):\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nInterpretation:\n\nNumerator: Outcomes where both A and B occur\nDenominator: All outcomes where B occurs\nRatio: Fraction of B-outcomes where A also occurs"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#understanding-the-formula",
    "href": "files/lecture_notes/lecture8/lecture8.html#understanding-the-formula",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Understanding the Formula",
    "text": "Understanding the Formula\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWhy this formula makes sense:\n\nWe restrict our attention to outcomes where B occurs\nAmong those outcomes, what fraction also have A?\nThis is exactly \\(\\frac{P(A \\cap B)}{P(B)}\\)\n\n\nRearranging: \\(P(A \\cap B) = P(A|B) \\times P(B)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-1",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-1",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\nA card is drawn from a standard 52-card deck. Find:\n\n\\(P(\\text{King | Face card})\\)\n\\(P(\\text{Heart | Red card})\\)\n\n\\(P(\\text{Ace | Black card})\\)\n\n\nSolutions:\n\n\\(P(\\text{King | Face}) = \\frac{4}{12} = \\frac{1}{3}\\) (4 kings among 12 face cards)\n\\(P(\\text{Heart | Red}) = \\frac{13}{26} = \\frac{1}{2}\\) (13 hearts among 26 red cards)\n\\(P(\\text{Ace | Black}) = \\frac{2}{26} = \\frac{1}{13}\\) (2 black aces among 26 black cards)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#two-way-tables",
    "href": "files/lecture_notes/lecture8/lecture8.html#two-way-tables",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Two-Way Tables",
    "text": "Two-Way Tables\nTwo-way tables are excellent for conditional probability problems\nExample: Survey of 1000 people about coffee preference\n\n\n\n\nCoffee\nNo Coffee\nTotal\n\n\n\n\nMorning\n350\n150\n500\n\n\nEvening\n200\n300\n500\n\n\nTotal\n550\n450\n1000"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#using-two-way-tables",
    "href": "files/lecture_notes/lecture8/lecture8.html#using-two-way-tables",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Using Two-Way Tables",
    "text": "Using Two-Way Tables\nFind: \\(P(\\text{Coffee | Morning person})\\)\nFrom the table:\n\nMorning people: 500\nMorning people who drink coffee: 350\n\n\n\\(P(\\text{Coffee | Morning}) = \\frac{350}{500} = 0.7\\)\nCompare to: \\(P(\\text{Coffee}) = \\frac{550}{1000} = 0.55\\)\nBeing a morning person increases coffee probability!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-2",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-2",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\nUsing the coffee table, find:\n\n\\(P(\\text{Morning | Coffee drinker})\\)\n\\(P(\\text{No Coffee | Evening person})\\)\n\\(P(\\text{Evening | No Coffee})\\)\n\n\nSolutions:\n\n\\(P(\\text{Morning | Coffee}) = \\frac{350}{550} = \\frac{7}{11} \\approx 0.636\\)\n\\(P(\\text{No Coffee | Evening}) = \\frac{300}{500} = 0.6\\)\n\\(P(\\text{Evening | No Coffee}) = \\frac{300}{450} = \\frac{2}{3} \\approx 0.667\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#independence-revisited",
    "href": "files/lecture_notes/lecture8/lecture8.html#independence-revisited",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Independence Revisited",
    "text": "Independence Revisited\nEvents A and B are independent if knowing that B occurred doesn’t change the probability of A\n\\[P(A|B) = P(A)\\]\nEquivalently: \\[P(A \\cap B) = P(A) \\times P(B)\\]\n\nExample: Two coin flips are independent because \\(P(\\text{H}_2 | \\text{H}_1) = P(\\text{H}_2) = 0.5\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#testing-for-independence",
    "href": "files/lecture_notes/lecture8/lecture8.html#testing-for-independence",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Testing for Independence",
    "text": "Testing for Independence\nMethod 1: Check if \\(P(A|B) = P(A)\\)\nMethod 2: Check if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nMethod 3: Check if \\(P(B|A) = P(B)\\)\n\nCoffee Example: Are coffee preference and time preference independent?\n\\(P(\\text{Coffee}) = 0.55\\)\n\\(P(\\text{Coffee | Morning}) = 0.7\\)\nSince \\(0.7 \\neq 0.55\\), they are not independent"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#the-multiplication-rule",
    "href": "files/lecture_notes/lecture8/lecture8.html#the-multiplication-rule",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "The Multiplication Rule",
    "text": "The Multiplication Rule\nGeneral Multiplication Rule: \\[P(A \\cap B) = P(A) \\times P(B|A) = P(B) \\times P(A|B)\\]\nFor Independent Events: \\[P(A \\cap B) = P(A) \\times P(B)\\]\n\nExtension to Multiple Events: \\[P(A_1 \\cap A_2 \\cap A_3) = P(A_1) \\times P(A_2|A_1) \\times P(A_3|A_1 \\cap A_2)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#multiplication-rule-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#multiplication-rule-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Multiplication Rule Example",
    "text": "Multiplication Rule Example\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement. What’s the probability both are red?\n\nLet \\(R_1\\) = first ball is red, \\(R_2\\) = second ball is red\n\\(P(R_1 \\cap R_2) = P(R_1) \\times P(R_2|R_1)\\)\n\\(= \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\nA box contains 4 defective and 6 working items. Three items are selected without replacement. Find the probability that:\n\nAll three work\nThe first two work and the third is defective\nExactly two work\n\n\nSolutions: a) \\(P(\\text{WWW}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{120}{720} = \\frac{1}{6}\\)\n\n\\(P(\\text{WWD}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{120}{720} = \\frac{1}{6}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3-continued",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3-continued",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 3 (continued)",
    "text": "Practice Problem 3 (continued)\n\nExactly two work (three scenarios: WWD, WDW, DWW)\n\n\\(P(\\text{WWD}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{1}{6}\\)\n\\(P(\\text{WDW}) = \\frac{6}{10} \\times \\frac{4}{9} \\times \\frac{5}{8} = \\frac{1}{6}\\)\n\\(P(\\text{DWW}) = \\frac{4}{10} \\times \\frac{6}{9} \\times \\frac{5}{8} = \\frac{1}{6}\\)\n\nTotal: \\(\\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#tree-diagrams",
    "href": "files/lecture_notes/lecture8/lecture8.html#tree-diagrams",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\nTree diagrams help visualize sequential events and conditional probabilities\n                    0.5   Red\n            0.6 ──┐\n                    0.5   Blue\nBall 1      \n                    0.4   Red  \n            0.4 ──┐\n                    0.6   Blue\nEach branch shows conditional probabilities"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#tree-diagram-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#tree-diagram-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagram Example",
    "text": "Tree Diagram Example\nMedical test scenario: - 2% of population has disease - Test is 95% accurate for sick people\n- Test is 90% accurate for healthy people\nWhat’s the probability of testing positive?"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#tree-diagram-solution",
    "href": "files/lecture_notes/lecture8/lecture8.html#tree-diagram-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagram Solution",
    "text": "Tree Diagram Solution\n                    0.95   Test +\n            0.02 ──┐\n                    0.05   Test -\nDisease?    \n                    0.10   Test +\n            0.98 ──┐\n                    0.90   Test -\n\n\\(P(\\text{Test+}) = P(\\text{Test+|Disease}) \\times P(\\text{Disease}) + P(\\text{Test+|Healthy}) \\times P(\\text{Healthy})\\)\n\\(= 0.95 \\times 0.02 + 0.10 \\times 0.98 = 0.019 + 0.098 = 0.117\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#law-of-total-probability",
    "href": "files/lecture_notes/lecture8/lecture8.html#law-of-total-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space (mutually exclusive and exhaustive), then:\n\\[P(A) = \\sum_{i=1}^{n} P(A|B_i) \\times P(B_i)\\]\n\nPartition means:\n\n\\(B_i \\cap B_j = \\emptyset\\) for \\(i \\neq j\\) (mutually exclusive)\n\\(\\bigcup_{i=1}^{n} B_i = S\\) (exhaustive)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#law-of-total-probability-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\nA factory has three machines: - Machine A: 50% of production, 1% defective - Machine B: 30% of production, 2% defective\n- Machine C: 20% of production, 3% defective\nWhat’s the overall defect rate?\n\n\\(P(\\text{Defective}) = P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C)\\)\n\\(= 0.01 \\times 0.5 + 0.02 \\times 0.3 + 0.03 \\times 0.2\\)\n\\(= 0.005 + 0.006 + 0.006 = 0.017 = 1.7\\%\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-4",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-4",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\nA student studies for an exam with three possible outcomes based on study time: - Studies hard (40%): 90% chance of passing - Studies moderately (35%): 70% chance of passing\n- Doesn’t study (25%): 30% chance of passing\nWhat’s the overall probability of passing?\n\n\\(P(\\text{Pass}) = 0.9 \\times 0.4 + 0.7 \\times 0.35 + 0.3 \\times 0.25\\)\n\\(= 0.36 + 0.245 + 0.075 = 0.68\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem",
    "href": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nThe Foundation: We often want to “reverse” conditional probabilities\nGiven: \\(P(B|A)\\), \\(P(A)\\), \\(P(B)\\) Want: \\(P(A|B)\\)\nBayes’ Theorem: \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem-components",
    "href": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem-components",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\n\n\\(P(A|B)\\): Posterior probability (what we want)\n\\(P(B|A)\\): Likelihood (what we observe)\n\n\\(P(A)\\): Prior probability (initial belief)\n\\(P(B)\\): Evidence (marginal probability)\n\n\n“In light of evidence B, how should we update our belief in A?”"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem-with-total-probability",
    "href": "files/lecture_notes/lecture8/lecture8.html#bayes-theorem-with-total-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem with Total Probability",
    "text": "Bayes’ Theorem with Total Probability\nWhen we need to find \\(P(B)\\):\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B|A) \\times P(A) + P(B|A^c) \\times P(A^c)}\\]\nThis is the most common form for applications"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#medical-diagnosis-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#medical-diagnosis-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Medical Diagnosis Example",
    "text": "Medical Diagnosis Example\nRevisiting our medical test: - 2% of population has disease (prior) - Test positive (evidence)\n- Test is 95% accurate for sick, 90% accurate for healthy\nGiven a positive test, what’s the probability of having the disease?"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#medical-diagnosis-solution",
    "href": "files/lecture_notes/lecture8/lecture8.html#medical-diagnosis-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Medical Diagnosis Solution",
    "text": "Medical Diagnosis Solution\nLet D = disease, T+ = positive test\n\\[P(D|T+) = \\frac{P(T+|D) \\times P(D)}{P(T+|D) \\times P(D) + P(T+|D^c) \\times P(D^c)}\\]\n\\[= \\frac{0.95 \\times 0.02}{0.95 \\times 0.02 + 0.10 \\times 0.98}\\]\n\\[= \\frac{0.019}{0.019 + 0.098} = \\frac{0.019}{0.117} \\approx 0.162\\]\n\nSurprising: Only 16.2% chance of disease despite positive test!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#why-the-low-probability",
    "href": "files/lecture_notes/lecture8/lecture8.html#why-the-low-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Why the Low Probability?",
    "text": "Why the Low Probability?\nBase Rate Fallacy: When disease is rare (2%), most positive tests are false positives\nIntuition: Out of 10,000 people: - 200 have disease → 190 test positive\n- 9,800 healthy → 980 test positive - Total positive tests: 1,170 - True positives: 190\n\\(P(\\text{Disease | Positive}) = \\frac{190}{1,170} \\approx 0.162\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-5",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-5",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 5",
    "text": "Practice Problem 5\nEmail spam filter: - 60% of emails are spam - Filter catches 95% of spam - Filter incorrectly flags 8% of legitimate emails\nIf an email is flagged as spam, what’s the probability it’s actually spam?\n\n\\(P(\\text{Spam | Flagged}) = \\frac{0.95 \\times 0.6}{0.95 \\times 0.6 + 0.08 \\times 0.4}\\)\n\\(= \\frac{0.57}{0.57 + 0.032} = \\frac{0.57}{0.602} \\approx 0.947\\)\nThe filter is quite reliable!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#multiple-events-and-bayes",
    "href": "files/lecture_notes/lecture8/lecture8.html#multiple-events-and-bayes",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Multiple Events and Bayes’",
    "text": "Multiple Events and Bayes’\nExtended Bayes’ Theorem: If \\(A_1, A_2, \\ldots, A_n\\) partition the sample space:\n\\[P(A_i|B) = \\frac{P(B|A_i) \\times P(A_i)}{\\sum_{j=1}^{n} P(B|A_j) \\times P(A_j)}\\]\nThis allows us to update probabilities for multiple hypotheses"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#three-machine-example-revisited",
    "href": "files/lecture_notes/lecture8/lecture8.html#three-machine-example-revisited",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Three Machine Example Revisited",
    "text": "Three Machine Example Revisited\nA defective item is found. Which machine most likely produced it?\nFrom before: - Machine A: 50% production, 1% defective\n- Machine B: 30% production, 2% defective - Machine C: 20% production, 3% defective - Overall defect rate: 1.7%"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#three-machine-solution",
    "href": "files/lecture_notes/lecture8/lecture8.html#three-machine-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Three Machine Solution",
    "text": "Three Machine Solution\n\\[P(A|\\text{Defective}) = \\frac{0.01 \\times 0.5}{0.017} = \\frac{0.005}{0.017} \\approx 0.294\\]\n\\[P(B|\\text{Defective}) = \\frac{0.02 \\times 0.3}{0.017} = \\frac{0.006}{0.017} \\approx 0.353\\]\n\\[P(C|\\text{Defective}) = \\frac{0.03 \\times 0.2}{0.017} = \\frac{0.006}{0.017} \\approx 0.353\\]\n\nMachine B or C are most likely sources of the defective item"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-6",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-6",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\nThree boxes contain colored balls: - Box 1: 3 red, 2 blue (chosen 40% of time) - Box 2: 2 red, 3 blue (chosen 35% of time)\n- Box 3: 1 red, 4 blue (chosen 25% of time)\nA red ball is drawn. Which box was it most likely from?\n\n\\(P(\\text{Red}) = \\frac{3}{5} \\times 0.4 + \\frac{2}{5} \\times 0.35 + \\frac{1}{5} \\times 0.25 = 0.24 + 0.14 + 0.05 = 0.43\\)\n\\(P(\\text{Box 1 | Red}) = \\frac{0.6 \\times 0.4}{0.43} \\approx 0.558\\) \\(P(\\text{Box 2 | Red}) = \\frac{0.4 \\times 0.35}{0.43} \\approx 0.326\\)\n\\(P(\\text{Box 3 | Red}) = \\frac{0.2 \\times 0.25}{0.43} \\approx 0.116\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#conditional-independence",
    "href": "files/lecture_notes/lecture8/lecture8.html#conditional-independence",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Conditional Independence",
    "text": "Conditional Independence\nEvents A and B are conditionally independent given C if:\n\\[P(A \\cap B | C) = P(A|C) \\times P(B|C)\\]\nImportant: Conditional independence doesn’t imply independence!\n\nExample: Weather in two cities may be independent normally, but conditionally dependent given a major weather system"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#simpsons-paradox",
    "href": "files/lecture_notes/lecture8/lecture8.html#simpsons-paradox",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nSimpson’s Paradox: A trend in subgroups can reverse when groups are combined\nClassic Example: University admissions by gender\n\n\n\n\nMen\nWomen\n\n\n\n\nDept A\n62% (825/1327)\n82% (108/131)\n\n\nDept B\n63% (560/893)\n68% (25/37)\n\n\nOverall\n44% (1385/2220)\n30% (133/168)\n\n\n\nWomen have higher rates in each department but lower overall!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#common-fallacies",
    "href": "files/lecture_notes/lecture8/lecture8.html#common-fallacies",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Common Fallacies",
    "text": "Common Fallacies\n1. Confusion of the Inverse - Confusing \\(P(A|B)\\) with \\(P(B|A)\\) - “If it rains, the ground is wet” ≠ “If the ground is wet, it rained”\n2. Base Rate Neglect\n- Ignoring prior probabilities - Medical test example\n3. Prosecutor’s Fallacy - \\(P(\\text{Evidence | Innocent}) \\neq P(\\text{Innocent | Evidence})\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#prosecutors-fallacy-example",
    "href": "files/lecture_notes/lecture8/lecture8.html#prosecutors-fallacy-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Prosecutor’s Fallacy Example",
    "text": "Prosecutor’s Fallacy Example\nDNA evidence matches defendant with probability 1 in a million for random person\nWrong reasoning: “Probability of innocence is 1 in a million”\nCorrect reasoning: Need to consider: - How many people could have committed the crime? - What’s the prior probability of guilt? - Possibility of lab error, planted evidence, etc."
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-7",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-7",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\nQuality control: 5% of items are defective. A test detects 96% of defective items but has a 4% false positive rate.\n\nWhat’s the probability an item testing positive is actually defective?\nWhat’s the probability an item testing negative is actually good?\n\n\n\n\\(P(\\text{Defective | Positive}) = \\frac{0.96 \\times 0.05}{0.96 \\times 0.05 + 0.04 \\times 0.95} = \\frac{0.048}{0.086} \\approx 0.558\\)\n\\(P(\\text{Good | Negative}) = \\frac{0.96 \\times 0.95}{0.96 \\times 0.95 + 0.04 \\times 0.05} = \\frac{0.912}{0.914} \\approx 0.998\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#real-world-applications",
    "href": "files/lecture_notes/lecture8/lecture8.html#real-world-applications",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Screening: - Mammograms, COVID tests - Balancing sensitivity vs specificity\nMachine Learning: - Naive Bayes classifiers - Spam detection, recommendation systems\nFinance: - Credit scoring - Fraud detection\nLegal System: - DNA evidence interpretation - Probability of guilt/innocence"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#technology-and-tools",
    "href": "files/lecture_notes/lecture8/lecture8.html#technology-and-tools",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Technology and Tools",
    "text": "Technology and Tools\nCalculators: - Basic probability calculations - Watch for rounding errors\nSoftware: - R: conditional probability tables - Python: pandas for two-way tables - Excel: pivot tables for conditional analysis\nVisualization: - Tree diagrams\n- Contingency tables - Bayes networks"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#diagnostic-thinking",
    "href": "files/lecture_notes/lecture8/lecture8.html#diagnostic-thinking",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Diagnostic Thinking",
    "text": "Diagnostic Thinking\nQuestions to ask: 1. What information am I conditioning on? 2. How does this information change the probability? 3. What’s the base rate or prior probability? 4. Am I confusing \\(P(A|B)\\) with \\(P(B|A)\\)? 5. Are the events independent?"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture8/lecture8.html#problem-solving-strategy",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\nIdentify the type: Direct conditional, Bayes’, or law of total probability?\nDefine events clearly: Use precise notation\nOrganize information: Two-way tables or tree diagrams\nCheck for independence: Does additional info matter?\nApply appropriate formula: Don’t forget denominators!\nVerify answer: Does it make intuitive sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-8",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-8",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 8",
    "text": "Practice Problem 8\nA survey shows: - 70% of people like pizza - 60% of people like movies\n- 40% like both pizza and movies\n\nAre liking pizza and movies independent?\nWhat’s \\(P(\\text{Pizza | Movies})\\)?\nWhat’s \\(P(\\text{Movies | Pizza})\\)?"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-8-solutions",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-8-solutions",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 8 Solutions",
    "text": "Practice Problem 8 Solutions\n\nCheck independence: \\(P(\\text{Pizza}) \\times P(\\text{Movies}) = 0.7 \\times 0.6 = 0.42 \\neq 0.4\\) Not independent!\n\\(P(\\text{Pizza | Movies}) = \\frac{P(\\text{Pizza} \\cap \\text{Movies})}{P(\\text{Movies})} = \\frac{0.4}{0.6} = \\frac{2}{3}\\)\n\\(P(\\text{Movies | Pizza}) = \\frac{P(\\text{Pizza} \\cap \\text{Movies})}{P(\\text{Pizza})} = \\frac{0.4}{0.7} = \\frac{4}{7}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#advanced-topics-preview",
    "href": "files/lecture_notes/lecture8/lecture8.html#advanced-topics-preview",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Advanced Topics Preview",
    "text": "Advanced Topics Preview\nMarkov Chains: - Sequences where future depends only on present - \\(P(X_{n+1} | X_n, X_{n-1}, \\ldots, X_1) = P(X_{n+1} | X_n)\\)\nBayesian Statistics: - Using Bayes’ theorem for statistical inference - Updating beliefs with data\nInformation Theory: - Conditional entropy - Mutual information"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#historical-context",
    "href": "files/lecture_notes/lecture8/lecture8.html#historical-context",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Historical Context",
    "text": "Historical Context\nThomas Bayes (1701-1761): - Presbyterian minister and mathematician - Bayes’ theorem published posthumously\nPierre-Simon Laplace (1749-1827): - Developed and popularized Bayesian methods - “Probability is nothing but common sense reduced to calculation”\nModern Applications: AI, machine learning, medical diagnosis, finance"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#common-student-questions",
    "href": "files/lecture_notes/lecture8/lecture8.html#common-student-questions",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Common Student Questions",
    "text": "Common Student Questions\nQ: “How do I know when to use Bayes’ theorem?” A: When you want to “reverse” a conditional probability\nQ: “Why are medical test problems so counterintuitive?”\nA: Base rates matter more than we intuitively expect\nQ: “What’s the difference between independence and conditional independence?” A: Independence means no relationship; conditional independence means no relationship given specific information"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture8/lecture8.html#key-formulas-summary",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\times P(B_i)\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)\nIndependence: \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#looking-ahead",
    "href": "files/lecture_notes/lecture8/lecture8.html#looking-ahead",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Statistical Inference\nTopics we’ll cover:\n\nSampling distributions\nConfidence intervals\nHypothesis testing\np-values and significance\n\n\nConnection: Continuous distributions (especially normal) form the foundation for statistical inference"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#study-tips",
    "href": "files/lecture_notes/lecture8/lecture8.html#study-tips",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Study Tips",
    "text": "Study Tips\n\nPractice with real scenarios: Medical tests, quality control\nDraw diagrams: Tree diagrams and two-way tables\nCheck your intuition: Do answers make sense?\nMaster the basics: Conditional probability formula\nWatch for fallacies: Don’t confuse \\(P(A|B)\\) and \\(P(B|A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#final-thoughts",
    "href": "files/lecture_notes/lecture8/lecture8.html#final-thoughts",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nConditional probability is everywhere: - Updates beliefs with new information - Foundation of Bayesian thinking - Critical for proper statistical reasoning - Essential for machine learning and AI\n\nKey insight: Information changes probability - embrace this uncertainty!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#questions",
    "href": "files/lecture_notes/lecture8/lecture8.html#questions",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Statistical Inference and Hypothesis Testing"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#bonus-monty-hall-problem",
    "href": "files/lecture_notes/lecture8/lecture8.html#bonus-monty-hall-problem",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bonus: Monty Hall Problem",
    "text": "Bonus: Monty Hall Problem\nThree doors: one has a car, two have goats 1. You choose a door 2. Host opens a door with a goat 3. Do you switch?\n\nAnswer: Yes! Switch! - \\(P(\\text{Car behind your door}) = \\frac{1}{3}\\) - \\(P(\\text{Car behind other remaining door}) = \\frac{2}{3}\\)\nConditional probability in action!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#bonus-birthday-paradox-connection",
    "href": "files/lecture_notes/lecture8/lecture8.html#bonus-birthday-paradox-connection",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bonus: Birthday Paradox Connection",
    "text": "Bonus: Birthday Paradox Connection\nIn a room of 23 people, probability of shared birthday ≈ 50%\nConditional approach: What’s \\(P(\\text{no match | first $k$ people have different birthdays})\\)?\nThis helps build intuition for why the probability grows so quickly!\nSurprising results often involve conditional probability!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html",
    "href": "files/lecture_notes/lecture7/lecture7.html",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "",
    "text": "Discrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-5",
    "href": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-5",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 5",
    "text": "Welcome to Lecture 5\n\n\n\n\n\n\n\nCounting\n\n\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture7/lecture7.html#todays-learning-objectives",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine random variables and distinguish discrete from continuous\nWork with probability mass functions (PMFs) and cumulative distribution functions (CDFs)\nCalculate expected values and variances for discrete random variables\nApply properties of expectation and variance\nWork with common discrete distributions (Bernoulli, Binomial, Geometric, Poisson)\nSolve real-world problems using discrete random variables\nUse python to compute probabilities and parameters"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#why-study-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#why-study-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Why Study Counting?",
    "text": "Why Study Counting?\n\n\n\n\n\nCounting helps us:\n\nCalculate probabilities for complex events\n\nSolve optimization problems\n\nUnderstand combinations in genetics, computer science\n\nAnalyze algorithms and data structures\n\nMake decisions involving arrangements and selections\n\n\n\nReal-world applications of counting include:\n\nCryptography: Password strength and encryption key space\nGenetics: DNA sequence analysis and gene combinations\n\nTournament brackets: March Madness and sports competitions\nLottery odds: Probability calculations for games of chance\nPassword security: Character combinations and brute force protection"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle",
    "href": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\nStart\n├── Task 1: n₁ ways\n│   ├── Choice 1 ──┐\n│   ├── Choice 2 ──┼── Task 2: n₂ ways\n│   └── Choice n₁ ──┘\n│       ├── Choice 1 ──┐\n│       ├── Choice 2 ──┼── Total: n₁ × n₂ × ... × nₖ\n│       └── Choice n₂ ──┘"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#simple-counting-example",
    "href": "files/lecture_notes/lecture7/lecture7.html#simple-counting-example",
    "title": "PSTAT 5A: Counting",
    "section": "Simple Counting Example",
    "text": "Simple Counting Example\n\nFormat ABC-123\n\nFirst position: 26 letters\nSecond position: 26 letters\nThird position: 26 letters\nFourth position: 10 digits\nFifth position: 10 digits\nSixth position: 10 digits\n\n\n\n\nSolution. Total possibilities: \\(26 \\times 26 \\times 26 \\times 10 \\times 10 \\times 10 = 26^3 \\times 10^3 = 17,576,000\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#restaurant-menu-example",
    "href": "files/lecture_notes/lecture7/lecture7.html#restaurant-menu-example",
    "title": "PSTAT 5A: Counting",
    "section": "Restaurant Menu Example",
    "text": "Restaurant Menu Example\n\nA restaurant offers:\n🍤 Appetizers: 4\n🍲 Main Courses: 6\n🍰 Desserts: 3\nHow many different three-course meals are possible?\n\n\nSolution. \\(4 \\times 6 \\times 3 = 72\\) different meals"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-1",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-1",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\nA box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let \\(X\\) = number of red balls drawn. Find the PMF of \\(X\\).\n\nShow Solution\n\n\nSolution. \\(X\\) can take values 0, 1, or 2.\n\\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{2}{2}}{\\binom{5}{2}} = \\frac{1 \\times 1}{10} = \\frac{1}{10}\\)\n\\(P(X = 1) = \\frac{\\binom{3}{1}\\binom{2}{1}}{\\binom{5}{2}} = \\frac{3 \\times 2}{10} = \\frac{6}{10}\\)\n\\(P(X = 2) = \\frac{\\binom{3}{2}\\binom{2}{0}}{\\binom{5}{2}} = \\frac{3 \\times 1}{10} = \\frac{3}{10}\\)\nCheck: \\(\\frac{1}{10} + \\frac{6}{10} + \\frac{3}{10} = 1\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#what-are-permutations",
    "href": "files/lecture_notes/lecture7/lecture7.html#what-are-permutations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Permutations?",
    "text": "What Are Permutations?\n\n\n\n\n\n\n\nPermutation\n\n\nAn arrangement of objects where order matters\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll permutations of ABC:\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\n\n\n\n\n\nRace finish positions (1st, 2nd, 3rd)\nSeating arrangements\nPasswords\nDNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-of-n-distinct-objects",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-of-n-distinct-objects",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(n\\) Distinct Objects",
    "text": "Permutations of \\(n\\) Distinct Objects\n\n\n\n\n\n\n\n\n\nKey Formula\n\n\nThe number of ways to arrange \\(n\\) distinct objects is:\n\\[n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\]\n\n\n\n\n\n\n\n\n\n\n\nSeating Process:\n1st position: 5 choices (Alice, Bob, Carol, David, Eve)\n2nd position: 4 choices (whoever is left)\n3rd position: 3 choices (whoever is left)\n4th position: 2 choices (whoever is left)\n5th position: 1 choice (last person)\nHow many ways can 5 people sit in a row?\n\n\nSolution. \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#factorial-values",
    "href": "files/lecture_notes/lecture7/lecture7.html#factorial-values",
    "title": "PSTAT 5A: Counting",
    "section": "Factorial Values",
    "text": "Factorial Values\n\n\n\n\n\n\\(n\\)\n\\(n!\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n2\n2\n\n\n3\n6\n\n\n4\n24\n\n\n5\n120\n\n\n10\n3,628,800\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nNote\n\n\n\\(0! = 1\\) by definition"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-of-r-objects-from-n",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-of-r-objects-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\)",
    "text": "Permutations of \\(r\\) Objects from \\(n\\)\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\n\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#understanding-pnr",
    "href": "files/lecture_notes/lecture7/lecture7.html#understanding-pnr",
    "title": "PSTAT 5A: Counting",
    "section": "Understanding \\(P(n,r)\\)",
    "text": "Understanding \\(P(n,r)\\)\nWhy is \\(P(n,r) = \\frac{n!}{(n-r)!}\\)?\n\nFirst position: \\(n\\) choices\nSecond position: \\((n-1)\\) choices\n\nThird position: \\((n-2)\\) choices\n…\n\\(r\\)-th position: \\((n-r+1)\\) choices\n\n\nTotal: \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-r+1) = \\frac{n!}{(n-r)!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-2",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-2",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\nUsing the red balls example from Problem 1, find:\n\n\\(F(0.5)\\)\n\\(F(1)\\)\n\n\\(P(X &gt; 1)\\)\n\\(P(0.5 &lt; X \\leq 1.5)\\)\n\n\nSolutions: a) \\(F(0.5) = P(X \\leq 0.5) = P(X = 0) = \\frac{1}{10}\\) b) \\(F(1) = P(X \\leq 1) = P(X = 0) + P(X = 1) = \\frac{1}{10} + \\frac{6}{10} = \\frac{7}{10}\\) c) \\(P(X &gt; 1) = 1 - F(1) = 1 - \\frac{7}{10} = \\frac{3}{10}\\) d) \\(P(0.5 &lt; X \\leq 1.5) = F(1.5) - F(0.5) = \\frac{7}{10} - \\frac{1}{10} = \\frac{6}{10}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-with-repetition",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-with-repetition",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition",
    "text": "Permutations with Repetition\n\n\n\n\n\n\n\nPermutations with Repetition\n\n\nWhen some objects are identical, we have fewer distinct arrangements\nIf we have \\(n\\) objects where: - \\(n_1\\) are of type 1 - \\(n_2\\) are of type 2\n- … - \\(n_k\\) are of type \\(k\\)\nNumber of distinct arrangements: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-with-repetition-example",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-with-repetition-example",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition Example",
    "text": "Permutations with Repetition Example\n\n\n\nHow many distinct arrangements are there of the letters in “STATISTICS”?\n\nS-T-A-T-I-S-T-I-C-S\n\nTotal letters: 10\nS appears 3 times\nT appears 3 times\nA appears 1 time\nI appears 2 times\nC appears 1 time\n\n\n\nSolution. \\(\\frac{10!}{3! \\times 3! \\times 1! \\times 2! \\times 1!} = \\frac{3,628,800}{6 \\times 6 \\times 1 \\times 2 \\times 1} = \\frac{3,628,800}{72} = 50,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#what-are-combinations",
    "href": "files/lecture_notes/lecture7/lecture7.html#what-are-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\n\n\n\n\n\n\n\nCombination\n\n\nA selection of objects where order does NOT matter\n\n\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\n\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\nKey Point: Order doesn't matter for combinations\n\n\n\nChoosing committee members\nSelecting pizza toppings\nForming study groups\nLottery number selection\n\n\n\n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#combinations-formula",
    "href": "files/lecture_notes/lecture7/lecture7.html#combinations-formula",
    "title": "PSTAT 5A: Counting",
    "section": "Combinations Formula",
    "text": "Combinations Formula\nThis section addresses the learning objective: Calculate combinations and understand when to use them\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(C(n,r)\\) or \\(\\binom{n}{r}\\): Number of ways to choose \\(r\\) objects from \\(n\\) distinct objects (order doesn’t matter)\n\\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]\n\n\n\n\n\nHow many ways can we choose 3 people from a group of 8 for a committee?\n\\(C(8,3) = \\frac{8!}{3!(8-3)!} = \\frac{8!}{3! \\times 5!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 56\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#relationship-permutations-vs-combinations",
    "href": "files/lecture_notes/lecture7/lecture7.html#relationship-permutations-vs-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Relationship: Permutations vs Combinations",
    "text": "Relationship: Permutations vs Combinations\n\n\n\n\n\n\n\nRelationship\n\n\n\\(P(n,r) = C(n,r) \\times r!\\)\nWhy? For each combination of \\(r\\) objects, there are \\(r!\\) ways to arrange them\n\n\n\n\n\nRelationship: P(n,r) = C(n,r) × r!\n\nExample: Choose 3 from 8 people\n- C(8,3) = 56 combinations\n- For each combination, arrange 3 people: 3! = 6 ways\n- Total: 56 × 6 = 336 = P(8,3)\n\nCombination ABC → Permutations: ABC, ACB, BAC, BCA, CAB, CBA\n\n\n\\(P(8,3) = C(8,3) \\times 3! = 56 \\times 6 = 336\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#key-decision-permutation-or-combination",
    "href": "files/lecture_notes/lecture7/lecture7.html#key-decision-permutation-or-combination",
    "title": "PSTAT 5A: Counting",
    "section": "Key Decision: Permutation or Combination?",
    "text": "Key Decision: Permutation or Combination?\n\n\n\n\n\n\n\n\n\nHow to Decide\n\n\nAsk yourself: Does order matter?\nOrder matters → Use Permutations - Arrangements, sequences, rankings\nOrder doesn’t matter → Use Combinations\n- Selections, groups, subsets"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-3",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-3",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nA student takes a 10-question multiple choice quiz with 4 options per question. If the student guesses randomly, what’s the probability of getting exactly 3 correct?\n\nShow Solution\n\n\nSolution. This is a binomial distribution with \\(n = 10\\), \\(p = 1/4 = 0.25\\)\n\\[P(X = 3) = \\binom{10}{3} \\times (0.25)^3 \\times (0.75)^7\\]\n\\[P(X = 3) = 120 \\times 0.015625 \\times 0.1335 \\approx 0.2503\\]\nSo there’s about a 25% chance of getting exactly 3 correct by guessing."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#properties-of-combinations",
    "href": "files/lecture_notes/lecture7/lecture7.html#properties-of-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Properties of Combinations",
    "text": "Properties of Combinations\n\n\n\n\n\n\n\nProperties\n\n\n\nSymmetry: \\(\\binom{n}{r} = \\binom{n}{n-r}\\)\nPascal’s Identity: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nBoundary conditions: \\(\\binom{n}{0} = \\binom{n}{n} = 1\\)\n\n\n\n\n\n\n\\(\\binom{8}{3} = \\binom{8}{5} = 56\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#pascals-triangle",
    "href": "files/lecture_notes/lecture7/lecture7.html#pascals-triangle",
    "title": "PSTAT 5A: Counting",
    "section": "Pascal’s Triangle",
    "text": "Pascal’s Triangle\n\n\n\n           1\n         1   1\n       1   2   1\n     1   3   3   1\n   1   4   6   4   1\n 1   5  10  10   5   1\n1   6  15  20  15   6   1\nPattern: Each number is the sum of the two numbers above it.\nFormula: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nExample: \\(\\binom{4}{2} = 6\\) (row 4, position 2)\n\n\n\n\n\n                            \n                                            \n\n\nPascal's Triangle (showing C(n,r) values):\nRow 0: [1]\nRow 1: [1, 1]\nRow 2: [1, 2, 1]\nRow 3: [1, 3, 3, 1]\nRow 4: [1, 4, 6, 4, 1]\nRow 5: [1, 5, 10, 10, 5, 1]\nRow 6: [1, 6, 15, 20, 15, 6, 1]\nRow 7: [1, 7, 21, 35, 35, 21, 7, 1]\n\n\n\nEach number is \\(\\binom{n}{r}\\) where \\(n\\) is the row and \\(r\\) is the position"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#binomial-theorem",
    "href": "files/lecture_notes/lecture7/lecture7.html#binomial-theorem",
    "title": "PSTAT 5A: Counting",
    "section": "Binomial Theorem",
    "text": "Binomial Theorem\n\n\n\n\n\n\n\nKey Formula\n\n\n\\[(x + y)^n = \\sum_{r=0}^{n} \\binom{n}{r} x^{n-r} y^r\\]\n\n\n\n\n\n\\((x + y)^3 = \\binom{3}{0}x^3 + \\binom{3}{1}x^2y + \\binom{3}{2}xy^2 + \\binom{3}{3}y^3\\)\n\\(= x^3 + 3x^2y + 3xy^2 + y^3\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#counting-and-probability",
    "href": "files/lecture_notes/lecture7/lecture7.html#counting-and-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\n\n\nA committee of 4 people is chosen from 7 women and 5 men. What’s the probability that exactly 2 are women?\n\nTotal ways to choose 4 from 12: \\(\\binom{12}{4} = 495\\)\nWays to choose 2 women from 7: \\(\\binom{7}{2} = 21\\) Ways to choose 2 men from 5: \\(\\binom{5}{2} = 10\\)\n\nFavorable outcomes: \\(\\binom{7}{2} \\times \\binom{5}{2} = 21 \\times 10 = 210\\)\nProbability: \\(\\frac{210}{495} = \\frac{14}{33}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-4",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-4",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\nFor the red balls example, find the variance and standard deviation.\nWe found \\(E[X] = 1.2\\). First find \\(E[X^2]\\):\n\n\\[E[X^2] = 0^2 \\cdot \\frac{1}{10} + 1^2 \\cdot \\frac{6}{10} + 2^2 \\cdot \\frac{3}{10}\\] \\[= 0 + \\frac{6}{10} + \\frac{12}{10} = \\frac{18}{10} = 1.8\\]\n\\[\\text{Var}(X) = 1.8 - (1.2)^2 = 1.8 - 1.44 = 0.36\\]\n\\[\\sigma = \\sqrt{0.36} = 0.6\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-4-continued",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-4-continued",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4 (continued)",
    "text": "Practice Problem 4 (continued)\n\n\nAt least 1 ace = 1 - (no aces)\n\n\n\n\n\n\n\n\nTip\n\n\nWays to choose 5 cards with no aces: \\(\\binom{48}{5} = 1,712,304\\)\n\n\n\n\nSolution. \\(P(\\text{at least 1 ace}) = 1 - \\frac{\\binom{48}{5}}{\\binom{52}{5}} = 1 - \\frac{1,712,304}{2,598,960} \\approx 0.341\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#circular-permutations",
    "href": "files/lecture_notes/lecture7/lecture7.html#circular-permutations",
    "title": "PSTAT 5A: Counting",
    "section": "Circular Permutations",
    "text": "Circular Permutations\n\n\n\n\n\n\n\nCircular Permutations\n\n\nWhen arranging objects in a circle, we fix one object to eliminate rotational symmetry\nNumber of circular permutations of \\(n\\) objects: \\((n-1)!\\)\n\n\n\n\n\nLinear Arrangement: ABCDEF\n- 6! = 720 different ways\n- ABCDEF ≠ BCDEFA\n\nCircular Arrangement: ABCDEF\n- 5! = 120 different ways  \n- ABCDEF = BCDEFA (same arrangement rotated)\n\nKey Point: In a circle, rotations are the same!\n\n\n\n\n                            \n                                            \n\n\nCircular arrangement of 6 people:\nNumber of arrangements: (6-1)! = 120\nPeople: A, B, C, D, E, F\n\n\n\n\nHow many ways can 6 people sit around a circular table?\n\\((6-1)! = 5! = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-with-restrictions",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-with-restrictions",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Restrictions",
    "text": "Permutations with Restrictions\n\nHow many 6-letter “words” can be formed from the letters A, B, C, D, E, F if:\n\nNo letter is repeated\nA and B must be adjacent\n\n\n\nSolution. Treat AB as a single unit\n\n5 units to arrange: (AB), C, D, E, F → \\(5! = 120\\) ways\nA and B can be arranged within their unit: \\(2! = 2\\) ways\nTotal: \\(5! \\times 2! = 240\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#the-inclusion-exclusion-principle",
    "href": "files/lecture_notes/lecture7/lecture7.html#the-inclusion-exclusion-principle",
    "title": "PSTAT 5A: Counting",
    "section": "The Inclusion-Exclusion Principle",
    "text": "The Inclusion-Exclusion Principle\n\n\n\n\n\n\n\nKey Formula\n\n\nFor two sets \\(A\\) and \\(B\\): \\[|A \\cup B| = |A| + |B| - |A \\cap B|\\]\nFor three sets \\(A\\), \\(B\\), and \\(C\\): \\[|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\]\n\n\n\n\n\nStep-by-step process:\n1. Count elements in A: |A|\n2. Count elements in B: |B|  \n3. Subtract overlap: |A∩B| (avoid double counting)\n4. Result: |A∪B| = |A| + |B| - |A∩B|\n\nThe overlap gets counted twice, so we subtract it once!\n\n\n\n\n                            \n                                            \n\n\nInclusion-Exclusion Principle:\n|A∪B| = |A| + |B| - |A∩B|\nWe add |A| and |B|, then subtract |A∩B| to avoid double counting!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion-example",
    "href": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion-example",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Example",
    "text": "Inclusion-Exclusion Example\n\nHow many integers from 1 to 100 are divisible by 2, 3, or 5?\n\nLet:\n\n\\(A\\) = divisible by 2: \\(|A| = 50\\)\n\\(B\\) = divisible by 3: \\(|B| = 33\\)\n\\(C\\) = divisible by 5: \\(|C| = 20\\)\n\n\n\n\n\n\n\nNote\n\n\n\\(|A \\cap B| = 16\\) (divisible by 6)\n\\(|A \\cap C| = 10\\) (divisible by 10)\n\\(|B \\cap C| = 6\\) (divisible by 15)\n\\(|A \\cap B \\cap C| = 3\\) (divisible by 30)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion-solution",
    "href": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion-solution",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Solution",
    "text": "Inclusion-Exclusion Solution\n\nSolution. \\[|A \\cup B \\cup C| = 50 + 33 + 20 - 16 - 10 - 6 + 3 = 74\\]\nAnswer: 74 integers from 1 to 100 are divisible by at least one of 2, 3, or 5"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#derangements",
    "href": "files/lecture_notes/lecture7/lecture7.html#derangements",
    "title": "PSTAT 5A: Counting",
    "section": "Derangements",
    "text": "Derangements\n\n\n\n\n\n\n\nDerangement\n\n\nA derangement is a permutation where no object appears in its original position\nNumber of derangements of \\(n\\) objects: \\[D_n = n! \\sum_{k=0}^{n} \\frac{(-1)^k}{k!} \\approx \\frac{n!}{e}\\]\n\n\n\n\n\nHow many ways can 4 people return hats to the wrong owners?\n\\(D_4 = 4! \\left(\\frac{1}{0!} - \\frac{1}{1!} + \\frac{1}{2!} - \\frac{1}{3!} + \\frac{1}{4!}\\right) = 24 \\times \\frac{9}{24} = 9\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#multinomial-coefficients",
    "href": "files/lecture_notes/lecture7/lecture7.html#multinomial-coefficients",
    "title": "PSTAT 5A: Counting",
    "section": "Multinomial Coefficients",
    "text": "Multinomial Coefficients\n\n\n\n\n\n\n\nMultinomial Coefficient\n\n\nNumber of ways to divide \\(n\\) objects into groups of sizes \\(n_1, n_2, \\ldots, n_k\\):\n\\[\\binom{n}{n_1, n_2, \\ldots, n_k} = \\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\]\n\n\n\n\n\nHow many ways can 12 people be divided into 3 teams of 4?\n\\(\\binom{12}{4,4,4} = \\frac{12!}{4! \\times 4! \\times 4!} = 34,650\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#stars-and-bars",
    "href": "files/lecture_notes/lecture7/lecture7.html#stars-and-bars",
    "title": "PSTAT 5A: Counting",
    "section": "Stars and Bars",
    "text": "Stars and Bars\n\n\n\n\n\n\n\nStars and Bars\n\n\nProblem: Number of ways to distribute \\(n\\) identical objects into \\(k\\) distinct bins\nSolution: \\(\\binom{n+k-1}{k-1}\\) or \\(\\binom{n+k-1}{n}\\)\n\n\n\n\n\nStars and Bars Method:\n- Use * for objects (candies)\n- Use | for dividers (separating kids)\n- Example: ***|**|***|** means 3,2,3,2 candies\n\nTotal positions: 10 candies + 3 dividers = 13\nChoose 3 positions for dividers: C(13,3) = 286 ways\n\nOther examples:\n**|***|*|**** → 2,3,1,4 candies\n****|*|**|*** → 4,1,2,3 candies\n\n\n\n\n                            \n                                            \n\n\nStars and Bars Examples:\nExample 1: ****|***|***|\nExample 2: |*|*|********\nExample 3: *||********|*\nExample 4: **|*||*******\nExample 5: **|*****||***\n\n\n\n\nHow many ways can you distribute 10 identical candies to 4 children?\n\\(\\binom{10+4-1}{4-1} = \\binom{13}{3} = 286\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-5",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-5",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 5",
    "text": "Practice Problem 5\nFor the quiz example:\n\nWhat’s the expected number of correct answers?\nWhat’s the standard deviation?\nWhat’s the probability of getting at least 4 correct?\n\n\nSolutions: a) \\(E[X] = np = 10 \\times 0.25 = 2.5\\) b) \\(\\sigma = \\sqrt{np(1-p)} = \\sqrt{10 \\times 0.25 \\times 0.75} = \\sqrt{1.875} \\approx 1.37\\) c) \\(P(X \\geq 4) = 1 - P(X \\leq 3) = 1 - [P(X=0) + P(X=1) + P(X=2) + P(X=3)]\\) Use binomial table or calculator: \\(P(X \\geq 4) \\approx 0.224\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#advanced-counting-stirling-numbers",
    "href": "files/lecture_notes/lecture7/lecture7.html#advanced-counting-stirling-numbers",
    "title": "PSTAT 5A: Counting",
    "section": "Advanced Counting: Stirling Numbers",
    "text": "Advanced Counting: Stirling Numbers\n\n\n\n\n\n\n\nStirling Numbers\n\n\nStirling numbers of the second kind \\(S(n,k)\\): Number of ways to partition \\(n\\) objects into \\(k\\) non-empty subsets\nBell numbers \\(B_n\\): Total number of ways to partition \\(n\\) objects \\[B_n = \\sum_{k=0}^{n} S(n,k)\\]\nThese are advanced topics for further study"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture7/lecture7.html#problem-solving-strategy",
    "title": "PSTAT 5A: Counting",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\nThis section addresses the learning objective: Solve complex counting problems systematically\n\n\n\n\n\n\n\nStrategy\n\n\n\nRead carefully: What exactly are we counting?\nIdentify the type: Permutation, combination, or other?\nCheck for restrictions: Are there constraints?\nDoes order matter?: This determines permutation vs combination\nBreak down complex problems: Use multiplication principle\nVerify your answer: Does it make sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture7/lecture7.html#common-mistakes-to-avoid",
    "title": "PSTAT 5A: Counting",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n\n\n\n\n\n\nCommon Mistakes\n\n\n\nConfusing permutations and combinations\n\nAlways ask: “Does order matter?”\n\nForgetting about restrictions\n\nRead the problem carefully\n\nDouble counting\n\nMake sure you’re not counting the same arrangement twice\n\nNot considering the complement\n\nSometimes “at least” problems are easier using complements"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-6",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-6",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\nA quality control inspector tests items until finding the first defective one. If 5% of items are defective:\n\nWhat’s the probability the first defective item is the 10th one tested?\nWhat’s the expected number of items tested?\nWhat’s the probability of testing more than 20 items?\n\n\nSolutions: a) \\(P(X = 10) = (0.95)^9 \\times 0.05 \\approx 0.0315\\) b) \\(E[X] = \\frac{1}{0.05} = 20\\) items c) \\(P(X &gt; 20) = (0.95)^{20} \\approx 0.358\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#counting-in-computer-science",
    "href": "files/lecture_notes/lecture7/lecture7.html#counting-in-computer-science",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Computer Science",
    "text": "Counting in Computer Science\n\nPassword Security:\n\n8-character password with letters, digits, symbols\n\\((26 + 26 + 10 + 32)^8 = 94^8 \\approx 6.1 \\times 10^{15}\\)\n\nHash Functions:\n\nDistributing data into buckets\nCollision probability calculations\n\nAlgorithm Analysis:\n\nCounting operations, comparisons\nBig O notation foundations"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#counting-in-genetics",
    "href": "files/lecture_notes/lecture7/lecture7.html#counting-in-genetics",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Genetics",
    "text": "Counting in Genetics\n\nDNA Sequences:\n\n4 bases (A, T, G, C)\nGene of length \\(n\\): \\(4^n\\) possible sequences\n\nProtein Folding:\n\nNumber of possible conformations\nCombinatorial explosion\n\nPopulation Genetics:\n\nHardy-Weinberg calculations\nAllele combinations"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#real-world-applications",
    "href": "files/lecture_notes/lecture7/lecture7.html#real-world-applications",
    "title": "PSTAT 5A: Counting",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nLottery:\n\nPowerball: Choose 5 from 69, then 1 from 26\nOdds: \\(\\frac{1}{\\binom{69}{5} \\times 26} \\approx \\frac{1}{292,000,000}\\)\n\nCryptography:\n\nKey space size determines security\nRSA encryption relies on large number factorization\n\nSports Tournaments:\n\nMarch Madness bracket: \\(2^{63}\\) possible outcomes\nRound-robin tournaments: \\(\\binom{n}{2}\\) games"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture7/lecture7.html#key-formulas-summary",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\nExpected Value: \\(E[X] = \\sum x \\cdot P(X = x)\\)\nVariance: \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\)\nBinomial: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nGeometric: \\(P(X = k) = (1-p)^{k-1} p\\)\nPoisson: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#technology-and-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#technology-and-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Technology and Counting",
    "text": "Technology and Counting\n\n\n\n\n\n\n\nTools\n\n\nCalculators:\n\nUse nPr and nCr functions\nBe careful with large numbers\n\nSoftware:\n\nR: factorial(), choose(), combn()\nPython: math.factorial(), math.comb()\nExcel: FACT(), COMBIN(), PERMUT()\n\nOnline Tools:\n\nWolfram Alpha for complex calculations\nCombination/permutation calculators"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-7",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\nFor the call center example:\n\nWhat’s the probability of no calls in a minute?\nWhat’s the probability of at most 2 calls?\nIn a 2-minute period, what’s the expected number of calls?\n\n\nSolutions: a) \\(P(X = 0) = \\frac{3^0 e^{-3}}{0!} = e^{-3} \\approx 0.0498\\) b) \\(P(X \\leq 2) = P(X=0) + P(X=1) + P(X=2) \\approx 0.0498 + 0.1494 + 0.2240 = 0.423\\) c) For 2 minutes: \\(Y \\sim \\text{Poisson}(6)\\), so \\(E[Y] = 6\\) calls"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#extending-to-probability",
    "href": "files/lecture_notes/lecture7/lecture7.html#extending-to-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Extending to Probability",
    "text": "Extending to Probability\n\n\n\n\n\n\n\nDistributions\n\n\nHypergeometric Distribution:\n\nDrawing without replacement\nUses combinations: \\(P(X = k) = \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\nBinomial Distribution:\n\nDrawing with replacement\nUses combinations: \\(P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\)\n\nWe’ll explore these distributions in detail in future lectures"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#historical-note",
    "href": "files/lecture_notes/lecture7/lecture7.html#historical-note",
    "title": "PSTAT 5A: Counting",
    "section": "Historical Note",
    "text": "Historical Note\n\n\n\n\n\n\n\nHistory\n\n\nBlaise Pascal (1623-1662) and Pierre de Fermat (1601-1665): - Founded probability theory through gambling problems - Pascal’s triangle and combinations\nLeonhard Euler (1707-1783): - Advanced combinatorics - Graph theory connections\nModern applications span computer science, biology, physics, and economics"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#common-student-questions",
    "href": "files/lecture_notes/lecture7/lecture7.html#common-student-questions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Student Questions",
    "text": "Common Student Questions\nQ: “How do I choose between binomial and hypergeometric?” A: Use binomial for sampling with replacement, hypergeometric without\nQ: “When can I use Poisson approximation?” A: When \\(n\\) is large, \\(p\\) is small, and \\(np\\) is moderate\nQ: “Why does Poisson have mean = variance?” A: Mathematical property arising from its derivation as a limit\nQ: “How do I know if trials are independent?” A: Check if outcome of one trial affects others"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#looking-ahead",
    "href": "files/lecture_notes/lecture7/lecture7.html#looking-ahead",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Continuous Random Variables\nTopics we’ll cover:\n\nProbability density functions (PDFs)\nNormal distribution\nExponential distribution\nCentral Limit Theorem applications\n\n\nConnection: Discrete distributions often approximate continuous ones, and vice versa"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#study-tips",
    "href": "files/lecture_notes/lecture7/lecture7.html#study-tips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Study Tips",
    "text": "Study Tips\n\nMaster the basics: PMF, CDF, expectation, variance\nLearn distribution characteristics: When to use each one\nPractice with technology: Get comfortable with calculators/software\nWork real problems: Apply distributions to actual scenarios\nCheck your intuition: Do answers make practical sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#final-thoughts",
    "href": "files/lecture_notes/lecture7/lecture7.html#final-thoughts",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nDiscrete random variables are fundamental to: - Modeling real-world phenomena - Making statistical inferences - Understanding probability theory - Building more complex models\n\nKey insight: Random variables transform unpredictable outcomes into predictable patterns"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#questions",
    "href": "files/lecture_notes/lecture7/lecture7.html#questions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Continuous Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#bonus-large-number-approximations",
    "href": "files/lecture_notes/lecture7/lecture7.html#bonus-large-number-approximations",
    "title": "PSTAT 5A: Counting",
    "section": "Bonus: Large Number Approximations",
    "text": "Bonus: Large Number Approximations\n\n\n\n\n\n\n\nApproximations\n\n\nStirling’s Approximation: \\(n! \\approx \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n\\)\nNormal Approximation: For large \\(n\\), \\(\\binom{n}{k} \\approx \\frac{1}{\\sqrt{2\\pi n p(1-p)}} \\exp\\left(-\\frac{(k-np)^2}{2np(1-p)}\\right)\\)\nUseful for computational purposes when exact values are too large"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Course Resources",
    "section": "",
    "text": "🔍"
  },
  {
    "objectID": "resources.html#week-1-foundations-of-data-science",
    "href": "resources.html#week-1-foundations-of-data-science",
    "title": "Week 4: Continuous Random Variables & Confidence Intervals",
    "section": "Week 1: Foundations of Data Science",
    "text": "Week 1: Foundations of Data Science"
  },
  {
    "objectID": "resources.html#getting-started-with-data",
    "href": "resources.html#getting-started-with-data",
    "title": "Course Resources",
    "section": "Getting Started with Data",
    "text": "Getting Started with Data\nThis week introduces fundamental concepts in data science, including data types, basic statistics, and essential Python tools for data manipulation and analysis."
  },
  {
    "objectID": "resources.html#week-2-introduction-to-probability",
    "href": "resources.html#week-2-introduction-to-probability",
    "title": "Week 4: Continuous Random Variables & Confidence Intervals",
    "section": "Week 2: Introduction to Probability",
    "text": "Week 2: Introduction to Probability"
  },
  {
    "objectID": "resources.html#understanding-uncertainty-through-statistics",
    "href": "resources.html#understanding-uncertainty-through-statistics",
    "title": "Course Resources",
    "section": "Understanding Uncertainty Through Statistics",
    "text": "Understanding Uncertainty Through Statistics\nThis week introduces fundamental concepts in probability theory, including sample spaces, events, conditional probability, independence, and Bayes’ theorem. You’ll learn to quantify uncertainty and make informed decisions with incomplete information."
  },
  {
    "objectID": "resources.html#week-3-statistical-analysis",
    "href": "resources.html#week-3-statistical-analysis",
    "title": "Course Resources",
    "section": "Week 3: Statistical Analysis",
    "text": "Week 3: Statistical Analysis\n\n\n📊\n\n\nStatistical Inference & Confidence Intervals (CI’s)\nWeek 3 will cover statistical inference,and confidence intervals. Materials will be posted by week 3."
  },
  {
    "objectID": "resources.html#week-4-statistical-methods-testing",
    "href": "resources.html#week-4-statistical-methods-testing",
    "title": "Course Resources",
    "section": "Week 4: Statistical Methods & Testing",
    "text": "Week 4: Statistical Methods & Testing\n\n\n🔬\n\n\nHypothesis Testing Fundamentals\nWeek 4 will cover hypothesis testing, and two sample t-Tests. Materials will be posted by week 4."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#law-of-large-numbers",
    "href": "files/lecture_notes/lecture9/lecture9.html#law-of-large-numbers",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nAs \\(n\\) increases, the sample mean approaches the expected value:\n\\[\\bar{X}_n = \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\to E[X]\\]\n\n\n\n\n\nline\n    title Law of Large Numbers Simulation\n    x-axis 0 100 200 300 400 500 600 700 800 900 1000\n    y-axis Sample Mean\n    0: 0.0\n    100: 0.48\n    200: 0.51\n    300: 0.49\n    400: 0.50\n    500: 0.51\n    600: 0.50\n    700: 0.50\n    800: 0.49\n    900: 0.50\n    1000: 0.50\n\n\n\n\n\n\nFigure: As the number of trials increases, the sample mean converges to the expected value."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#simulationmonte-carlo",
    "href": "files/lecture_notes/lecture9/lecture9.html#simulationmonte-carlo",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Simulation/Monte Carlo",
    "text": "Simulation/Monte Carlo\nMonte Carlo Method: Use computer simulation to estimate probabilities\n Figure: Monte Carlo simulation: repeated random experiments to estimate probabilities."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#mutually-exclusive-vs.-independent",
    "href": "files/lecture_notes/lecture5/lecture5.html#mutually-exclusive-vs.-independent",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive vs. Independent",
    "text": "Mutually Exclusive vs. Independent\n\nMutually Exclusive (left): the circles A and B do not overlap, so \\(P(A\\cap B)=0\\).\nIndependent (right): the circles overlap, and we’ve sized the intersection so that \\(P(A\\cap B)=P(A)\\,P(B)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#mutually-exclusive-vs.-independent-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#mutually-exclusive-vs.-independent-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Mutually Exclusive vs. Independent Example",
    "text": "Mutually Exclusive vs. Independent Example\n\n\n\nDraw a single card from a 52-card deck:\n\nLet A={“draw an Ace”}, so P(A)=4/52.\nLet B={“draw a King”}, so P(B)=4/52.\n\nQ: What is \\(P(A\\cap B)\\) ?\n\n\n\n\n\nSolution. They’re disjoint (you can’t draw an Ace and a King), so \\(P(A\\cap B) = 0\\).\nBut \\(P(A)\\,P(B) = \\frac{4}{52}\\times\\frac{4}{52} = \\frac{16}{2704} \\neq 0\\).\nHence, \\(P(A\\cap B)\\neq P(A)P(B)\\), so they’re not independent."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#multiplication-rule",
    "href": "files/lecture_notes/lecture5/lecture5.html#multiplication-rule",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nGeneral case: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nIndependent events: \\(P(A \\cap B) = P(A) \\times P(B)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#tree-diagrams",
    "href": "files/lecture_notes/lecture5/lecture5.html#tree-diagrams",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\n\n🎯 Definition Tree diagrams help visualize sequential events and calculate probabilities."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#tree-diagram-examples",
    "href": "files/lecture_notes/lecture5/lecture5.html#tree-diagram-examples",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Tree Diagram Examples",
    "text": "Tree Diagram Examples"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#practice-problem-2",
    "href": "files/lecture_notes/lecture5/lecture5.html#practice-problem-2",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\n\n\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement.\n\nWhat’s the probability both balls are red?\nWhat’s the probability the first is red and second is blue?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{both red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)\n\\(P(\\text{red then blue}) = \\frac{5}{8} \\times \\frac{3}{7} = \\frac{15}{56}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#law-of-total-probability",
    "href": "files/lecture_notes/lecture5/lecture5.html#law-of-total-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n🎯 Definition\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space, then:\n\\[P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#law-of-total-probability-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\n\nA factory has two machines:\n\nMachine 1: Produces 60% of items, 5% defective\nMachine 2: Produces 40% of items, 3% defective\n\nQ: What’s the overall probability an item is defective?\n\n\n\nSolution. \\(P(\\text{defective}) = P(D|M_1)P(M_1) + P(D|M_2)P(M_2)\\)\n\\(= 0.05 \\times 0.6 + 0.03 \\times 0.4 = 0.03 + 0.012 = 0.042\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem",
    "href": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n🎯 Definition \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nThis allows us to “reverse” conditional probabilities\nNamed after Thomas Bayes (1701-1761)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-components",
    "href": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-components",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\n\\(A,B\\): Events\n\\(P(A|B)\\): Posterior probability - what we want to find\n\\(P(B|A)\\): Likelihood - given \\(A\\), probability of observing \\(B\\)\n\\(P(A)\\): Prior probability - initial probability of \\(A\\)\n\\(P(B)\\): Marginal probability - total probability of \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Example",
    "text": "Bayes’ Theorem Example\n\n\n\nMedical test for a disease: 1\n\nDisease affects 1% of population\nTest is 95% accurate for sick people\nTest is 90% accurate for healthy people\n\nQ:If someone tests positive, what’s the probability they have the disease?\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-solution",
    "href": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-solution",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Solution",
    "text": "Bayes’ Theorem Solution\nLet:\n\n\\(D\\): Person has disease\n\\(T^+\\): Test is positive\n\nGiven:\n\n\\(P(D) = 0.01\\)\n\\(P(T^+|D) = 0.95\\)\n\\(P(T^-|D^c) = 0.90\\), so \\(P(T^+|D^c) = 0.10\\)\n\n\n\nSolution. \\(P(T^+) = P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\)\n\\(= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.1085\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-solution-cont.",
    "href": "files/lecture_notes/lecture5/lecture5.html#bayes-theorem-solution-cont.",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Bayes’ Theorem Solution (cont.)",
    "text": "Bayes’ Theorem Solution (cont.)\n\\[P(D|T^+) = \\frac{P(T^+|D) \\times P(D)}{P(T^+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\]\n\n\n\nSurprising result: Even with a positive test, there’s only an 8.8% chance of having the disease!\nThis is due to the low base rate of the disease"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#common-probability-mistakes",
    "href": "files/lecture_notes/lecture5/lecture5.html#common-probability-mistakes",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nConfusing \\(P(A|B)\\) with \\(P(B|A)\\)\n\n\nProsecutor’s fallacy is a specific error in interpreting conditional probabilities. Confusing\n\\(P(\\text{Evidence}\\mid\\text{Innocent})\n\\quad\\text{with}\\quad\nP(\\text{Innocent}\\mid\\text{Evidence})\\).\nEx: OJ Simpson Case 1\n\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#common-probability-mistakes-1",
    "href": "files/lecture_notes/lecture5/lecture5.html#common-probability-mistakes-1",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nAssuming independence when events are dependent\nIgnoring base rates (as in the medical test example)\n\n\nBase rate fallacy is when you ignore or underweight the prior probability \\(P(H)\\) of a hypothesis, focusing only on the new evidence \\(E\\).\n\n\nDouble counting in union calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#practice-problem-3",
    "href": "files/lecture_notes/lecture5/lecture5.html#practice-problem-3",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nTwo fair dice are rolled. Find:\n\n\\(P(\\text{sum} = 7)\\)\n\\(P(\\text{sum} = 7 | \\text{first die shows 3})\\)\nAre these events independent?\n\n\n\n\nSolution. \n\n6 ways out of 36: \\(P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\nGiven first die is 3, need second die to be 4: \\(P(\\text{sum} = 7 | \\text{first} = 3) = \\frac{1}{6}\\)\nYes, they’re independent since \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#counting-and-probability",
    "href": "files/lecture_notes/lecture5/lecture5.html#counting-and-probability",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\nSometimes we need to count outcomes:\n\nMultiplication Principle: If task 1 can be done in \\(m\\) ways and task 2 in \\(n\\) ways, both can be done in \\(m \\times n\\) ways\n\n\nPermutations: Arrangements where order matters \\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\nCombinations: Selections where order doesn’t matter \\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#counting-example",
    "href": "files/lecture_notes/lecture5/lecture5.html#counting-example",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Counting Example",
    "text": "Counting Example\n\nQ: How many ways can you arrange 5 people in a row?\n\n\n\nSolution. This is a permutation: \\(P(5,5) = 5! = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#probability-with-counting",
    "href": "files/lecture_notes/lecture5/lecture5.html#probability-with-counting",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Probability with Counting",
    "text": "Probability with Counting\n\nExample: A committee of 3 people is chosen from 8 people (5 women, 3 men). What’s the probability all 3 are women?\n\n\n\nSolution. Total ways to choose 3 from 8: \\(\\binom{8}{3} = 56\\)\nWays to choose 3 women from 5: \\(\\binom{5}{3} = 10\\)\nProbability: \\(\\frac{10}{56} = \\frac{5}{28}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#real-world-applications",
    "href": "files/lecture_notes/lecture5/lecture5.html#real-world-applications",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Diagnosis: Using Bayes’ theorem for test interpretation\nQuality Control: Probability of defective items\nFinance: Risk assessment and portfolio theory\nSports: Probability of wins, fantasy sports\nInsurance: Calculating premiums based on risk"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture5/lecture5.html#key-formulas-summary",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\nBasic probability: \\(P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\)\nComplement: \\(P(A^c) = 1 - P(A)\\)\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nConditional: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nIndependence: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nBayes’: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#problem-solving-strategy",
    "href": "files/lecture_notes/lecture5/lecture5.html#problem-solving-strategy",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\nIdentify the sample space and events\nDetermine if events are independent or mutually exclusive\nChoose the appropriate rule or formula\nCalculate step by step\nCheck if your answer makes sense"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#practice-problem-4",
    "href": "files/lecture_notes/lecture5/lecture5.html#practice-problem-4",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA bag contains 4 red, 3 blue, and 2 green marbles. Three marbles are drawn without replacement.\nFind the probability that: a) All three are red b) No two are the same color c) At least one is blue"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#practice-problem-4-solutions",
    "href": "files/lecture_notes/lecture5/lecture5.html#practice-problem-4-solutions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Practice Problem 4 Solutions",
    "text": "Practice Problem 4 Solutions\n\nSolution. \n\nAll red: \\(\\frac{4}{9} \\times \\frac{3}{8} \\times \\frac{2}{7} = \\frac{24}{504} = \\frac{1}{21}\\)\nDifferent colors: \\(\\frac{4 \\times 3 \\times 2}{9 \\times 8 \\times 7} \\times 3! = \\frac{24 \\times 6}{504} = \\frac{144}{504} = \\frac{2}{7}\\)\nAt least one blue: \\(1 - P(\\text{no blue}) = 1 - \\frac{6 \\times 5 \\times 4}{9 \\times 8 \\times 7} = 1 - \\frac{120}{504} = \\frac{384}{504} = \\frac{16}{21}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#common-questions",
    "href": "files/lecture_notes/lecture5/lecture5.html#common-questions",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Common Questions",
    "text": "Common Questions\n\nQ1.: “Why isn’t \\(P(A \\cup B) = P(A) + P(B)\\) always?”\nA: We’d double-count outcomes in both events\nQ2.: “How do I know if events are independent?”\nA: Check if \\(P(A|B) = P(A)\\) or if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nQ3.: “When do I use Bayes’ theorem?”\nA: When you want to “reverse” a conditional probability\n\n\nQ3 note (Bayes Example)\n\nForward: I know my test picks up disease 95% of the time ⇒ \\(P(+\\mid D)=0.95\\).\nReverse: I want the chance I really have the disease when the test is positive ⇒ \\(P(D\\mid +)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#looking-ahead",
    "href": "files/lecture_notes/lecture5/lecture5.html#looking-ahead",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture:\n\nCounting\nRandom Variables and Probability Distributions\nDiscrete vs. continuous random variables\nExpected value and variance"
  },
  {
    "objectID": "files/lecture_notes/lecture5/lecture5.html#final-thoughts",
    "href": "files/lecture_notes/lecture5/lecture5.html#final-thoughts",
    "title": "PSTAT 5A: Introduction to Probability",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nProbability is the foundation of statistics:\n\nHelps us quantify uncertainty\nProvides tools for making decisions with incomplete information\nEssential for understanding statistical inference\n\n\n\nPractice: The key to mastering probability is working through many problems!"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-1-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-1-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 1 Solutions",
    "text": "Task 1 Solutions\n\nTask 1 Solutions:\n\nImport the numpy module as np, and check that np.sin(0) returns a value of 0.\nImport the datascience module as ds, and check the table creation works correctly.\n\n\n\n# Solution to Task 1\n\n# Part 1: Import numpy and test sin function\nimport numpy as np\nprint(\"np.sin(0) =\", np.sin(0))\n\n# Part 2: Import datascience and test table creation\nimport datascience as ds\n\ntable_result = ds.Table().with_columns(\n    \"Col1\", [1, 2, 3],\n    \"Col2\", [2, 3, 4]\n)\nprint(table_result)\n\nnp.sin(0) = 0.0\nCol1 | Col2\n1    | 2\n2    | 3\n3    | 4"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-2-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-2-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 2 Solutions",
    "text": "Task 2 Solutions\n\nTask 2 Solutions: Create x_list and x_array containing elements 1, 2, and 3, then compute mean and median.\n\n\n# Solution to Task 2\n\n# Create x_list as a regular Python list\nx_list = [1, 2, 3]\n\n# Create x_array as a numpy array\nx_array = np.array([1, 2, 3])\n\n# Compute mean and median for x_list\nprint(\"x_list mean:\", np.mean(x_list))\nprint(\"x_list median:\", np.median(x_list))\n\n# Compute mean and median for x_array\nprint(\"x_array mean:\", np.mean(x_array))\nprint(\"x_array median:\", np.median(x_array))\n\n# Verify they give the same results\nprint(\"\\nBoth give the same results!\")\n\nx_list mean: 2.0\nx_list median: 2.0\nx_array mean: 2.0\nx_array median: 2.0\n\nBoth give the same results!"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-3-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-3-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 3 Solutions",
    "text": "Task 3 Solutions\n\nTask 3 Solutions: Look up np.ptp() function and apply it to the data.\n\nAnswer: The np.ptp() function computes the range of values (maximum - minimum) along an axis. PTP stands for “Peak To Peak” - the difference between the maximum peak and minimum peak values.\n\n# Solution to Task 3\n\n# Apply np.ptp() to x_list and x_array from Task 2\nprint(\"Range of x_list using np.ptp():\", np.ptp(x_list))\nprint(\"Range of x_array using np.ptp():\", np.ptp(x_array))\n\n# Manual verification: max - min\nprint(\"Manual calculation: max - min =\", max(x_list) - min(x_list))\n\n# Both should give the same result: 3 - 1 = 2\n\nRange of x_list using np.ptp(): 2\nRange of x_array using np.ptp(): 2\nManual calculation: max - min = 2"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-4-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-4-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 4 Solutions",
    "text": "Task 4 Solutions\n\nTask 4 Solutions: Compute standard deviation by hand and compare with np.std() function.\n\n\n# Solution to Task 4\n\nx_list = [1, 2, 3]  # From Task 2\n\n# Part (a): Calculate sample standard deviation by hand (using n-1)\n# Mean = (1 + 2 + 3) / 3 = 2\nmean_x = 2\n\n# Sample variance = [(1-2)² + (2-2)² + (3-2)²] / (3-1)\n# = [1 + 0 + 1] / 2 = 2/2 = 1\nsample_variance = ((1-2)**2 + (2-2)**2 + (3-2)**2) / (3-1)\nsample_std = np.sqrt(sample_variance)\n\nprint(\"Sample standard deviation (by hand):\", sample_std)\nprint(\"Sample standard deviation (by hand):\", np.sqrt(1))  # Should be 1.0\n\n# Part (b): Compare with np.std(x_list)\nprint(\"np.std(x_list) default:\", np.std(x_list))\nprint(\"Does this match part (a)?\", np.isclose(sample_std, np.std(x_list)))\n\n# Part (c): Calculate population standard deviation by hand (using n)\n# Population variance = [(1-2)² + (2-2)² + (3-2)²] / 3\n# = [1 + 0 + 1] / 3 = 2/3\npopulation_variance = ((1-2)**2 + (2-2)**2 + (3-2)**2) / 3\npopulation_std = np.sqrt(population_variance)\n\nprint(\"Population standard deviation (by hand):\", population_std)\nprint(\"This matches np.std(x_list):\", np.isclose(population_std, np.std(x_list)))\n\n# Part (d): Use ddof=1 to get sample standard deviation\nprint(\"np.std(x_list, ddof=1):\", np.std(x_list, ddof=1))\nprint(\"This matches part (a):\", np.isclose(sample_std, np.std(x_list, ddof=1)))\n\nSample standard deviation (by hand): 1.0\nSample standard deviation (by hand): 1.0\nnp.std(x_list) default: 0.816496580928\nDoes this match part (a)? False\nPopulation standard deviation (by hand): 0.816496580928\nThis matches np.std(x_list): True\nnp.std(x_list, ddof=1): 1.0\nThis matches part (a): True"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#optional-task-solutions",
    "href": "files/labs/lab3/lab3_sln.html#optional-task-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Optional Task Solutions",
    "text": "Optional Task Solutions\n\nOptional Task Solutions: Create a custom IQR function.\n\n\n# Solution to Optional Task\n\ndef calculate_iqr(data):\n    \"\"\"\n    Calculate the Interquartile Range (IQR) of a dataset.\n    \n    Parameter:\n    data - a list or array of numbers\n    \n    Returns:\n    The IQR value (Q3 - Q1)\n    \"\"\"\n    # Calculate the IQR using the numpy method we learned\n    iqr_value = np.diff(np.percentile(data, [25, 75]))[0]\n    return iqr_value\n\n# Test the function\ntest_scores = [72, 85, 90, 78, 92, 88, 76, 94, 82, 89, 91, 77]\n\n# Use our custom function\nmy_iqr = calculate_iqr(test_scores)\nprint(f\"IQR using our function: {my_iqr}\")\n\n# Compare with the direct method\ndirect_iqr = np.diff(np.percentile(test_scores, [25, 75]))[0]\nprint(f\"IQR using direct method: {direct_iqr}\")\n\n# They should be the same!\nprint(f\"Results match: {np.isclose(my_iqr, direct_iqr)}\")\n\nIQR using our function: 12.5\nIQR using direct method: 12.5\nResults match: True"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-5-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-5-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 5 Solutions",
    "text": "Task 5 Solutions\n\nTask 5 Solutions: Create boxplots with various customizations.\n\n\n# Solution to Task 5\n\n# Import matplotlib for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Step 1: Create the list y\ny = [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2]\n\n# Step 2: Create basic vertical boxplot\nplt.figure(figsize=(8, 6))\nplt.subplot(2, 2, 1)\nplt.boxplot(y)\nplt.title(\"Basic Vertical Boxplot\")\n\n# Step 3: Create horizontal boxplot\nplt.subplot(2, 2, 2)\nplt.boxplot(y, orientation='horizontal')\nplt.title(\"Horizontal Boxplot\")\n\n# Step 4: Add color\nplt.subplot(2, 2, 3)\nplt.boxplot(y, orientation='horizontal', patch_artist=True, \n            boxprops=dict(facecolor=\"aquamarine\"))\nplt.title(\"Colored Horizontal Boxplot\")\n\n# Step 5: Final version with title\nplt.subplot(2, 2, 4)\nplt.boxplot(y, orientation='horizontal', patch_artist=True, \n            boxprops=dict(facecolor=\"aquamarine\"))\nplt.title(\"My First Python Boxplot\")\n\nplt.tight_layout()\nplt.show()\n\n# Answer the IQR question\nprint(\"\\nBased on the boxplot:\")\nprint(\"Q1 (25th percentile) appears to be around 2\")\nprint(\"Q3 (75th percentile) appears to be around 4.5\")\nprint(\"So IQR ≈ 4.5 - 2 = 2.5\")\n\n# Verify with Python calculation\niqr_calculated = np.diff(np.percentile(y, [25, 75]))[0]\nprint(f\"\\nActual IQR calculated by Python: {iqr_calculated}\")\n\n\n\n\n\n\n\n\n\nBased on the boxplot:\nQ1 (25th percentile) appears to be around 2\nQ3 (75th percentile) appears to be around 4.5\nSo IQR ≈ 4.5 - 2 = 2.5\n\nActual IQR calculated by Python: 2.0\n\n\n\n# Solution to Task 5\n\n# Import matplotlib for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# Step 1: Create the list y\ny = [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2]\n\n# Step 2: Create basic vertical boxplot\nplt.figure(figsize=(8, 6))\nplt.subplot(2, 2, 1)\nplt.boxplot(y)\nplt.title(\"Basic Vertical Boxplot\")\n\n# Step 3: Create horizontal boxplot\nplt.subplot(2, 2, 2)\nplt.boxplot(y, orientation='horizontal')\nplt.title(\"Horizontal Boxplot\")\n\n# Step 4: Add color\nplt.subplot(2, 2, 3)\nplt.boxplot(y, orientation='horizontal', patch_artist=True, \n            boxprops=dict(facecolor=\"aquamarine\"))\nplt.title(\"Colored Horizontal Boxplot\")\n\n# Step 5: Final version with title\nplt.subplot(2, 2, 4)\nplt.boxplot(y, orientation='horizontal', patch_artist=True, \n            boxprops=dict(facecolor=\"aquamarine\"))\nplt.title(\"My First Python Boxplot\")\n\nplt.tight_layout()\nplt.show()\n\n# Answer the IQR question\nprint(\"\\nBased on the boxplot:\")\nprint(\"Q1 (25th percentile) appears to be around 2\")\nprint(\"Q3 (75th percentile) appears to be around 4.5\")\nprint(\"So IQR ≈ 4.5 - 2 = 2.5\")\n\n# Verify with Python calculation\niqr_calculated = np.diff(np.percentile(y, [25, 75]))[0]\nprint(f\"\\nActual IQR calculated by Python: {iqr_calculated}\")\n\n\n\n\n\n\n\n\n\nBased on the boxplot:\nQ1 (25th percentile) appears to be around 2\nQ3 (75th percentile) appears to be around 4.5\nSo IQR ≈ 4.5 - 2 = 2.5\n\nActual IQR calculated by Python: 2.0"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-6-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-6-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 6 Solutions",
    "text": "Task 6 Solutions\n\nTask 6 Solutions: Create a histogram with appropriate bins and labels.\n\n\n# Solution to Task 6\n\ny = [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2]  # From Task 5\n\n# Create histogram with appropriate bins\nplt.figure(figsize=(10, 6))\n\n# Method 1: Using explicit bin edges\nplt.subplot(1, 2, 1)\nplt.hist(y, bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], rwidth=0.9, edgecolor='black')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Histogram of y values (Method 1)')\n\n# Method 2: Using range and bins parameters\nplt.subplot(1, 2, 2)\nplt.hist(y, bins=5, range=(0.5, 5.5), rwidth=0.9, edgecolor='black')\nplt.xlabel('Values')\nplt.ylabel('Frequency')\nplt.title('Histogram of y values (Method 2)')\n\nplt.tight_layout()\nplt.show()\n\n# Show frequency count for verification\nunique, counts = np.unique(y, return_counts=True)\nprint(\"Value frequencies:\")\nfor value, count in zip(unique, counts):\n    print(f\"Value {value}: appears {count} times\")\n\n\n\n\n\n\n\n\nValue frequencies:\nValue 1: appears 2 times\nValue 2: appears 2 times\nValue 3: appears 2 times\nValue 4: appears 3 times\nValue 5: appears 2 times"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-7-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-7-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 7 Solutions",
    "text": "Task 7 Solutions\n\nTask 7 Solutions: Create a scatterplot with proper labels.\n\n\n# Solution to Task 7\n\n# Part 1: Copy and run the provided code\nnp.random.seed(5)\n\nx1 = np.random.normal(0, 1, 100)\nx2 = x1 + np.random.normal(0, 1, 100)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x1, x2)\n\n# Part 2: Add labels and title\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('My First Python Scatterplot')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Additional information about the plot\nprint(f\"Number of points plotted: {len(x1)}\")\nprint(f\"x1 range: {x1.min():.2f} to {x1.max():.2f}\")\nprint(f\"x2 range: {x2.min():.2f} to {x2.max():.2f}\")\n\n\n\n\n\n\n\n\nNumber of points plotted: 100\nx1 range: -2.86 to 2.43\nx2 range: -4.00 to 4.10"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#task-8-solutions",
    "href": "files/labs/lab3/lab3_sln.html#task-8-solutions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Task 8 Solutions",
    "text": "Task 8 Solutions\n\nTask 8 Solutions: Plot the function f(x) = x - x²sin(x) between x = -10 and x = 10.\n\n\n# Solution to Task 8\n\n# Create x values using linspace for a smooth plot\nx = np.linspace(-10, 10, 1000)  # 1000 points for smoothness\n\n# Define the function f(x) = x - x²sin(x)\ny = x - x**2 * np.sin(x)\n\n# Create the plot with red color\nplt.figure(figsize=(12, 8))\nplt.plot(x, y, color='red', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('Plot of f(x) = x - x²sin(x)')\nplt.grid(True, alpha=0.3)  # Add a light grid for better readability\nplt.show()\n\n# Show what happens with fewer points for comparison\nplt.figure(figsize=(12, 4))\n\n# Fewer points - not smooth\nplt.subplot(1, 2, 1)\nx_few = np.linspace(-10, 10, 20)\ny_few = x_few - x_few**2 * np.sin(x_few)\nplt.plot(x_few, y_few, color='blue', linewidth=2, marker='o')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('With 20 points (not smooth)')\nplt.grid(True, alpha=0.3)\n\n# Many points - smooth\nplt.subplot(1, 2, 2)\nplt.plot(x, y, color='red', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.title('With 1000 points (smooth)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Notice how more points create a smoother curve!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how more points create a smoother curve!"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#summary-of-key-learning-points",
    "href": "files/labs/lab3/lab3_sln.html#summary-of-key-learning-points",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Summary of Key Learning Points",
    "text": "Summary of Key Learning Points\n\nKey Functions Learned:\n\nnp.mean() - Calculate mean\nnp.median() - Calculate median\nnp.std() - Calculate standard deviation (use ddof=1 for sample std)\nnp.ptp() - Calculate range (peak-to-peak)\nnp.percentile() - Calculate percentiles\nnp.diff(np.percentile(data, [25,75]))[0] - Calculate IQR"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#key-plotting-functions",
    "href": "files/labs/lab3/lab3_sln.html#key-plotting-functions",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Key Plotting Functions:",
    "text": "Key Plotting Functions:\n\nplt.boxplot() - Create boxplots\nplt.hist() - Create histograms\nplt.scatter() - Create scatterplots\nplt.plot() - Create line plots\nplt.xlabel(), plt.ylabel(), plt.title() - Add labels"
  },
  {
    "objectID": "files/labs/lab3/lab3_sln.html#important-concepts",
    "href": "files/labs/lab3/lab3_sln.html#important-concepts",
    "title": "Lab 3: Descriptive Statistics - SOLUTIONS",
    "section": "Important Concepts:",
    "text": "Important Concepts:\n\nSample vs Population Standard Deviation: Use ddof=1 for sample statistics\nFunction Parameters: Many plotting functions accept optional parameters for customization\nSmooth Plotting: Use more points in np.linspace() for smoother function plots\nModule Importing: import module as abbreviation saves typing\n\nThis concludes Lab 3 Solutions!"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-1-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-1-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 1 Solution",
    "text": "Task 1 Solution\n\nObjective: Rename your notebook from Untitled to Lab1\nSteps:\n\nLocated the notebook in the file browser (appears as Untitled.ipynb)\nRight-clicked on the notebook name\nSelected “Rename” from the context menu\nChanged the name to Lab1\nPressed Enter to confirm\nVerified the title bar now shows Lab1.ipynb\n\nExpected Result:\nYour notebook should now be named Lab1.ipynb and this should be visible in both the file browser and the title bar."
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-2-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-2-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 2 Solution",
    "text": "Task 2 Solution\n\nObjective: Create a Markdown cell with heading and a Code cell with basic arithmetic\nMarkdown Cell:\n# Task 2\nCode Cell:\n\n2 + 2\n\n4\n\n\nExpected Result:\n\nThe code cell executes the arithmetic operation\nPython displays the result 4 below the cell\nA new empty code cell automatically appears below\nThe cell is marked as executed with a number like [1]"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-3-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-3-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 3 Solution",
    "text": "Task 3 Solution\n\nObjective: Demonstrate syntax error and understand error messages\nMarkdown Cell:\n# Task 3\nCode Cell with Intentional Error:\n\n2 plus 2\n\nExpected Error Output:\n  Cell In[2], line 1\n    2 plus 2\n      ^^^^\nSyntaxError: invalid syntax\nExplanation:\n\nPython doesn’t understand plus as an operator\nThe ^^^^ points to where Python detected the problem\nThe error message tells us it’s a SyntaxError meaning invalid Python syntax\nIn Python, we must use + for addition, not the word plus\n\nCorrected Version:\n\n2 + 2  # This works correctly\n\n4"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-4-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-4-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 4 Solution",
    "text": "Task 4 Solution\n\nObjective: Compute mathematical expressions using Python**\nProblem 1: \\(\\frac{2 + 3}{4 + 5^6}\\)\nPython Code:\n\n(2 + 3) / (4 + 5**6)\n\n0.0003199181009661527\n\n\nStep by Step:\n\nnumerator = 2 + 3\nprint(f\"Numerator: {numerator}\")\n\ndenominator = 4 + 5**6\nprint(f\"Denominator: {denominator}\")\n\nresult = numerator / denominator\nprint(f\"Final result: {result}\")\n\nNumerator: 5\nDenominator: 15629\nFinal result: 0.0003199181009661527\n\n\nProblem 2: \\((1 - 3 \\cdot 4^5)^6\\)\nPython Code:\n\n(1 - 3 * 4**5)**6\n\n838839550121163601921\n\n\nStep by Step:\n\ninner_exponent = 4**5\nprint(f\"4^5 = {inner_exponent}\")\n\nmultiplication = 3 * inner_exponent\nprint(f\"3 * 4^5 = {multiplication}\")\n\nsubtraction = 1 - multiplication\nprint(f\"1 - 3 * 4^5 = {subtraction}\")\n\nfinal_result = subtraction**6\nprint(f\"(1 - 3 * 4^5)^6 = {final_result}\")\n\n4^5 = 1024\n3 * 4^5 = 3072\n1 - 3 * 4^5 = -3071\n(1 - 3 * 4^5)^6 = 838839550121163601921"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-5-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-5-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 5 Solution",
    "text": "Task 5 Solution\n\nObjective: Understand module importing and fix NameError\nStep 1: Code that produces error\n\nsin(1)\n\nExpected Error:\nNameError: name 'sin' is not defined\nExplanation: Python doesn’t recognize sin because the math functions aren’t loaded by default.\nStep 2: Import module and retry\n\nfrom math import *\nsin(1)\n\n0.8414709848078965\n\n\nAlternative Solutions:\n\n# Method 1: Import specific function\nfrom math import sin\nprint(sin(1))\n\n# Method 2: Import entire module\nimport math\nprint(math.sin(1))\n\n# Method 3: Import with alias\nimport math as m\nprint(m.sin(1))\n\n0.8414709848078965\n0.8414709848078965\n0.8414709848078965\n\n\nAll produce the same result: 0.8414709848078965"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-6-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-6-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 6 Solution",
    "text": "Task 6 Solution\n\nObjective: Understand Python case sensitivity\nStep 1: Variable assignment\n\nmy_variable = 5\n\nStep 2: Wrong capitalization\n\nprint(My_variable)\n\nExpected Error:\nNameError: name 'My_variable' is not defined\nExplanation: Python is case-sensitive, so My_variable ≠ my_variable\nStep 3: Correct capitalization\n\nprint(my_variable)\n\n5\n\n\nAdditional Examples:\n\n# These are all different variables in Python\nmy_variable = 5\nMy_variable = 10\nMY_VARIABLE = 15\nmy_Variable = 20\n\nprint(f\"my_variable = {my_variable}\")\nprint(f\"My_variable = {My_variable}\")\nprint(f\"MY_VARIABLE = {MY_VARIABLE}\")\nprint(f\"my_Variable = {my_Variable}\")\n\nmy_variable = 5\nMy_variable = 10\nMY_VARIABLE = 15\nmy_Variable = 20"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-7-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-7-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 7 Solution",
    "text": "Task 7 Solution\n\nObjective: Add descriptive comments to previous code\nExamples of well-commented code from previous tasks:\n\n# Task 2: Basic arithmetic\n2 + 2  # Adding two integers\n\n# Task 4: Complex mathematical expression\n# Calculate (2 + 3) / (4 + 5^6)\nnumerator = 2 + 3  # Sum of 2 and 3\ndenominator = 4 + 5**6  # 4 plus 5 to the 6th power\nresult = numerator / denominator  # Final division\nprint(f\"Result: {result}\")\n\n# Task 5: Import math module and use sin function\nfrom math import *  # Import all math functions\nangle_in_radians = 1  # Input angle in radians\nsine_value = sin(angle_in_radians)  # Calculate sine\nprint(f\"sin(1) = {sine_value}\")\n\n# Task 6: Variable assignment with proper naming\nmy_variable = 5  # Store the value 5 in my_variable\nprint(my_variable)  # Display the value\n\n\"\"\"\nThis is a multi-line comment.\nIt can span multiple lines and is useful\nfor longer explanations or documentation.\n\"\"\"\n\nGood commenting practices demonstrated:\n\nExplain what the code does\nClarify complex calculations\nDocument variable purposes\nUse both inline (#) and block (\"\"\") comments"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-8-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-8-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 8 Solution",
    "text": "Task 8 Solution\n\nObjective: Explore Python data types using the type() function\nCode and Expected Outputs:\n\nprint(type(1))           # Output: &lt;class 'int'&gt;\nprint(type(1.1))         # Output: &lt;class 'float'&gt;\nprint(type(\"hello\"))     # Output: &lt;class 'str'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\nAdditional Examples:\n\n# More data type examples\nprint(\"Integer:\", type(42))\nprint(\"Float:\", type(3.14159))\nprint(\"String with single quotes:\", type('Python'))\nprint(\"String with double quotes:\", type(\"Programming\"))\nprint(\"Boolean True:\", type(True))\nprint(\"Boolean False:\", type(False))\nprint(\"List:\", type([1, 2, 3]))\nprint(\"Tuple:\", type((1, 2, 3)))\nprint(\"Dictionary:\", type({\"key\": \"value\"}))\n\nInteger: &lt;class 'int'&gt;\nFloat: &lt;class 'float'&gt;\nString with single quotes: &lt;class 'str'&gt;\nString with double quotes: &lt;class 'str'&gt;\nBoolean True: &lt;class 'bool'&gt;\nBoolean False: &lt;class 'bool'&gt;\nList: &lt;class 'list'&gt;\nTuple: &lt;class 'tuple'&gt;\nDictionary: &lt;class 'dict'&gt;"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#task-9-solution",
    "href": "files/labs/lab1/lab1_sln.html#task-9-solution",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Task 9 Solution",
    "text": "Task 9 Solution\n\nObjective: Practice variable assignment, updating, and calculations\nMarkdown cell:\n# Task 9\nStep 2: Initial variable assignments\n\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\n\nStep 3: Update num_sections (correct approach)\n\nnum_sections = num_sections + 1\nprint(f\"Updated number of sections: {num_sections}\")\n# Alternative: num_sections += 1\n# Alternative: num_sections = 4 + 1\n\nUpdated number of sections: 5\n\n\nStep 4: Predict and test expressions\n\nprint(type(course))           # Expected: &lt;class 'str'&gt;\nprint(type(num_sections))     # Expected: &lt;class 'int'&gt;\nprint(num_sections * section_capacity) # Expected: 125\n\n&lt;class 'str'&gt;\n&lt;class 'int'&gt;\n125\n\n\nStep 5: Calculate course capacity\n\ncourse_capacity = num_sections * section_capacity\nprint(f\"Course: {course}\")\nprint(f\"Number of sections: {num_sections}\")\nprint(f\"Capacity per section: {section_capacity}\")\nprint(f\"Total course capacity: {course_capacity}\")\n\nCourse: PSTAT 5A\nNumber of sections: 5\nCapacity per section: 25\nTotal course capacity: 125\n\n\nComplete Solution with Comments:\n\n# Step 2: Initial variable assignments\ncourse = \"PSTAT 5A\"          # Course name as string\nnum_sections = 4             # Initial number of sections\nsection_capacity = 25        # Maximum students per section\n\n# Step 3: A new section has been added\nnum_sections = num_sections + 1  # Increment by 1, now equals 5\n\n# Step 4: Testing expressions with predictions\nprint(\"Testing type() function:\")\nprint(f\"type(course) = {type(course)}\")  # Expected: &lt;class 'str'&gt;\nprint(f\"type(num_sections) = {type(num_sections)}\")  # Expected: &lt;class 'int'&gt;\nprint(f\"num_sections * section_capacity = {num_sections * section_capacity}\")  # Expected: 125\n\n# Step 5: Calculate total course capacity\ncourse_capacity = num_sections * section_capacity  # 5 × 25 = 125\nprint(f\"\\nFinal Results:\")\nprint(f\"Course: {course}\")\nprint(f\"Total sections: {num_sections}\")\nprint(f\"Capacity per section: {section_capacity}\")\nprint(f\"Total course capacity: {course_capacity} students\")\n\nTesting type() function:\ntype(course) = &lt;class 'str'&gt;\ntype(num_sections) = &lt;class 'int'&gt;\nnum_sections * section_capacity = 125\n\nFinal Results:\nCourse: PSTAT 5A\nTotal sections: 5\nCapacity per section: 25\nTotal course capacity: 125 students"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#summary-of-key-concepts-learned",
    "href": "files/labs/lab1/lab1_sln.html#summary-of-key-concepts-learned",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Summary of Key Concepts Learned",
    "text": "Summary of Key Concepts Learned\n✅ JupyterHub Environment\n\nCreating and renaming notebooks\nUnderstanding cell types (Code vs Markdown)\nRunning cells with ▶️ button or Shift+Enter\nNavigating the interface\n\n✅ Python Basics\n\nArithmetic operations: +, -, *, /, **\nOrder of operations: Parentheses, Exponents, Multiplication/Division, Addition/Subtraction\nError reading: Understanding SyntaxError and NameError messages\n\n✅ Variables and Data Types\n\nVariable assignment: variable_name = value\nCase sensitivity: my_var ≠ My_var\nBasic types: int, float, str, bool\nType checking: type() function\n\n✅ Modules and Imports\n\nImport syntax: from module import * or import module\nUsing functions: After importing, functions become available\nMath module: Contains mathematical functions like sin(), cos(), etc.\n\n✅ Comments and Documentation\n\nInline comments: # This is a comment\nBlock comments: \"\"\"Multi-line comment\"\"\"\nPurpose: Document code for yourself and others\n\n✅ Programming Best Practices\n\nWrite descriptive variable names\nAdd comments to explain complex logic\nTest your code incrementally\nRead and understand error messages\nUse existing variables in calculations when possible"
  },
  {
    "objectID": "files/labs/lab1/lab1_sln.html#next-steps",
    "href": "files/labs/lab1/lab1_sln.html#next-steps",
    "title": "PSTAT 5A Lab 1 - SOLUTIONS",
    "section": "Next Steps",
    "text": "Next Steps\nIn Lab 2, you’ll learn about:\n\nPython functions and how to create them\nData structures (lists, dictionaries)\nControl flow (if statements, loops)\nMore advanced programming concepts\n\nGreat work completing Lab 1! You now have the foundation needed for statistical programming in Python."
  },
  {
    "objectID": "files/labs/lab1/lab1.html",
    "href": "files/labs/lab1/lab1.html",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Welcome to the first PSTAT 5A Lab! As we will soon learn, computers play an integral part in effectively and efficiently performing statistical analyses. The primary goal of these Labs is to develop the skills to communicate with computers and learn the basic principles and language of programming.\nThis first lab will introduce you to the JupyterHub environment, Python as a programming language, and some basic concepts of programming. You will also complete a series of tasks to familiarize yourself with the tools and concepts we will use throughout the course.\nThis lab is designed to be completed during your first lab section of the week, and it will set the foundation for the rest of the course. Make sure to read through all the material carefully, as it will be essential for your success in PSTAT 5A.\n\n\n\nEvery week we (the course staff) will publish a lab document, which is intended to be completed during your Lab Section (i.e., your first Section) of the week. Each lab document will consist of a combination of text, tips, and the occasional task for you to complete based on the text provided. Your TA will cover exactly what you need to turn in at the end of each lab in order to receive credit, but you should read all lab material carefully and thoroughly as content from labs will appear on quizzes and exams.\n\n\n\n\nComputers, though incredibly useful, are fairly complex machines. To communicate with them, we need to use a specific language, known as a programming language. There are a number of programming languages currently in use—R, Julia, MatLab, and the language we will use for this course, Python.\nPython programs can be written in many environments (e.g., text editors like VS Code or in a Terminal window). For this class we will use Jupyter Notebook (pronounced “Jew-pi-ter”), an interactive environment that’s hosted online so you don’t have to install anything to run Python code!\n\n\n\n\n\nNavigate to (https://pstat5a.lsit.ucsb.edu)\n\nIf you are using a personal computer, you may want to bookmark this page for easy access later.\n\nClick Sign in with your UCSB NetID, and sign in.\n\nNavigate to the Labs folder on the left-hand side of the JupyterHub interface. \nUnder Notebook, click Python 3 (ipykernel).\n\n\nCongratulations, you have just made your first Jupyter notebook! Now, it’s time for our first task:\n\n\n\n\n\nFind your new notebook in the left-hand file browser (it will be named Untitled or Untitled1 by default).\n\nRight-click the notebook and select → Rename.\n\nRename it to Lab1 and hit Enter.\n\nWatch the title bar update to Lab1.ipynb.\n\n\n\n\n\nJupyter notebooks are built from cells—the shaded boxes you see on screen. Here’s how to work with them:\n\n\n\nInactive cell\n\nAppearance: light grey background\n\nAction: click anywhere inside the cell to activate\n\n\nActive cell\n\nAppearance: colored border (green or blue)\n\nYou can now type code or Markdown here.\n\n\n\nTip: Only the active cell runs when you press Run.\n\n\n\n\n\nClick the ▶️ Run button in the toolbar\n\nOr press Shift + Enter to run and advance to the next cell\n\n\n\n\n\n\n\nYou can switch any cell between Code and Markdown:\n\n\n\nPurpose: write and execute Python code\n\nSelect:\n\nClick the cell\n\nChoose Code from the toolbar dropdown\n\n\n\nRun: ▶️ Run button or Shift + Enter\n\n\n\n\n\nPurpose: write formatted text, headings, lists, math, and embed images\n\nSelect:\n\nClick the cell\n\nChoose Markdown from the toolbar dropdown\n\n\nRender: ▶️ Run button or Shift + Enter\n\n\n\n\n\n\nClick into the initial cell ( marked by [ ] on the left).\n\nIn the toolbar dropdown (that currently says Code), select Markdown.\n\nCopy-paste the following (including the #):\n# Task 2\nRun the cell (▶️).\nCreate a new code cell by clicking the + button in the toolbar.\n\n\nAlternatively, you can press B to add a cell below the current one or A to add one above it.\n\nThis option preserves the previous cell type (Code or Markdown).\n\nYou can also right-click the cell and select Insert Cell Below or Insert Cell Above.\nYou can also use the Insert menu at the top of the page. &gt; Tip: Press Shift + Enter to run a cell and move to (or create) the next one.\n\nEnter and run:\n2 + 2\nObserve that a new cell appears under it automatically.\n\n\nTip: Press Shift + Enter to run a cell and move to (or create) the next one.\n\n\n\n\n\nCreate a new Markdown cell labeled:\n# Task 3\nCreate a new code cell and run:\n2 plus 2\nObserve the SyntaxError and note how Python points to the problem.\n\n\nNote: Always read error messages, they tell you what went wrong!\n\n\n\n\n\nPython follows the usual order of operations:\n\nParentheses\n\nExponents\n\nMultiplication / Division\n\nAddition / Subtraction\n\n\n\n\nOperation\nPython Syntax\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1.0\n\n\nExponentiation\n**\n2 ** 2\n4\n\n\n\n\n\n\nCompute the following in separate code cells:\n\n\\[\\frac{2 + 3}{4 + 5^6}\\]\n\\[(1 - 3 \\cdot 4^5)^6\\]\n\n\n\n\n\nIn Python, a module is simply a file (with a .py extension) that contains related code, functions, classes, and variables—that you can reuse in other programs. Modules help you organize your code, avoid naming conflicts, and leverage functionality written by others.\n\n\n\nReusability: Write a function once, then import it wherever you need it.\n\nOrganization: Group related functionality into logical units (e.g., math operations).\n\nNamespace Management: Keep your global namespace clean by accessing code through the module’s name.\n\n\n\n\nThere are several ways to bring module code into your current script or notebook:\n\nImport the entire module\nimport math\nprint(math.sin(1))\nImport specific names\nfrom math import sin, pi\nprint(sin(pi/2))\nImport with an alias\nimport numpy as np\narr = np.array([1, 2, 3])\n\n\nTip: Use specific imports (from module import name) to keep your namespace tidy, or aliases (import module as m) for brevity.\n\n\n\n\n\n\nStandard library: Modules like math, random, and datetime come with Python.\n\nThird-party: Install via pip install package_name (e.g. pip install pandas).\n\nYour own: Create my_utils.py and then import my_utils in your project.\n\nModules are the building blocks of larger Python applications; get comfortable importing and exploring them!\n\n\n\n\n\n\nIn a code cell, type:\nsin(1)\nObserve the NameError.\n\nIn the same (or a new) cell, load the module and retry:\nfrom math import *\nsin(1)\n\n\n\n\n\nVariables in Python are used to store data values. You can think of them as labels for data that you want to use later in your program.\n\nAssignment:\nx = 2\nPrinting:\nprint(x)\n\nPython is case-sensitive: my_variable ≠ My_variable.\n\nBehind the scenes, print() is a function that takes one or more values and displays them on the screen. We’ll learn what functions are and how to create our own functions soon.\n\n\n\n\n\n\nAssign:\nmy_variable = 5\nIn a new cell, run:\nprint(My_variable)\n– observe the NameError due to wrong capitalization.\nNameError: name 'My_variable' is not defined\nIn the same cell, run:\nprint(my_variable)\nNow you should see 5 printed without any errors.\n\n\n\n\n\nComments are notes in your code that Python ignores when running the program. They help you and others understand what your code does. Comments are essential for documenting your code, explaining complex logic, or leaving reminders for yourself or others. They do not affect the execution of your program.\nYou can add comments anywhere in your code, and they can be on their own line or at the end of a line of code.\nIn Python, comments start with a # symbol. Everything after the # on that line is considered a comment and will not be executed by Python. You can also use multi-line comments with triple quotes (\"\"\" or '''), which allows you to write longer explanations or block comments that span multiple lines. These are often used for documentation strings (docstrings) to describe functions, classes, or modules.\nYou can add comments in two ways:\n\nInline comment: Use # to comment out a single line.\nExample:\n# This is an inline comment\nx = 5  # Assign 5 to x\nBlock comment: Use triple quotes \"\"\" or ''' to comment out multiple lines.\n\nExample:\n\"\"\"\nMultiple lines\nof comment here\n\"\"\"\n\n\n\n\n\nGo back and add descriptive comments to some of your previous code cells.\n\n\n\n\nPython has several basic data types, which are the building blocks for more complex data structures. The most common ones are: - bool — boolean (e.g. True, False)\n\nNoneType — represents the absence of a value (e.g. None)\nlist — ordered collection (e.g. [1, 2, 3])\ntuple — immutable ordered collection (e.g. (1, 2, 3))\ndict — key-value pairs (e.g. `{“key”: “value”})\nset — unordered collection of unique items (e.g. {1, 2, 3})\n\nThe most basic data types you will use in this course are:\n\nint — integer (e.g. 1, 42)\nfloat — real number (e.g. 1.0, 3.14)\n\nstr — string/text (e.g. \"hello\", 'abc')\n\n\n\n\n\nRun each in its own cell:\ntype(1)\ntype(1.1)\ntype(\"hello\")\n\n\n\n\nYou can assign values to variables and use them in expressions. Here’s an example:\n\n\n\n\n\nCreate a new Markdown cell labeled:\n# Task 9\nIn a new code cell, perform the following variable assignments:\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\nA new section has been added! Update the variable num_sections to be one more than when you initially defined it above. (Don’t just use num_sections = 5- think about our discussion on updating variables above!)\nUsing comments, write down what you think the output of each of the following expressions will be:\ntype(course)\ntype(num_sections)\nnum_sections * section_capacity\nThen, run each expression in a separate code chunk and comment on the results.\nCreate a new variable called course_capacity and assign it the value of the maximum capacity of the course. (Hint: there are only 5 sections, and each section has a maximum capacity of 25. Try to use your already-defined variables as much as possible!)\n\n\n\n\n\nThat wraps up Lab 1! You’ve successfully navigated the JupyterHub environment, learned how to switch between and run Code and Markdown cells, experimented with basic Python expressions, and practiced variable assignment. In Lab 2, we’ll dive deeper into Python functions, data structures, and more advanced programming concepts. Great work, see you next time!\n\n\n\n\n\n\n🔖 Table of Contents\n- PSTAT 5A Lab 1  - Structure of Labs  - What Is Programming?  - Getting Started  - Task 1  - The JupyterHub Environment  - 1. Cell Activation  - 2. Running Cells  - Cell Types  - Code Cells  - Markdown Cells  - Task 2  - Task 3  - Python as a Calculator  - Task 4  - Python Modules  - Why Use Modules?  - Importing Modules  - Finding and Installing Modules  - Task 5  - Variable Assignment  - Task 6  - Comments  - Task 7  - Basic Data Types  - Task 8  - Using Variables and Data Types  - Task 9  - Conclusion"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#structure-of-labs",
    "href": "files/labs/lab1/lab1.html#structure-of-labs",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Every week we (the course staff) will publish a lab document, which is intended to be completed during your Lab Section (i.e., your first Section) of the week. Each lab document will consist of a combination of text, tips, and the occasional task for you to complete based on the text provided. Your TA will cover exactly what you need to turn in at the end of each lab in order to receive credit, but you should read all lab material carefully and thoroughly as content from labs will appear on quizzes and exams."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#what-is-programming",
    "href": "files/labs/lab1/lab1.html#what-is-programming",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Computers, though incredibly useful, are fairly complex machines. To communicate with them, we need to use a specific language, known as a programming language. There are a number of programming languages currently in use—R, Julia, MatLab, and the language we will use for this course, Python.\nPython programs can be written in many environments (e.g., text editors like VS Code or in a Terminal window). For this class we will use Jupyter Notebook (pronounced “Jew-pi-ter”), an interactive environment that’s hosted online so you don’t have to install anything to run Python code!"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#getting-started",
    "href": "files/labs/lab1/lab1.html#getting-started",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Navigate to (https://pstat5a.lsit.ucsb.edu)\n\nIf you are using a personal computer, you may want to bookmark this page for easy access later.\n\nClick Sign in with your UCSB NetID, and sign in.\n\nNavigate to the Labs folder on the left-hand side of the JupyterHub interface. \nUnder Notebook, click Python 3 (ipykernel).\n\n\nCongratulations, you have just made your first Jupyter notebook! Now, it’s time for our first task:"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-1",
    "href": "files/labs/lab1/lab1.html#task-1",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Find your new notebook in the left-hand file browser (it will be named Untitled or Untitled1 by default).\n\nRight-click the notebook and select → Rename.\n\nRename it to Lab1 and hit Enter.\n\nWatch the title bar update to Lab1.ipynb."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#the-jupyterhub-environment",
    "href": "files/labs/lab1/lab1.html#the-jupyterhub-environment",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Jupyter notebooks are built from cells—the shaded boxes you see on screen. Here’s how to work with them:\n\n\n\nInactive cell\n\nAppearance: light grey background\n\nAction: click anywhere inside the cell to activate\n\n\nActive cell\n\nAppearance: colored border (green or blue)\n\nYou can now type code or Markdown here.\n\n\n\nTip: Only the active cell runs when you press Run.\n\n\n\n\n\nClick the ▶️ Run button in the toolbar\n\nOr press Shift + Enter to run and advance to the next cell"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#cell-types",
    "href": "files/labs/lab1/lab1.html#cell-types",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "You can switch any cell between Code and Markdown:\n\n\n\nPurpose: write and execute Python code\n\nSelect:\n\nClick the cell\n\nChoose Code from the toolbar dropdown\n\n\n\nRun: ▶️ Run button or Shift + Enter\n\n\n\n\n\nPurpose: write formatted text, headings, lists, math, and embed images\n\nSelect:\n\nClick the cell\n\nChoose Markdown from the toolbar dropdown\n\n\nRender: ▶️ Run button or Shift + Enter"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-2",
    "href": "files/labs/lab1/lab1.html#task-2",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Click into the initial cell ( marked by [ ] on the left).\n\nIn the toolbar dropdown (that currently says Code), select Markdown.\n\nCopy-paste the following (including the #):\n# Task 2\nRun the cell (▶️).\nCreate a new code cell by clicking the + button in the toolbar.\n\n\nAlternatively, you can press B to add a cell below the current one or A to add one above it.\n\nThis option preserves the previous cell type (Code or Markdown).\n\nYou can also right-click the cell and select Insert Cell Below or Insert Cell Above.\nYou can also use the Insert menu at the top of the page. &gt; Tip: Press Shift + Enter to run a cell and move to (or create) the next one.\n\nEnter and run:\n2 + 2\nObserve that a new cell appears under it automatically.\n\n\nTip: Press Shift + Enter to run a cell and move to (or create) the next one."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-1-rename-your-notebook",
    "href": "files/labs/lab1/lab1.html#task-1-rename-your-notebook",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 1: Rename Your Notebook",
    "text": "Task 1: Rename Your Notebook\n\n\nFind your new notebook in the left-hand file browser (it will be named Untitled or Untitled1 by default).\n\nRight-click the notebook and select → Rename.\n\nRename it to Lab1 and hit Enter.\n\nWatch the title bar update to Lab1.ipynb."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-2-markdown-and-code-cells",
    "href": "files/labs/lab1/lab1.html#task-2-markdown-and-code-cells",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 2: Markdown and Code Cells",
    "text": "Task 2: Markdown and Code Cells\n\n\nClick into the initial cell (marked by [ ] on the left).\n\nIn the toolbar dropdown (that currently says Code), select Markdown.\n\nCopy-paste the following (including the #):\n# Task 2\nRun the cell. You should see a large heading that says “Task 2”.\nAdd a new cell below (use the + button or menu).\nMake sure the new cell is a Code cell.\nEnter the following code and run it:\n2 + 2\n\nExpected Output:\n4"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-3-understanding-errors",
    "href": "files/labs/lab1/lab1.html#task-3-understanding-errors",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 3: Understanding Errors",
    "text": "Task 3: Understanding Errors\n\n\nAdd a new Markdown cell with the heading:\n# Task 3\nAdd a new Code cell and enter the following (intentional error):\n2 plus 2\nRun the cell. You should see an error message like:\n\n  Cell In[2], line 1\n    2 plus 2\n      ^^^^\nSyntaxError: invalid syntax\nExplanation: - Python doesn’t understand plus as an operator - The ^^^^ points to where Python detected the problem - The error message tells us it’s a SyntaxError meaning invalid Python syntax - In Python, we must use + for addition, not the word plus\n\nCorrect the code to:\n2 + 2  # This works correctly\nRun the corrected cell. You should see:\n\n4"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-4-math-in-python",
    "href": "files/labs/lab1/lab1.html#task-4-math-in-python",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 4: Math in Python",
    "text": "Task 4: Math in Python\n\n\nIn a new Code cell, compute the following:\n\\(\\frac{2 + 3}{4 + 5^6}\\)\n(2 + 3) / (4 + 5**6)\nBreak it down step by step:\nnumerator = 2 + 3\nprint(f\"Numerator: {numerator}\")\n\ndenominator = 4 + 5**6\nprint(f\"Denominator: {denominator}\")\n\nresult = numerator / denominator\nprint(f\"Final result: {result}\")\n\nExpected Output:\nNumerator: 5\nDenominator: 15629\nFinal result: 0.00032002048131121975\n\nTry this one as well:\n\\((1 - 3 \\cdot 4^5)^6\\)\n(1 - 3 * 4**5)**6\nStep by step:\ninner_exponent = 4**5\nprint(f\"4^5 = {inner_exponent}\")\n\nmultiplication = 3 * inner_exponent\nprint(f\"3 * 4^5 = {multiplication}\")\n\nsubtraction = 1 - multiplication\nprint(f\"1 - 3 * 4^5 = {subtraction}\")\n\nfinal_result = subtraction**6\nprint(f\"(1 - 3 * 4^5)^6 = {final_result}\")\n\nExpected Output:\n4^5 = 1024\n3 * 4^5 = 3072\n1 - 3 * 4^5 = -3071\n(1 - 3 * 4^5)^6 = 729071973630476174071"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-5-importing-modules",
    "href": "files/labs/lab1/lab1.html#task-5-importing-modules",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 5: Importing Modules",
    "text": "Task 5: Importing Modules\n\n\nIn a new Code cell, try running:\nsin(1)\nYou should see:\nNameError: name 'sin' is not defined\nTo fix this, import the math module:\nfrom math import *\nsin(1)\nExpected Output:\n0.8414709848078965\nAlternative ways to import and use sin:\n# Method 1: Import specific function\nfrom math import sin\nprint(sin(1))\n\n# Method 2: Import entire module\nimport math\nprint(math.sin(1))\n\n# Method 3: Import with alias\nimport math as m\nprint(m.sin(1))"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-6-case-sensitivity",
    "href": "files/labs/lab1/lab1.html#task-6-case-sensitivity",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 6: Case Sensitivity",
    "text": "Task 6: Case Sensitivity\n\n\nAssign a value to a variable:\nmy_variable = 5\nTry printing with the wrong capitalization:\nprint(My_variable)\nYou should see:\nNameError: name 'My_variable' is not defined\nPrint with the correct capitalization:\nprint(my_variable)\nOutput:\n5\nTry these examples to see how Python treats variable names:\nmy_variable = 5\nMy_variable = 10\nMY_VARIABLE = 15\nmy_Variable = 20\n\nprint(f\"my_variable = {my_variable}\")\nprint(f\"My_variable = {My_variable}\")\nprint(f\"MY_VARIABLE = {MY_VARIABLE}\")\nprint(f\"my_Variable = {my_Variable}\")\nOutput:\nmy_variable = 5\nMy_variable = 10\nMY_VARIABLE = 15\nmy_Variable = 20"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-7-commenting-code",
    "href": "files/labs/lab1/lab1.html#task-7-commenting-code",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 7: Commenting Code",
    "text": "Task 7: Commenting Code\n\nAdd comments to your code from previous tasks. For example:\n# Task 2: Basic arithmetic\n2 + 2  # Adding two integers\n\n# Task 4: Complex mathematical expression\n# Calculate (2 + 3) / (4 + 5^6)\nnumerator = 2 + 3  # Sum of 2 and 3\ndenominator = 4 + 5**6  # 4 plus 5 to the 6th power\nresult = numerator / denominator  # Final division\nprint(f\"Result: {result}\")\n\n# Task 5: Import math module and use sin function\nfrom math import *  # Import all math functions\nangle_in_radians = 1  # Input angle in radians\nsine_value = sin(angle_in_radians)  # Calculate sine\nprint(f\"sin(1) = {sine_value}\")\n\n# Task 6: Variable assignment with proper naming\nmy_variable = 5  # Store the value 5 in my_variable\nprint(my_variable)  # Display the value\n\n\"\"\"\nThis is a multi-line comment.\nIt can span multiple lines and is useful\nfor longer explanations or documentation.\n\"\"\"\nGood commenting practices: - Explain what the code does - Clarify complex calculations - Document variable purposes - Use both inline (#) and block (\"\"\") comments"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-8-data-types",
    "href": "files/labs/lab1/lab1.html#task-8-data-types",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 8: Data Types",
    "text": "Task 8: Data Types\n\n\nUse the type() function to check data types:\nprint(type(1))           # Output: &lt;class 'int'&gt;\nprint(type(1.1))         # Output: &lt;class 'float'&gt;\nprint(type(\"hello\"))     # Output: &lt;class 'str'&gt;\nTry more examples:\nprint(\"Integer:\", type(42))\nprint(\"Float:\", type(3.14159))\nprint(\"String with single quotes:\", type('Python'))\nprint(\"String with double quotes:\", type(\"Programming\"))\nprint(\"Boolean True:\", type(True))\nprint(\"Boolean False:\", type(False))\nprint(\"List:\", type([1, 2, 3]))\nprint(\"Tuple:\", type((1, 2, 3)))\nprint(\"Dictionary:\", type({\"key\": \"value\"}))"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-9-variables-and-calculations",
    "href": "files/labs/lab1/lab1.html#task-9-variables-and-calculations",
    "title": "PSTAT 5A Lab 1",
    "section": "Task 9: Variables and Calculations",
    "text": "Task 9: Variables and Calculations\n\n\nAdd a Markdown cell:\n# Task 9\nAssign values to variables:\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\nUpdate num_sections:\nnum_sections = num_sections + 1\nprint(f\"Updated number of sections: {num_sections}\")\n# Alternative: num_sections += 1\n# Alternative: num_sections = 4 + 1\nPredict and test expressions:\nprint(type(course))           # Expected: &lt;class 'str'&gt;\nprint(type(num_sections))     # Expected: &lt;class 'int'&gt;\nprint(num_sections * section_capacity) # Expected: 125\nCalculate course capacity:\ncourse_capacity = num_sections * section_capacity\nprint(f\"Course: {course}\")\nprint(f\"Number of sections: {num_sections}\")\nprint(f\"Capacity per section: {section_capacity}\")\nprint(f\"Total course capacity: {course_capacity}\")\nComplete solution with comments:\n# Step 2: Initial variable assignments\ncourse = \"PSTAT 5A\"          # Course name as string\nnum_sections = 4             # Initial number of sections\nsection_capacity = 25        # Maximum students per section\n\n# Step 3: A new section has been added\nnum_sections = num_sections + 1  # Increment by 1, now equals 5\n\n# Step 4: Testing expressions with predictions\nprint(\"Testing type() function:\")\nprint(f\"type(course) = {type(course)}\")  # Expected: &lt;class 'str'&gt;\nprint(f\"type(num_sections) = {type(num_sections)}\")  # Expected: &lt;class 'int'&gt;\nprint(f\"num_sections * section_capacity = {num_sections * section_capacity}\")  # Expected: 125\n\n# Step 5: Calculate total course capacity\ncourse_capacity = num_sections * section_capacity  # 5 × 25 = 125\nprint(f\"\\nFinal Results:\")\nprint(f\"Course: {course}\")\nprint(f\"Total sections: {num_sections}\")\nprint(f\"Capacity per section: {section_capacity}\")\nprint(f\"Total course capacity: {course_capacity} students\")"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#summary-of-key-concepts-learned",
    "href": "files/labs/lab1/lab1.html#summary-of-key-concepts-learned",
    "title": "PSTAT 5A Lab 1",
    "section": "Summary of Key Concepts Learned",
    "text": "Summary of Key Concepts Learned\n\nJupyterHub Environment: Creating and renaming notebooks, understanding cell types (Code vs Markdown), running cells, navigating the interface\nPython Basics: Arithmetic operations, order of operations, error reading\nVariables and Data Types: Assignment, case sensitivity, types, type()\nModules and Imports: Import syntax, using functions, math module\nComments and Documentation: Inline and block comments, purpose\nProgramming Best Practices: Descriptive variable names, comments, incremental testing, reading errors, using variables"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#next-steps",
    "href": "files/labs/lab1/lab1.html#next-steps",
    "title": "PSTAT 5A Lab 1",
    "section": "Next Steps",
    "text": "Next Steps\nIn Lab 2, you’ll learn about: - Python functions and how to create them - Data structures (lists, dictionaries) - Control flow (if statements, loops) - More advanced programming concepts\nGreat work completing Lab 1! You now have the foundation needed for statistical programming in Python."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-3",
    "href": "files/labs/lab1/lab1.html#task-3",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Create a new Markdown cell labeled:\n# Task 3\nCreate a new code cell and run:\n2 plus 2\nObserve the SyntaxError and note how Python points to the problem.\n\n\nNote: Always read error messages, they tell you what went wrong!"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#python-as-a-calculator",
    "href": "files/labs/lab1/lab1.html#python-as-a-calculator",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Python follows the usual order of operations:\n\nParentheses\n\nExponents\n\nMultiplication / Division\n\nAddition / Subtraction\n\n\n\n\nOperation\nPython Syntax\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1.0\n\n\nExponentiation\n**\n2 ** 2\n4"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-4",
    "href": "files/labs/lab1/lab1.html#task-4",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Compute the following in separate code cells:\n\n\\[\\frac{2 + 3}{4 + 5^6}\\]\n\\[(1 - 3 \\cdot 4^5)^6\\]"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#python-modules",
    "href": "files/labs/lab1/lab1.html#python-modules",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "In Python, a module is simply a file (with a .py extension) that contains related code, functions, classes, and variables—that you can reuse in other programs. Modules help you organize your code, avoid naming conflicts, and leverage functionality written by others.\n\n\n\nReusability: Write a function once, then import it wherever you need it.\n\nOrganization: Group related functionality into logical units (e.g., math operations).\n\nNamespace Management: Keep your global namespace clean by accessing code through the module’s name.\n\n\n\n\nThere are several ways to bring module code into your current script or notebook:\n\nImport the entire module\nimport math\nprint(math.sin(1))\nImport specific names\nfrom math import sin, pi\nprint(sin(pi/2))\nImport with an alias\nimport numpy as np\narr = np.array([1, 2, 3])\n\n\nTip: Use specific imports (from module import name) to keep your namespace tidy, or aliases (import module as m) for brevity.\n\n\n\n\n\n\nStandard library: Modules like math, random, and datetime come with Python.\n\nThird-party: Install via pip install package_name (e.g. pip install pandas).\n\nYour own: Create my_utils.py and then import my_utils in your project.\n\nModules are the building blocks of larger Python applications; get comfortable importing and exploring them!"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-5",
    "href": "files/labs/lab1/lab1.html#task-5",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "In a code cell, type:\nsin(1)\nObserve the NameError.\n\nIn the same (or a new) cell, load the module and retry:\nfrom math import *\nsin(1)"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#variable-assignment",
    "href": "files/labs/lab1/lab1.html#variable-assignment",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Variables in Python are used to store data values. You can think of them as labels for data that you want to use later in your program.\n\nAssignment:\nx = 2\nPrinting:\nprint(x)\n\nPython is case-sensitive: my_variable ≠ My_variable.\n\nBehind the scenes, print() is a function that takes one or more values and displays them on the screen. We’ll learn what functions are and how to create our own functions soon."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-6",
    "href": "files/labs/lab1/lab1.html#task-6",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Assign:\nmy_variable = 5\nIn a new cell, run:\nprint(My_variable)\n– observe the NameError due to wrong capitalization.\nNameError: name 'My_variable' is not defined\nIn the same cell, run:\nprint(my_variable)\nNow you should see 5 printed without any errors."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#comments",
    "href": "files/labs/lab1/lab1.html#comments",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Comments are notes in your code that Python ignores when running the program. They help you and others understand what your code does. Comments are essential for documenting your code, explaining complex logic, or leaving reminders for yourself or others. They do not affect the execution of your program.\nYou can add comments anywhere in your code, and they can be on their own line or at the end of a line of code.\nIn Python, comments start with a # symbol. Everything after the # on that line is considered a comment and will not be executed by Python. You can also use multi-line comments with triple quotes (\"\"\" or '''), which allows you to write longer explanations or block comments that span multiple lines. These are often used for documentation strings (docstrings) to describe functions, classes, or modules.\nYou can add comments in two ways:\n\nInline comment: Use # to comment out a single line.\nExample:\n# This is an inline comment\nx = 5  # Assign 5 to x\nBlock comment: Use triple quotes \"\"\" or ''' to comment out multiple lines.\n\nExample:\n\"\"\"\nMultiple lines\nof comment here\n\"\"\""
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-7",
    "href": "files/labs/lab1/lab1.html#task-7",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Go back and add descriptive comments to some of your previous code cells."
  },
  {
    "objectID": "files/labs/lab1/lab1.html#basic-data-types",
    "href": "files/labs/lab1/lab1.html#basic-data-types",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Python has several basic data types, which are the building blocks for more complex data structures. The most common ones are: - bool — boolean (e.g. True, False)\n\nNoneType — represents the absence of a value (e.g. None)\nlist — ordered collection (e.g. [1, 2, 3])\ntuple — immutable ordered collection (e.g. (1, 2, 3))\ndict — key-value pairs (e.g. `{“key”: “value”})\nset — unordered collection of unique items (e.g. {1, 2, 3})\n\nThe most basic data types you will use in this course are:\n\nint — integer (e.g. 1, 42)\nfloat — real number (e.g. 1.0, 3.14)\n\nstr — string/text (e.g. \"hello\", 'abc')"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-8",
    "href": "files/labs/lab1/lab1.html#task-8",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Run each in its own cell:\ntype(1)\ntype(1.1)\ntype(\"hello\")"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#using-variables-and-data-types",
    "href": "files/labs/lab1/lab1.html#using-variables-and-data-types",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "You can assign values to variables and use them in expressions. Here’s an example:"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#task-9",
    "href": "files/labs/lab1/lab1.html#task-9",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "Create a new Markdown cell labeled:\n# Task 9\nIn a new code cell, perform the following variable assignments:\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\nA new section has been added! Update the variable num_sections to be one more than when you initially defined it above. (Don’t just use num_sections = 5- think about our discussion on updating variables above!)\nUsing comments, write down what you think the output of each of the following expressions will be:\ntype(course)\ntype(num_sections)\nnum_sections * section_capacity\nThen, run each expression in a separate code chunk and comment on the results.\nCreate a new variable called course_capacity and assign it the value of the maximum capacity of the course. (Hint: there are only 5 sections, and each section has a maximum capacity of 25. Try to use your already-defined variables as much as possible!)"
  },
  {
    "objectID": "files/labs/lab1/lab1.html#conclusion",
    "href": "files/labs/lab1/lab1.html#conclusion",
    "title": "PSTAT 5A Lab 1",
    "section": "",
    "text": "That wraps up Lab 1! You’ve successfully navigated the JupyterHub environment, learned how to switch between and run Code and Markdown cells, experimented with basic Python expressions, and practiced variable assignment. In Lab 2, we’ll dive deeper into Python functions, data structures, and more advanced programming concepts. Great work, see you next time!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-6",
    "href": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-6",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 6",
    "text": "Welcome to Lecture 6\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#learning-objectives",
    "href": "files/lecture_notes/lecture7/lecture7.html#learning-objectives",
    "title": "PSTAT 5A: Counting",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\n\n\n\n\n\n\n\nLearning Objectives\n\n\nBy the end of this lecture, you will be able to:\n\nApply the fundamental counting principles (Section 0.4)\nCalculate permutations with and without repetition (Section 0.8, Section 0.11, Section 0.14)\nCalculate combinations and understand when to use them (Section 0.16, Section 0.17)\nDistinguish between permutations and combinations (Section 0.19)\nUse counting techniques to solve probability problems (Section 0.24)\nApply the inclusion-exclusion principle (Section 0.28)\nSolve complex counting problems systematically (Section 0.32)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#fundamental-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\nThis section addresses the learning objective: Apply the fundamental counting principles\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\nStart\n├── Task 1: n₁ ways\n│   ├── Choice 1 ──┐\n│   ├── Choice 2 ──┼── Task 2: n₂ ways\n│   └── Choice n₁ ──┘\n│       ├── Choice 1 ──┐\n│       ├── Choice 2 ──┼── Total: n₁ × n₂ × ... × nₖ\n│       └── Choice n₂ ──┘"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Permutations?",
    "text": "What Are Permutations?\nThis section addresses the learning objectives: Calculate permutations with and without repetition and Distinguish between permutations and combinations\n\n\n\n\n\n\n\nPermutation\n\n\nAn arrangement of objects where order matters\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll permutations of ABC:\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\n\n\n\n\n\nRace finish positions (1st, 2nd, 3rd)\nSeating arrangements\nPasswords\nDNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-r-from-n",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-r-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\)",
    "text": "Permutations of \\(r\\) Objects from \\(n\\)\nThis section addresses the learning objective: Calculate permutations with and without repetition\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\n\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-repetition",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-repetition",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition",
    "text": "Permutations with Repetition\nThis section addresses the learning objective: Calculate permutations with and without repetition\n\n\n\n\n\n\n\nPermutations with Repetition\n\n\nWhen some objects are identical, we have fewer distinct arrangements\nIf we have \\(n\\) objects where: - \\(n_1\\) are of type 1 - \\(n_2\\) are of type 2\n- … - \\(n_k\\) are of type \\(k\\)\nNumber of distinct arrangements: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#combinations",
    "href": "files/lecture_notes/lecture7/lecture7.html#combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\nThis section addresses the learning objectives: Calculate combinations and understand when to use them and Distinguish between permutations and combinations\n\n\n\n\n\n\n\nCombination\n\n\nA selection of objects where order does NOT matter\n\n\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\n\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\nKey Point: Order doesn't matter for combinations\n\n\n\nChoosing committee members\nSelecting pizza toppings\nForming study groups\nLottery number selection\n\n\n\n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutation-vs-combination",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutation-vs-combination",
    "title": "PSTAT 5A: Counting",
    "section": "Key Decision: Permutation or Combination?",
    "text": "Key Decision: Permutation or Combination?\nThis section addresses the learning objective: Distinguish between permutations and combinations\n\n\n\n\n\n\n\n\n\nHow to Decide\n\n\nAsk yourself: Does order matter?\nOrder matters → Use Permutations - Arrangements, sequences, rankings\nOrder doesn’t matter → Use Combinations\n- Selections, groups, subsets"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#counting-probability",
    "href": "files/lecture_notes/lecture7/lecture7.html#counting-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Counting and Probability",
    "text": "Counting and Probability\nThis section addresses the learning objective: Use counting techniques to solve probability problems\n\n\n\nA committee of 4 people is chosen from 7 women and 5 men. What’s the probability that exactly 2 are women?\n\nTotal ways to choose 4 from 12: \\(\\binom{12}{4} = 495\\)\nWays to choose 2 women from 7: \\(\\binom{7}{2} = 21\\) Ways to choose 2 men from 5: \\(\\binom{5}{2} = 10\\)\n\nFavorable outcomes: \\(\\binom{7}{2} \\times \\binom{5}{2} = 21 \\times 10 = 210\\)\nProbability: \\(\\frac{210}{495} = \\frac{14}{33}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion",
    "href": "files/lecture_notes/lecture7/lecture7.html#inclusion-exclusion",
    "title": "PSTAT 5A: Counting",
    "section": "The Inclusion-Exclusion Principle",
    "text": "The Inclusion-Exclusion Principle\nThis section addresses the learning objective: Apply the inclusion-exclusion principle\n\n\n\n\n\n\n\nKey Formula\n\n\nFor two sets \\(A\\) and \\(B\\): \\[|A \\cup B| = |A| + |B| - |A \\cap B|\\]\nFor three sets \\(A\\), \\(B\\), and \\(C\\): \\[|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\]\n\n\n\n\n\nStep-by-step process:\n1. Count elements in A: |A|\n2. Count elements in B: |B|  \n3. Subtract overlap: |A∩B| (avoid double counting)\n4. Result: |A∪B| = |A| + |B| - |A∩B|\n\nThe overlap gets counted twice, so we subtract it once!\n\n\n\n\n                            \n                                            \n\n\nInclusion-Exclusion Principle:\n|A∪B| = |A| + |B| - |A∩B|\nWe add |A| and |B|, then subtract |A∩B| to avoid double counting!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#learning-summary",
    "href": "files/lecture_notes/lecture7/lecture7.html#learning-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Learning Objectives Summary",
    "text": "Learning Objectives Summary\n\n\n\n\n\n\n\nWhat We’ve Covered\n\n\nIn this lecture, we’ve addressed all the learning objectives:\n\n✅ Apply the fundamental counting principles: Covered in Section 0.4\n✅ Calculate permutations with and without repetition: Covered in Section 0.8, Section 0.11, and Section 0.14\n\n✅ Calculate combinations and understand when to use them: Covered in Section 0.16 and Section 0.17\n✅ Distinguish between permutations and combinations: Covered in Section 0.19\n✅ Use counting techniques to solve probability problems: Covered in Section 0.24\n✅ Apply the inclusion-exclusion principle: Covered in Section 0.28\n✅ Solve complex counting problems systematically: Covered in Section 0.32"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-1",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-1",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\nA password must contain:\n\nExactly 8 characters\nEach character is either a letter (26 possibilities) or digit (10 possibilities)\n\nHow many possible passwords are there?\n\n\n\n\n\n\n\nSolution. Each position has \\(26 + 10 = 36\\) choices.\nTotal: \\(36^8 = 2,821,109,907,456\\) passwords"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-2",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-2",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\nA baseball team has 15 players. How many ways can the coach:\n\nArrange all 15 players in a line?\nChoose and arrange 9 players for the starting lineup (batting order matters)?\n\n\n\nSolution. \n\n\\(15! = 1,307,674,368,000\\)\n\\(P(15,9) = \\frac{15!}{6!} = 1,816,214,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-3",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-3",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nFrom a class of 20 students:\n\nHow many ways to choose 5 students for a study group?\nHow many ways to choose a president, vice-president, and secretary?\nHow many ways to arrange all 20 students in a circle?\n\n\n\nSolution. \n\n\\(C(20,5) = \\frac{20!}{5! \\times 15!} = 15,504\\) (order doesn’t matter)\n\\(P(20,3) = \\frac{20!}{17!} = 6,840\\) (order matters)\n\\((20-1)! = 19!\\) (circular permutation)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-4",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-4",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA standard deck has 52 cards. What’s the probability that a 5-card hand contains:\n\nExactly 3 aces?\nAt least 1 ace?\n\n\n\nSolution. \n\nWays to get 3 aces from 4: \\(\\binom{4}{3} = 4\\) Ways to get 2 non-aces from 48: \\(\\binom{48}{2} = 1,128\\) Total 5-card hands: \\(\\binom{52}{5} = 2,598,960\\)\nProbability: \\(\\frac{4 \\times 1,128}{2,598,960} = \\frac{4,512}{2,598,960} \\approx 0.00174\\)\nWays to choose 5 cards with no aces: \\(\\binom{48}{5} = 1,712,304\\)\n\\(P(\\text{at least 1 ace}) = 1 - \\frac{\\binom{48}{5}}{\\binom{52}{5}} = 1 - \\frac{1,712,304}{2,598,960} \\approx 0.341\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-5",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-5",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 5",
    "text": "Practice Problem 5\n\nA pizza shop offers 12 toppings. How many different pizzas can you order if:\n\nYou want exactly 4 toppings?\nYou want at most 3 toppings?\nYou want at least 1 topping?\n\n\n\nSolution. \n\n\\(\\binom{12}{4} = 495\\)\n\\(\\binom{12}{0} + \\binom{12}{1} + \\binom{12}{2} + \\binom{12}{3} = 1 + 12 + 66 + 220 = 299\\)\n\\(2^{12} - 1 = 4,095\\) (all subsets except empty set)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-6",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-6",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\n\nA class has 15 students: 8 women and 7 men. How many ways can we:\n\nForm a committee of 5 people with exactly 3 women?\nArrange 6 people in a row with alternating genders (starting with a woman)?\n\n\n\nSolution. \n\n\\(\\binom{8}{3} \\times \\binom{7}{2} = 56 \\times 21 = 1,176\\)\nChoose 3 women from 8: \\(P(8,3) = 336\\) Choose 3 men from 7: \\(P(7,3) = 210\\) Total: \\(336 \\times 210 = 70,560\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-7",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-7",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\n\nA standard deck of cards is shuffled. What’s the probability that:\n\nThe top 4 cards are all hearts?\nIn a 13-card hand, you get exactly one card from each rank?\n\n\n\nSolution. \n\n\\(\\frac{13}{52} \\times \\frac{12}{51} \\times \\frac{11}{50} \\times \\frac{10}{49} = \\frac{13 \\times 12 \\times 11 \\times 10}{52 \\times 51 \\times 50 \\times 49} \\approx 0.0026\\)\nChoose 1 card from each of 13 ranks: \\(4^{13}\\) Total 13-card hands: \\(\\binom{52}{13}\\) Probability: \\(\\frac{4^{13}}{\\binom{52}{13}} \\approx 6.3 \\times 10^{-6}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-7",
    "href": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#permutations-of-r-objects-from-n-permutations-r-from-n",
    "href": "files/lecture_notes/lecture7/lecture7.html#permutations-of-r-objects-from-n-permutations-r-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\) {@permutations-r-from-n}",
    "text": "Permutations of \\(r\\) Objects from \\(n\\) {@permutations-r-from-n}\nThis section addresses the learning objective: Calculate permutations with and without repetition\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\n\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle-fundamental-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle-fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle {@fundamental-counting}",
    "text": "The Fundamental Counting Principle {@fundamental-counting}\nThis section addresses the learning objective: Apply the fundamental counting principles\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\nStart\n├── Task 1: n₁ ways\n│   ├── Choice 1 ──┐\n│   ├── Choice 2 ──┼── Task 2: n₂ ways\n│   └── Choice n₁ ──┘\n│       ├── Choice 1 ──┐\n│       ├── Choice 2 ──┼── Total: n₁ × n₂ × ... × nₖ\n│       └── Choice n₂ ──┘"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-fundamental-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    Start([Start]) --&gt; T1[\"Task 1: n₁ ways\"]\n    T1 --&gt; C1[Choice 1]\n    T1 --&gt; C2[Choice 2]\n    T1 --&gt; Cn1[Choice n₁]\n    C1 --&gt; T2[\"Task 2: n₂ ways\"]\n    C2 --&gt; T2\n    Cn1 --&gt; T2\n    T2 --&gt; C21[Choice 1]\n    T2 --&gt; C22[Choice 2]\n    T2 --&gt; C2n[Choice n₂]\n    C21 --&gt; Total((\"Total ways:\\n n₁ × n₂ × ... × nₖ\"))\n    C22 --&gt; Total\n    C2n --&gt; Total\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-permutations",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-permutations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Permutations?",
    "text": "What Are Permutations?\n\n\n\n\n\n\n\nPermutation\n\n\nAn arrangement of objects where order matters\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll permutations of ABC:\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\n\n\n\n\n\nRace finish positions (1st, 2nd, 3rd)\nSeating arrangements\nPasswords\nDNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-permutations-r-from-n",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-permutations-r-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\)",
    "text": "Permutations of \\(r\\) Objects from \\(n\\)\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\n\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\n\nSolution. \\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-permutations-repetition",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-permutations-repetition",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition",
    "text": "Permutations with Repetition\n\n\n\n\n\n\n\nPermutations with Repetition\n\n\nWhen some objects are identical, we have fewer distinct arrangements\nIf we have \\(n\\) objects where:\n\n\\(n_1\\) are of type 1\n\\(n_2\\) are of type 2\n…\n\\(n_k\\) are of type \\(k\\)\n\nNumber of distinct arrangements: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-combinations",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\n\n\n\n\n\n\n\n\n\nCombination\n\n\nA selection of objects where order does NOT matter\n\n\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\n\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\nKey Point: Order doesn't matter for combinations\n\n\n\nChoosing committee members\nSelecting pizza toppings\nForming study groups\nLottery number selection\n\n\n\n\n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-combinations-formula",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-combinations-formula",
    "title": "PSTAT 5A: Counting",
    "section": "Combinations Formula",
    "text": "Combinations Formula\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(C(n,r)\\) or \\(\\binom{n}{r}\\): Number of ways to choose \\(r\\) objects from \\(n\\) distinct objects (order doesn’t matter)\n\\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]\n\n\n\n\n\nHow many ways can we choose 3 people from a group of 8 for a committee?\n\\(C(8,3) = \\frac{8!}{3!(8-3)!} = \\frac{8!}{3! \\times 5!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 56\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-permutation-vs-combination",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-permutation-vs-combination",
    "title": "PSTAT 5A: Counting",
    "section": "Key Decision: Permutation or Combination?",
    "text": "Key Decision: Permutation or Combination?\n\n\n\n\n\n\n\n\n\nHow to Decide\n\n\nAsk yourself: Does order matter?\nOrder matters → Use Permutations - Arrangements, sequences, rankings\nOrder doesn’t matter → Use Combinations\n- Selections, groups, subsets"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-counting-probability",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-counting-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\n\n\nA committee of 4 people is chosen from 7 women and 5 men. What’s the probability that exactly 2 are women?\n\nTotal ways to choose 4 from 12: \\(\\binom{12}{4} = 495\\)\nWays to choose 2 women from 7: \\(\\binom{7}{2} = 21\\)\nWays to choose 2 men from 5: \\(\\binom{5}{2} = 10\\)\n\nSolution. Favorable outcomes: \\(\\binom{7}{2} \\times \\binom{5}{2} = 21 \\times 10 = 210\\)\nProbability: \\(\\frac{210}{495} = \\frac{14}{33}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-inclusion-exclusion",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-inclusion-exclusion",
    "title": "PSTAT 5A: Counting",
    "section": "The Inclusion-Exclusion Principle",
    "text": "The Inclusion-Exclusion Principle\n\n\n\nFor two sets \\(A\\) and \\(B\\): \\[|A \\cup B| = |A| + |B| - |A \\cap B|\\]\nFor three sets \\(A\\), \\(B\\), and \\(C\\): \\[|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| \\\\\n- |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#sec-problem-solving-strategy",
    "href": "files/lecture_notes/lecture7/lecture7.html#sec-problem-solving-strategy",
    "title": "PSTAT 5A: Counting",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\n\n\n\n\n\nStrategy\n\n\n\nRead carefully: What exactly are we counting?\nIdentify the type: Permutation, combination, or other?\nCheck for restrictions: Are there constraints?\nDoes order matter?: This determines permutation vs combination\nBreak down complex problems: Use multiplication principle\nVerify your answer: Does it make sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#why-study-counting-.section-with-icon",
    "href": "files/lecture_notes/lecture7/lecture7.html#why-study-counting-.section-with-icon",
    "title": "PSTAT 5A: Counting",
    "section": "Why Study Counting? {.section-with-icon}",
    "text": "Why Study Counting? {.section-with-icon}\n\n\n\nCounting helps us:\n\nCalculate probabilities for complex events\nSolve optimization problems\nUnderstand combinations in genetics, computer science\nAnalyze algorithms and data structures\nMake decisions involving arrangements and selections\n\n\nReal-world applications of counting include:\n\nCryptography: Password strength and encryption key space\nGenetics: DNA sequence analysis and gene combinations\n\nTournament brackets: March Madness and sports competitions\nLottery odds: Probability calculations for games of chance\nPassword security: Character combinations and brute force protection"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle-sec-fundamental-counting",
    "href": "files/lecture_notes/lecture7/lecture7.html#the-fundamental-counting-principle-sec-fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle {#sec-fundamental-counting}",
    "text": "The Fundamental Counting Principle {#sec-fundamental-counting}\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    Start([Start]) --&gt; T1[\"Task 1: n₁ ways\"]\n    T1 --&gt; C1[Choice 1]\n    T1 --&gt; C2[Choice 2]\n    T1 --&gt; Cn1[Choice n₁]\n    C1 --&gt; T2[\"Task 2: n₂ ways\"]\n    C2 --&gt; T2\n    Cn1 --&gt; T2\n    T2 --&gt; C21[Choice 1]\n    T2 --&gt; C22[Choice 2]\n    T2 --&gt; C2n[Choice n₂]\n    C21 --&gt; Total((\"Total ways:\\n n₁ × n₂ × ... × nₖ\"))\n    C22 --&gt; Total\n    C2n --&gt; Total"
  },
  {
    "objectID": "files/worksheets/worksheet3_sln.html#probability-distributions",
    "href": "files/worksheets/worksheet3_sln.html#probability-distributions",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nValid distributions require: all probabilities ≥ 0 and sum = 1\nCheck both conditions systematically"
  },
  {
    "objectID": "files/worksheets/worksheet3_sln.html#permutations-vs-combinations",
    "href": "files/worksheets/worksheet3_sln.html#permutations-vs-combinations",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "Permutations vs Combinations",
    "text": "Permutations vs Combinations\n\nPermutations: Order matters, use \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: Order doesn’t matter, use \\(C(n,r) = \\frac{n!}{r!(n-r)!}\\)\nMultiplication principle: Combine independent choices"
  },
  {
    "objectID": "files/worksheets/worksheet3_sln.html#conditional-probability",
    "href": "files/worksheets/worksheet3_sln.html#conditional-probability",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nWithout replacement: Creates dependence between events\nUse definition: \\(P(B|A) = \\frac{P(A \\cap B)}{P(A)}\\)\nLaw of Total Probability: For calculating marginal probabilities"
  },
  {
    "objectID": "files/worksheets/worksheet3_sln.html#advanced-counting",
    "href": "files/worksheets/worksheet3_sln.html#advanced-counting",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "Advanced Counting",
    "text": "Advanced Counting\n\nCase analysis: Break complex problems into manageable parts\nHandle restrictions: Consider what’s allowed vs. not allowed\nVerification: Use complementary counting or direct calculation"
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#basic-probability",
    "href": "files/lecture_notes/lecture7/review_exercise.html#basic-probability",
    "title": "PSTAT 5A: Review Exercises",
    "section": "1. Basic Probability",
    "text": "1. Basic Probability\n\nA single fair six-sided die is rolled once.\n1.1 What is \\(P(\\text{roll is an even number})\\)?\n1.2 What is \\(P(\\text{roll is 5 or 6})\\)?"
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#basic-probability-answers",
    "href": "files/lecture_notes/lecture7/review_exercise.html#basic-probability-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "1. Basic Probability: Answers",
    "text": "1. Basic Probability: Answers\n\nSolution. 1.1 \\(P(\\text{even}) = 3/6 = \\tfrac{1}{2}\\)\n1.2 \\(P(5 \\text{ or } 6) = 2/6 = \\tfrac{1}{3}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#joint-conditional-probability",
    "href": "files/lecture_notes/lecture7/review_exercise.html#joint-conditional-probability",
    "title": "PSTAT 5A: Review Exercises",
    "section": "2. Joint & Conditional Probability",
    "text": "2. Joint & Conditional Probability\n\nYou draw two cards without replacement from a standard 52-card deck.\n2.1. What is the probability that the first card is an Ace?\n2.2. Given that the first card is an Ace, what is the probability that the second card is also an Ace?\n2.3. Compute \\(P(\\text{both cards are Aces})\\):\n\nas a direct joint probability.\nusing conditional probability formula."
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#joint-conditional-probability-answers",
    "href": "files/lecture_notes/lecture7/review_exercise.html#joint-conditional-probability-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "2. Joint & Conditional Probability: Answers",
    "text": "2. Joint & Conditional Probability: Answers\n\nSolution. 2.1. \\(4/52 = \\tfrac{1}{13}\\)\n2.2. \\(3/51 = \\tfrac{1}{17}\\)\n2.3.\n\n\\(4/52 \\times 3/51 = 1/221\\)\n\\((1/13)\\times(1/17) = 1/221\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#independence-vs.-mutual-exclusivity",
    "href": "files/lecture_notes/lecture7/review_exercise.html#independence-vs.-mutual-exclusivity",
    "title": "PSTAT 5A: Review Exercises",
    "section": "3. Independence vs. Mutual Exclusivity",
    "text": "3. Independence vs. Mutual Exclusivity\n\nFlip two fair coins in sequence. Define:\n\n\\(A =\\) first flip is Heads\n\n\\(B =\\) second flip is Heads\n\n\\(C =\\) both flips are Heads\n\n3.1 Are events \\(A\\) and \\(B\\) independent?\n3.2 Are events \\(A\\) and \\(C\\) mutually exclusive?\n3.3 Compute \\(P(A\\cap B)\\) and compare with \\(P(A)P(B)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#independence-vs.-mutual-exclusivity-answers",
    "href": "files/lecture_notes/lecture7/review_exercise.html#independence-vs.-mutual-exclusivity-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "3. Independence vs. Mutual Exclusivity: Answers",
    "text": "3. Independence vs. Mutual Exclusivity: Answers\n\nSolution. \n\nYes, independent: \\(P(B|A)=1/2 = P(B)\\)\nNo, not mutually exclusive: \\(A\\cap C \\neq \\varnothing\\).\n\n\\(P(A\\cap B)=1/4, \\;P(A)P(B)=1/4\\) → matches (independence)."
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#bonus-challenge",
    "href": "files/lecture_notes/lecture7/review_exercise.html#bonus-challenge",
    "title": "PSTAT 5A: Review Exercises",
    "section": "4. Bonus Challenge",
    "text": "4. Bonus Challenge\n\nA bag contains 3 red balls and 2 blue balls. You draw one ball, replace it, then draw again.\n4.1 Are the two draws independent? Why or why not?\n4.2 Compute \\(P(\\text{red then blue})\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture7/review_exercise.html#bonus-challenge-answers",
    "href": "files/lecture_notes/lecture7/review_exercise.html#bonus-challenge-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "4. Bonus Challenge: Answers",
    "text": "4. Bonus Challenge: Answers\n\nSolution. \n\nYes—they’re independent because of replacement.\n\n\\((3/5)\\times(2/5) = 6/25\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#welcome-to-lecture-7",
    "href": "files/lecture_notes/lecture6/lecture6.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#learning-objectives",
    "href": "files/lecture_notes/lecture6/lecture6.html#learning-objectives",
    "title": "PSTAT 5A: Counting",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\n\n\n\n\n\n\nLearning Objectives\n\n\n\nBy the end of this lecture, you will be able to:\n\nApply the fundamental counting principles (Section 0.4)\nCalculate permutations with and without repetition (Section 0.8, Section 0.11, Section 0.14)\nCalculate combinations and understand when to use them (Section 0.16, Section 0.17)\nDistinguish between permutations and combinations (Section 0.19)\nUse counting techniques to solve probability problems (Section 0.24)\nApply the inclusion-exclusion principle (Section 0.28)\nSolve complex counting problems systematically (Section 0.32)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#why-study-counting",
    "href": "files/lecture_notes/lecture6/lecture6.html#why-study-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Why Study Counting?",
    "text": "Why Study Counting?\n\n\n\n\n\nCounting helps us:\n\nCalculate probabilities for complex events\n\nSolve optimization problems\n\nUnderstand combinations in genetics, computer science\n\nAnalyze algorithms and data structures\n\nMake decisions involving arrangements and selections\n\n\n\nReal-world applications of counting include:\n\nCryptography: Password strength and encryption key space\nGenetics: DNA sequence analysis and gene combinations\n\nTournament brackets: March Madness and sports competitions\nLottery odds: Probability calculations for games of chance\nPassword security: Character combinations and brute force protection"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-fundamental-counting",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\n\n\n\nflowchart LR\n    Start([Start]) --&gt; T1[\"Task 1: n₁ ways\"]\n    T1 --&gt; C1[Choice 1]\n    T1 --&gt; C2[Choice 2]\n    T1 --&gt; Cn1[Choice n₁]\n    C1 --&gt; T2[\"Task 2: n₂ ways\"]\n    C2 --&gt; T2\n    Cn1 --&gt; T2\n    T2 --&gt; C21[Choice 1]\n    T2 --&gt; C22[Choice 2]\n    T2 --&gt; C2n[Choice n₂]\n    C21 --&gt; Total((\"Total ways:\\n n₁ × n₂ × ... × nₖ\"))\n    C22 --&gt; Total\n    C2n --&gt; Total"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#simple-counting-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#simple-counting-example",
    "title": "PSTAT 5A: Counting",
    "section": "Simple Counting Example",
    "text": "Simple Counting Example\n\nFormat ABC-123\n\nFirst position: 26 letters\nSecond position: 26 letters\nThird position: 26 letters\nFourth position: 10 digits\nFifth position: 10 digits\nSixth position: 10 digits\n\n\n\n\nSolution. Total possibilities: \\(26 \\times 26 \\times 26 \\times 10 \\times 10 \\times 10 = 26^3 \\times 10^3 = 17,576,000\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#restaurant-menu-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#restaurant-menu-example",
    "title": "PSTAT 5A: Counting",
    "section": "Restaurant Menu Example",
    "text": "Restaurant Menu Example\n\nA restaurant offers:\n🍤 Appetizers: 4\n🍲 Main Courses: 6\n🍰 Desserts: 3\nHow many different three-course meals are possible?\n\n\nSolution. \\(4 \\times 6 \\times 3 = 72\\) different meals"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-1",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-1",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\nA password must contain:\n\nExactly 8 characters\nEach character is either a letter (26 possibilities) or digit (10 possibilities)\n\nHow many possible passwords are there?\n\n\n\n\n\n\n\nSolution. Each position has \\(26 + 10 = 36\\) choices.\nTotal: \\(36^8 = 2,821,109,907,456\\) passwords"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-permutations",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-permutations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Permutations?",
    "text": "What Are Permutations?\n\n\n\n\n\n\nPermutation\n\n\n\nAn arrangement of objects where order matters\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll permutations of ABC:\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA\n\n\n\n\n\nRace finish positions (1st, 2nd, 3rd)\nSeating arrangements\nPasswords\nDNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#permutations-of-n-distinct-objects",
    "href": "files/lecture_notes/lecture6/lecture6.html#permutations-of-n-distinct-objects",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(n\\) Distinct Objects",
    "text": "Permutations of \\(n\\) Distinct Objects\n\n\n\n\n\n\n\n\nKey Formula\n\n\n\nThe number of ways to arrange \\(n\\) distinct objects is:\n\\[n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\]\n\n\n\n\n\n\n\n\n\nSeating Process:\n1st position: 5 choices (Alice, Bob, Carol, David, Eve)\n2nd position: 4 choices (whoever is left)\n3rd position: 3 choices (whoever is left)\n4th position: 2 choices (whoever is left)\n5th position: 1 choice (last person)\nHow many ways can 5 people sit in a row?\n\n\nSolution. \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#factorial-values",
    "href": "files/lecture_notes/lecture6/lecture6.html#factorial-values",
    "title": "PSTAT 5A: Counting",
    "section": "Factorial Values",
    "text": "Factorial Values\n\n\n\n\n\n\\(n\\)\n\\(n!\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n2\n2\n\n\n3\n6\n\n\n4\n24\n\n\n5\n120\n\n\n10\n3,628,800\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nNote\n\n\n\n\\(0! = 1\\) by definition"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-permutations-r-from-n",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-permutations-r-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\)",
    "text": "Permutations of \\(r\\) Objects from \\(n\\)\n\n\n\n\n\n\nKey Formula\n\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\n\nSolution. \\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#understanding-pnr",
    "href": "files/lecture_notes/lecture6/lecture6.html#understanding-pnr",
    "title": "PSTAT 5A: Counting",
    "section": "Understanding \\(P(n,r)\\)",
    "text": "Understanding \\(P(n,r)\\)\nWhy is \\(P(n,r) = \\frac{n!}{(n-r)!}\\)?\n\nFirst position: \\(n\\) choices\nSecond position: \\((n-1)\\) choices\n\nThird position: \\((n-2)\\) choices\n…\n\\(r\\)-th position: \\((n-r+1)\\) choices\n\n\nTotal: \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-r+1) = \\frac{n!}{(n-r)!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-2",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-2",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\nA baseball team has 15 players. How many ways can the coach:\n\nArrange all 15 players in a line?\nChoose and arrange 9 players for the starting lineup (batting order matters)?\n\n\n\nSolution. \n\n\\(15! = 1,307,674,368,000\\)\n\\(P(15,9) = \\frac{15!}{6!} = 1,816,214,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-permutations-repetition",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-permutations-repetition",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition",
    "text": "Permutations with Repetition\n\n\n\n\n\n\nPermutations with Repetition\n\n\n\nWhen some objects are identical, we have fewer distinct arrangements\nIf we have \\(n\\) objects where:\n\n\\(n_1\\) are of type 1\n\\(n_2\\) are of type 2\n…\n\\(n_k\\) are of type \\(k\\)\n\nNumber of distinct arrangements: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#permutations-with-repetition-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#permutations-with-repetition-example",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition Example",
    "text": "Permutations with Repetition Example\n\n\n\nHow many distinct arrangements are there of the letters in “STATISTICS”?\n\nS-T-A-T-I-S-T-I-C-S\n\nTotal letters: 10\nS appears 3 times\nT appears 3 times\nA appears 1 time\nI appears 2 times\nC appears 1 time\n\n\n\nSolution. \\(\\frac{10!}{3! \\times 3! \\times 1! \\times 2! \\times 1!} = \\frac{3,628,800}{6 \\times 6 \\times 1 \\times 2 \\times 1} = \\frac{3,628,800}{72} = 50,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-combinations",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\n\n\n\n\n\n\n\n\nCombination\n\n\n\nA selection of objects where order does NOT matter\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\n\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\nKey Point: Order doesn't matter for combinations\n\n\n\nChoosing committee members\nSelecting pizza toppings\nForming study groups\nLottery number selection\n\n\n\n\n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-combinations-formula",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-combinations-formula",
    "title": "PSTAT 5A: Counting",
    "section": "Combinations Formula",
    "text": "Combinations Formula\n\n\n\n\n\n\nKey Formula\n\n\n\n\\(C(n,r)\\) or \\(\\binom{n}{r}\\): Number of ways to choose \\(r\\) objects from \\(n\\) distinct objects (order doesn’t matter)\n\\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]\n\n\n\nHow many ways can we choose 3 people from a group of 8 for a committee?\n\\(C(8,3) = \\frac{8!}{3!(8-3)!} = \\frac{8!}{3! \\times 5!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 56\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#relationship-permutations-vs-combinations",
    "href": "files/lecture_notes/lecture6/lecture6.html#relationship-permutations-vs-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Relationship: Permutations vs Combinations",
    "text": "Relationship: Permutations vs Combinations\n\n\n\n\n\n\nRelationship\n\n\n\n\\(P(n,r) = C(n,r) \\times r!\\)\nWhy? For each combination of \\(r\\) objects, there are \\(r!\\) ways to arrange them\n\n\n\nRelationship: P(n,r) = C(n,r) × r!\n\nExample: Choose 3 from 8 people\n- C(8,3) = 56 combinations\n- For each combination, arrange 3 people: 3! = 6 ways\n- Total: 56 × 6 = 336 = P(8,3)\n\nCombination ABC → Permutations: ABC, ACB, BAC, BCA, CAB, CBA\n\n\n\\(P(8,3) = C(8,3) \\times 3! = 56 \\times 6 = 336\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-permutation-vs-combination",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-permutation-vs-combination",
    "title": "PSTAT 5A: Counting",
    "section": "Key Decision: Permutation or Combination?",
    "text": "Key Decision: Permutation or Combination?\n\n\n\n\n\n\n\n\nHow to Decide\n\n\n\nAsk yourself: Does order matter?\nOrder matters → Use Permutations - Arrangements, sequences, rankings\nOrder doesn’t matter → Use Combinations\n- Selections, groups, subsets\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nTip\n\n\n\n✅ \\(P(n,r)\\) = counts both selection & arrangement → grows faster\n✅ \\(C(n,r)\\) = counts only selection → grows slower\n✅ The difference comes from \\(r!\\), which is big even for modest \\(r\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-3",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-3",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nFrom a class of 20 students:\n\nHow many ways to choose 5 students for a study group?\nHow many ways to choose a president, vice-president, and secretary?\n\n\n\nSolution. \n\n\\(C(20,5) = \\frac{20!}{5! \\times 15!} = 15,504\\) (order doesn’t matter)\n\\(P(20,3) = \\frac{20!}{17!} = 6,840\\) (order matters)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#properties-of-combinations",
    "href": "files/lecture_notes/lecture6/lecture6.html#properties-of-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Properties of Combinations",
    "text": "Properties of Combinations\n\n\n\n\n\n\nProperties\n\n\n\n\nSymmetry: \\(\\binom{n}{r} = \\binom{n}{n-r}\\)\nPascal’s Identity: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nBoundary conditions: \\(\\binom{n}{0} = \\binom{n}{n} = 1\\)\n\n\n\n\n\\(\\binom{8}{3} = \\binom{8}{5} = 56\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#pascals-triangle",
    "href": "files/lecture_notes/lecture6/lecture6.html#pascals-triangle",
    "title": "PSTAT 5A: Counting",
    "section": "Pascal’s Triangle",
    "text": "Pascal’s Triangle\n\n\n\n           1\n         1   1\n       1   2   1\n     1   3   3   1\n   1   4   6   4   1\n 1   5  10  10   5   1\n1   6  15  20  15   6   1\nPattern: Each number is the sum of the two numbers above it.\nFormula: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nEach number is \\(\\binom{n}{r}\\) where \\(n\\) is the row and \\(r\\) is the position\nExample: \\(\\binom{4}{2} = 6\\) (row 4, position 2)\nRow 3:       1   3   3   1\n            ↙ ↘ ↙ ↘ ↙ ↘ ↙ ↘\nRow 4:     1   4   6   4   1\n\n\n\n\n\n\n                            \n                                            \n\n\nPascal's Triangle (showing C(n,r) values):\nRow 0: [1]\nRow 1: [1, 1]\nRow 2: [1, 2, 1]\nRow 3: [1, 3, 3, 1]\nRow 4: [1, 4, 6, 4, 1]\nRow 5: [1, 5, 10, 10, 5, 1]\nRow 6: [1, 6, 15, 20, 15, 6, 1]\nRow 7: [1, 7, 21, 35, 35, 21, 7, 1]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#binomial-theorem",
    "href": "files/lecture_notes/lecture6/lecture6.html#binomial-theorem",
    "title": "PSTAT 5A: Counting",
    "section": "Binomial Theorem",
    "text": "Binomial Theorem\n\n\n\n\n\n\nKey Formula\n\n\n\n\\[(x + y)^n = \\sum_{r=0}^{n} \\binom{n}{r} x^{n-r} y^r\\]\n\n\n\n\\((x + y)^3 = \\binom{3}{0}x^3 + \\binom{3}{1}x^2y + \\binom{3}{2}xy^2 + \\binom{3}{3}y^3\\)\n\\(= x^3 + 3x^2y + 3xy^2 + y^3\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-counting-probability",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-counting-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\n\n\nA committee of 4 people is chosen from 7 women and 5 men. What’s the probability that exactly 2 are women?\n\nTotal ways to choose 4 from 12: \\(\\binom{12}{4} = 495\\)\nWays to choose 2 women from 7: \\(\\binom{7}{2} = 21\\)\nWays to choose 2 men from 5: \\(\\binom{5}{2} = 10\\)\n\nSolution. Favorable outcomes: \\(\\binom{7}{2} \\times \\binom{5}{2} = 21 \\times 10 = 210\\)\nProbability: \\(\\frac{210}{495} = \\frac{14}{33}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-4",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-4",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA standard deck has 52 cards. What’s the probability that a 5-card hand contains:\n\nExactly 3 aces?\nAt least 1 ace?\n\n\n\nSolution. \n\nWays to get 3 aces from 4: \\(\\binom{4}{3} = 4\\) Ways to get 2 non-aces from 48: \\(\\binom{48}{2} = 1,128\\) Total 5-card hands: \\(\\binom{52}{5} = 2,598,960\\)\nProbability: \\(\\frac{4 \\times 1,128}{2,598,960} = \\frac{4,512}{2,598,960} \\approx 0.00174\\)\nWays to choose 5 cards with no aces: \\(\\binom{48}{5} = 1,712,304\\)\n\\(P(\\text{at least 1 ace}) = 1 - \\frac{\\binom{48}{5}}{\\binom{52}{5}} = 1 - \\frac{1,712,304}{2,598,960} \\approx 0.341\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4-continued",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-problem-4-continued",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4 (continued)",
    "text": "Practice Problem 4 (continued)\n\n\nAt least 1 ace = 1 - (no aces)\n\n\n\n\n\n\n\n\nTip\n\n\n\nWays to choose 5 cards with no aces: \\(\\binom{48}{5} = 1,712,304\\)\n\n\n\nSolution. \\(P(\\text{at least 1 ace}) = 1 - \\frac{\\binom{48}{5}}{\\binom{52}{5}} = 1 - \\frac{1,712,304}{2,598,960} \\approx 0.341\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#permutations-with-restrictions",
    "href": "files/lecture_notes/lecture6/lecture6.html#permutations-with-restrictions",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Restrictions",
    "text": "Permutations with Restrictions\n\nHow many 6-letter “words” can be formed from the letters A, B, C, D, E, F if:\n\nNo letter is repeated\nA and B must be adjacent\n\n\n\nSolution. Treat AB as a single unit\n\n5 units to arrange: (AB), C, D, E, F → \\(5! = 120\\) ways\nA and B can be arranged within their unit: \\(2! = 2\\) ways\nTotal: \\(5! \\times 2! = 240\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-inclusion-exclusion",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-inclusion-exclusion",
    "title": "PSTAT 5A: Counting",
    "section": "The Inclusion-Exclusion Principle",
    "text": "The Inclusion-Exclusion Principle\n\n\n\nFor two sets \\(A\\) and \\(B\\): \\[|A \\cup B| = |A| + |B| - |A \\cap B|\\]\nFor three sets \\(A\\), \\(B\\), and \\(C\\): \\[|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| \\\\\n- |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#inclusion-exclusion-example",
    "href": "files/lecture_notes/lecture6/lecture6.html#inclusion-exclusion-example",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Example",
    "text": "Inclusion-Exclusion Example\n\nHow many integers from 1 to 100 are divisible by 2, 3, or 5?\n\nLet:\n\n\\(A\\) = divisible by 2: \\(|A| = 50\\)\n\\(B\\) = divisible by 3: \\(|B| = 33\\)\n\\(C\\) = divisible by 5: \\(|C| = 20\\)\n\n\n\n\n\n\n\nNote\n\n\n\n\\(|A \\cap B| = 16\\) (divisible by 6)\n\\(|A \\cap C| = 10\\) (divisible by 10)\n\\(|B \\cap C| = 6\\) (divisible by 15)\n\\(|A \\cap B \\cap C| = 3\\) (divisible by 30)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#inclusion-exclusion-solution",
    "href": "files/lecture_notes/lecture6/lecture6.html#inclusion-exclusion-solution",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Solution",
    "text": "Inclusion-Exclusion Solution\n\nSolution. \\[|A \\cup B \\cup C| = 50 + 33 + 20 - 16 - 10 - 6 + 3 = 74\\]\nAnswer: 74 integers from 1 to 100 are divisible by at least one of 2, 3, or 5"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#multinomial-coefficients",
    "href": "files/lecture_notes/lecture6/lecture6.html#multinomial-coefficients",
    "title": "PSTAT 5A: Counting",
    "section": "Multinomial Coefficients",
    "text": "Multinomial Coefficients\n\n\n\n\n\n\nMultinomial Coefficient\n\n\n\nNumber of ways to divide \\(n\\) objects into groups of sizes \\(n_1, n_2, \\ldots, n_k\\):\n\\[\\binom{n}{n_1, n_2, \\ldots, n_k} = \\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\]\n\n\n\nHow many ways can 12 people be divided into 3 teams of 4?\n\\(\\binom{12}{4,4,4} = \\frac{12!}{4! \\times 4! \\times 4!} = 34,650\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#sec-problem-solving-strategy",
    "href": "files/lecture_notes/lecture6/lecture6.html#sec-problem-solving-strategy",
    "title": "PSTAT 5A: Counting",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\n\n\n\n\nStrategy\n\n\n\n\nRead carefully: What exactly are we counting?\nIdentify the type: Permutation, combination, or other?\nCheck for restrictions: Are there constraints?\nDoes order matter?: This determines permutation vs combination\nBreak down complex problems: Use multiplication principle\nVerify your answer: Does it make sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture6/lecture6.html#common-mistakes-to-avoid",
    "title": "PSTAT 5A: Counting",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n\n\n\n\n\nCommon Mistakes\n\n\n\n\nConfusing permutations and combinations\n\nAlways ask: “Does order matter?”\n\nForgetting about restrictions\n\nRead the problem carefully\n\nDouble counting\n\nMake sure you’re not counting the same arrangement twice\n\nNot considering the complement\n\nSometimes “at least” problems are easier using complements"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-6",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-6",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\n\nA class has 15 students: 8 women and 7 men. How many ways can we:\n\nForm a committee of 5 people with exactly 3 women?\nArrange 6 people in a row with alternating genders (starting with a woman)?\n\n\n\nSolution. \n\n\\(\\binom{8}{3} \\times \\binom{7}{2} = 56 \\times 21 = 1,176\\)\nChoose 3 women from 8: \\(P(8,3) = 336\\) Choose 3 men from 7: \\(P(7,3) = 210\\) Total: \\(336 \\times 210 = 70,560\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#counting-in-computer-science",
    "href": "files/lecture_notes/lecture6/lecture6.html#counting-in-computer-science",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Computer Science",
    "text": "Counting in Computer Science\n\nPassword Security:\n\n8-character password with letters, digits, symbols\n\\((26 + 26 + 10 + 32)^8 = 94^8 \\approx 6.1 \\times 10^{15}\\)\n\nHash Functions:\n\nDistributing data into buckets\nCollision probability calculations\n\nAlgorithm Analysis:\n\nCounting operations, comparisons\nBig O notation foundations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#counting-in-genetics",
    "href": "files/lecture_notes/lecture6/lecture6.html#counting-in-genetics",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Genetics",
    "text": "Counting in Genetics\n\nDNA Sequences:\n\n4 bases (A, T, G, C)\nGene of length \\(n\\): \\(4^n\\) possible sequences\n\nProtein Folding:\n\nNumber of possible conformations\nCombinatorial explosion\n\nPopulation Genetics:\n\nHardy-Weinberg calculations\nAllele combinations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#technology-and-counting",
    "href": "files/lecture_notes/lecture6/lecture6.html#technology-and-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Technology and Counting",
    "text": "Technology and Counting\n\n\n\n\n\n\nTools\n\n\n\nCalculators:\n\nUse nPr and nCr functions\nBe careful with large numbers\n\nSoftware:\n\nR: factorial(), choose(), combn()\nPython: math.factorial(), math.comb()\nExcel: FACT(), COMBIN(), PERMUT()\n\nOnline Tools:\n\nWolfram Alpha for complex calculations\nCombination/permutation calculators"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#practice-7",
    "href": "files/lecture_notes/lecture6/lecture6.html#practice-7",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\n\nA standard deck of cards is shuffled. What’s the probability that:\n\nThe top 4 cards are all hearts?\nIn a 13-card hand, you get exactly one card from each rank?\n\n\n\nSolution. \n\n\\(\\frac{13}{52} \\times \\frac{12}{51} \\times \\frac{11}{50} \\times \\frac{10}{49} = \\frac{13 \\times 12 \\times 11 \\times 10}{52 \\times 51 \\times 50 \\times 49} \\approx 0.0026\\)\nChoose 1 card from each of 13 ranks: \\(4^{13}\\) Total 13-card hands: \\(\\binom{52}{13}\\) Probability: \\(\\frac{4^{13}}{\\binom{52}{13}} \\approx 6.3 \\times 10^{-6}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#extending-to-probability",
    "href": "files/lecture_notes/lecture6/lecture6.html#extending-to-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Extending to Probability",
    "text": "Extending to Probability\n\n\n\n\n\n\nDistributions\n\n\n\nHypergeometric Distribution:\n\nDrawing without replacement\nUses combinations: \\(P(X = k) = \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\nBinomial Distribution:\n\nDrawing with replacement\nUses combinations: \\(P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\)\n\nWe’ll explore these distributions in detail in future lectures"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#historical-note",
    "href": "files/lecture_notes/lecture6/lecture6.html#historical-note",
    "title": "PSTAT 5A: Counting",
    "section": "Historical Note",
    "text": "Historical Note\n\n\n\n\n\n\nHistory\n\n\n\nBlaise Pascal (1623-1662) and Pierre de Fermat (1601-1665): - Founded probability theory through gambling problems - Pascal’s triangle and combinations\nLeonhard Euler (1707-1783): - Advanced combinatorics - Graph theory connections\nModern applications span computer science, biology, physics, and economics"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#common-student-questions",
    "href": "files/lecture_notes/lecture6/lecture6.html#common-student-questions",
    "title": "PSTAT 5A: Counting",
    "section": "Common Student Questions",
    "text": "Common Student Questions\n\nQ: “When do I use permutations vs combinations?”\nA: Ask “Does order matter?” Order matters → permutation\nQ: “How do I handle restrictions?”\nA: Break the problem into cases or use complementary counting\nQ: “What if objects are identical?”\nA: Use the formula for permutations with repetition\nQ: “How do I check my answer?”\nA: Verify with small examples or use different methods"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#study-tips",
    "href": "files/lecture_notes/lecture6/lecture6.html#study-tips",
    "title": "PSTAT 5A: Counting",
    "section": "Study Tips",
    "text": "Study Tips\n\n\n\n\n\n\nTips\n\n\n\n\nPractice, practice, practice: Work through many examples\nIdentify patterns: Learn to recognize problem types\nStart simple: Build up to complex problems\nCheck your work: Use different approaches when possible\nUnderstand concepts: Don’t just memorize formulas"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#learning-summary",
    "href": "files/lecture_notes/lecture6/lecture6.html#learning-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Learning Objectives Summary",
    "text": "Learning Objectives Summary\n\n\n\n\n\n\nWhat We’ve Covered\n\n\n\nIn this lecture, we’ve addressed all the learning objectives:\n\n✅ Apply the fundamental counting principles: Covered in Section 0.4\n✅ Calculate permutations with and without repetition: Covered in Section 0.8, Section 0.11, and Section 0.14\n\n✅ Calculate combinations and understand when to use them: Covered in Section 0.16 and Section 0.17\n✅ Distinguish between permutations and combinations: Covered in Section 0.19\n✅ Use counting techniques to solve probability problems: Covered in Section 0.24\n✅ Apply the inclusion-exclusion principle: Covered in Section 0.28\n✅ Solve complex counting problems systematically: Covered in Section 0.32"
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html",
    "title": "PSTAT 5A: Review Exercises",
    "section": "",
    "text": "A single fair six-sided die is rolled once.\n1.1 What is \\(P(\\text{roll is an even number})\\)?\n1.2 What is \\(P(\\text{roll is 5 or 6})\\)?"
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#basic-probability",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#basic-probability",
    "title": "PSTAT 5A: Review Exercises",
    "section": "",
    "text": "A single fair six-sided die is rolled once.\n1.1 What is \\(P(\\text{roll is an even number})\\)?\n1.2 What is \\(P(\\text{roll is 5 or 6})\\)?"
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#basic-probability-answers",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#basic-probability-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "1. Basic Probability: Answers",
    "text": "1. Basic Probability: Answers\n\nSolution. 1.1 \\(P(\\text{even}) = 3/6 = \\tfrac{1}{2}\\)\n1.2 \\(P(5 \\text{ or } 6) = 2/6 = \\tfrac{1}{3}\\)"
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#joint-conditional-probability",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#joint-conditional-probability",
    "title": "PSTAT 5A: Review Exercises",
    "section": "2. Joint & Conditional Probability",
    "text": "2. Joint & Conditional Probability\n\nYou draw two cards without replacement from a standard 52-card deck.\n2.1. What is the probability that the first card is an Ace?\n2.2. Given that the first card is an Ace, what is the probability that the second card is also an Ace?\n2.3. Compute \\(P(\\text{both cards are Aces})\\):\n\nas a direct joint probability.\nusing conditional probability formula."
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#joint-conditional-probability-answers",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#joint-conditional-probability-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "2. Joint & Conditional Probability: Answers",
    "text": "2. Joint & Conditional Probability: Answers\n\nSolution. 2.1. \\(4/52 = \\tfrac{1}{13}\\)\n2.2. \\(3/51 = \\tfrac{1}{17}\\)\n2.3.\n\n\\(4/52 \\times 3/51 = 1/221\\)\n\\((1/13)\\times(1/17) = 1/221\\)"
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#independence-vs.-mutual-exclusivity",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#independence-vs.-mutual-exclusivity",
    "title": "PSTAT 5A: Review Exercises",
    "section": "3. Independence vs. Mutual Exclusivity",
    "text": "3. Independence vs. Mutual Exclusivity\n\nFlip two fair coins in sequence. Define:\n\n\\(A =\\) first flip is Heads\n\n\\(B =\\) second flip is Heads\n\n\\(C =\\) both flips are Heads\n\n3.1 Are events \\(A\\) and \\(B\\) independent?\n3.2 Are events \\(A\\) and \\(C\\) mutually exclusive?\n3.3 Compute \\(P(A\\cap B)\\) and compare with \\(P(A)P(B)\\)."
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#independence-vs.-mutual-exclusivity-answers",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#independence-vs.-mutual-exclusivity-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "3. Independence vs. Mutual Exclusivity: Answers",
    "text": "3. Independence vs. Mutual Exclusivity: Answers\n\nSolution. \n\nYes, independent: \\(P(B|A)=1/2 = P(B)\\)\nNo, not mutually exclusive: \\(A\\cap C \\neq \\varnothing\\).\n\n\\(P(A\\cap B)=1/4, \\;P(A)P(B)=1/4\\) → matches (independence)."
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#bonus-challenge",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#bonus-challenge",
    "title": "PSTAT 5A: Review Exercises",
    "section": "4. Bonus Challenge",
    "text": "4. Bonus Challenge\n\nA bag contains 3 red balls and 2 blue balls. You draw one ball, replace it, then draw again.\n4.1 Are the two draws independent? Why or why not?\n4.2 Compute \\(P(\\text{red then blue})\\)."
  },
  {
    "objectID": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#bonus-challenge-answers",
    "href": "files/lecture_notes/review/Quiz1/q1_review_exercise.html#bonus-challenge-answers",
    "title": "PSTAT 5A: Review Exercises",
    "section": "4. Bonus Challenge: Answers",
    "text": "4. Bonus Challenge: Answers\n\nSolution. \n\nYes—they’re independent because of replacement.\n\n\\((3/5)\\times(2/5) = 6/25\\)"
  },
  {
    "objectID": "files/worksheets/worksheet4.html",
    "href": "files/worksheets/worksheet4.html",
    "title": "PSTAT 5A Practice Worksheet 4",
    "section": "",
    "text": "Instructions and Overview\n⏰ Time Allocation:\n\nQuiz Review : 15 minutes\nSection A (Warm-up): 15 minutes\nSection B (Intermediate): 20 minutes\nOptional Question: Do on your own\nTotal: 50 minutes\n\n\n📝 Important Instructions:\n\nUse the formulas provided for guidance\nRound final answers to 4 decimal places unless otherwise specified\nIdentify the distribution type before calculating\nShow your work for expected value and variance calculations\nUse calculator as needed for factorials and combinations\n\n\n\n📚 Key Formulas Reference:\nGeneral Random Variable Properties:\n\nExpected Value: \\(E[X] = \\sum_{k} k \\cdot P(X = k)\\)\nVariance: \\(\\text{Var}(X) = E[X^2] - (E[X])^2 = \\sum_{k} k^2 \\cdot P(X = k) - \\mu^2\\)\nStandard Deviation: \\(\\sigma = \\sqrt{\\text{Var}(X)}\\)\n\nDiscrete Distributions:\nBernoulli Distribution: \\(X \\sim \\text{Bernoulli}(p)\\)\n\nPMF: \\(P(X = k) = p^k(1-p)^{1-k}\\) for \\(k \\in \\{0,1\\}\\)\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\n\nBinomial Distribution: \\(X \\sim \\text{Binomial}(n,p)\\)\n\nPMF: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\) for \\(k = 0, 1, 2, ..., n\\)\nMean: \\(E[X] = np\\)\nVariance: \\(\\text{Var}(X) = np(1-p)\\)\n\nGeometric Distribution: \\(X \\sim \\text{Geometric}(p)\\)\n\nPMF: \\(P(X = k) = (1-p)^{k-1} p\\) for \\(k = 1, 2, 3, ...\\)\nMean: \\(E[X] = \\frac{1}{p}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1-p}{p^2}\\)\n\nPoisson Distribution: \\(X \\sim \\text{Poisson}(\\lambda)\\)\n\nPMF: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\) for \\(k = 0, 1, 2, ...\\)\nMean: \\(E[X] = \\lambda\\)\nVariance: \\(\\text{Var}(X) = \\lambda\\)\n\n\n\n\nSection A: Basic Concepts and Identification\n⏱️ Estimated time: 15 minutes\n\nProblem A1: Distribution Identification\nFor each scenario below, identify the appropriate discrete distribution and state the parameter(s). Do not calculate probabilities yet.\n(a) A fair coin is flipped until the first head appears. Let X = number of flips needed.\n(b) A quality control inspector tests 20 randomly selected items from a production line where 5% are defective. Let X = number of defective items found.\n(c) A website receives visitors at an average rate of 3 per minute. Let X = number of visitors in a 2-minute period.\n(d) A basketball player shoots one free throw with an 80% success rate. Let X = 1 if successful, 0 if unsuccessful.\n(e) A student keeps taking a driving test until they pass. The probability of passing on any attempt is 0.7. Let X = number of attempts needed to pass.\n\nWork Space:\n\n\nProblem A2: Probability Mass Function\nThe random variable X has the following probability distribution:\n\n\n\nX\n1\n2\n3\n4\n5\n\n\n\n\nP(X=k)\n0.1\n0.3\n0.4\na\n0.1\n\n\n\n(a) Find the value of \\(a\\).\n(b) Calculate \\(P(X \\leq 3)\\).\n(c) Calculate \\(P(X &gt; 2)\\).\n\nWork Space:\n\n\n\nSection B: Expected Value and Variance\n⏱️ Estimated time: 20 minutes\n\nProblem B1: Manual Calculations\nUsing the probability distribution from Problem A2, calculate:\n(a) The expected value \\(E[X]\\)\n(b) The variance \\(\\text{Var}(X)\\)\n(c) The standard deviation \\(\\sigma\\)\n\n\n\n\n\n\nTip\n\n\n\nCalculation Strategy:\nFor expected value: \\(E[X] = \\sum k \\cdot P(X = k)\\)\nFor variance: First find \\(E[X^2] = \\sum k^2 \\cdot P(X = k)\\), then use \\(\\text{Var}(X) = E[X^2] - (E[X])^2\\)\nShow your work step by step!\n\n\n\nWork Space:\n\n\nProblem B2: Bernoulli and Binomial Applications\nA manufacturing process produces items that are defective with probability 0.15.\n(a) If you select one item randomly, what is the expected value and variance of X = number of defective items?\n(b) If you select 25 items randomly, what is the expected number of defective items and the standard deviation?\n\n\n\n\n\n\nTip\n\n\n\nPart (a) is a Bernoulli distribution. Part (b) is a Binomial distribution. Use the formulas from the reference box!\n\n\n\nWork Space:\n\n\n\nOptional Questions\n\nOptional Problem : Conceptual Understanding\n(a) Explain the key difference between a Binomial distribution and a Geometric distribution in terms of what they count.\n(b) When would you use a Poisson distribution instead of a Binomial distribution?\n(c) If \\(X \\sim \\text{Binomial}(n, p)\\), under what conditions would the variance be maximized?\n\nWork Space:"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html",
    "href": "files/worksheets/worksheet4_sln.html",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "",
    "text": "Important\n\n\n\nInstructions: For each scenario below, identify the appropriate probability distribution and specify its parameters. Justify your choice by identifying the key characteristics.\n\n\n\n\n\n\nA fair coin is flipped until the first head appears. Let \\(X\\) = number of flips needed.\n\nSolution:\n\n\n\n\n\n\nGeometric Distribution with parameter \\(p = 0.5\\)\n\n\n\nKey Characteristics:\n\n✓ We count the number of trials until the first success\n✓ Each flip is independent with constant probability of success\n✓ Only two outcomes per trial (head or tail)\n✓ We stop as soon as we get a success\n\nNotation: \\(X \\sim \\text{Geometric}(p = 0.5)\\)\n\n\n\n\n\n\n\nA quality control inspector tests \\(20\\) randomly selected items from a production line where \\(5\\%\\) are defective. Let \\(X\\) = number of defective items found.\n\nSolution:\n\n\n\n\n\n\nBinomial Distribution with parameters \\(n = 20\\), \\(p = 0.05\\)\n\n\n\nKey Characteristics:\n\n✓ Fixed number of trials (\\(n = 20\\))\n✓ Each item has the same probability of being defective (\\(p = 0.05\\))\n✓ We count the number of successes (defective items)\n✓ Each test is independent\n\nNotation: \\(X \\sim \\text{Binomial}(n = 20, p = 0.05)\\)\n\n\n\n\n\n\n\nA website receives visitors at an average rate of \\(3\\) per minute. Let \\(X\\) = number of visitors in a 2-minute period.\n\nSolution:\n\n\n\n\n\n\nPoisson Distribution with parameter \\(\\lambda = 6\\)\n\n\n\nKey Characteristics:\n\n✓ Events occurring over time at a constant average rate\n✓ Events are independent and rare\n✓ Rate calculation: \\(3 \\text{ visitors/minute} \\times 2 \\text{ minutes} = 6\\) expected visitors\n\nNotation: \\(X \\sim \\text{Poisson}(\\lambda = 6)\\)\n\n\n\n\n\n\n\nA basketball player shoots one free throw with an \\(80\\%\\) success rate. Let \\(X = 1\\) if successful, \\(0\\) if unsuccessful.\n\nSolution:\n\n\n\n\n\n\nBernoulli Distribution with parameter \\(p = 0.8\\)\n\n\n\nKey Characteristics:\n\n✓ Single trial with exactly two outcomes\n✓ Success (make shot) vs. Failure (miss shot)\n✓ Binary outcome: \\(X \\in \\{0, 1\\}\\)\n\nNotation: \\(X \\sim \\text{Bernoulli}(p = 0.8)\\)\n\n\n\n\n\n\n\nA student keeps taking a driving test until they pass. The probability of passing on any attempt is \\(0.7\\). Let \\(X\\) = number of attempts needed to pass.\n\nSolution:\n\n\n\n\n\n\nGeometric Distribution with parameter \\(p = 0.7\\)\n\n\n\nKey Characteristics:\n\n✓ We count trials until first success (passing the test)\n✓ Each attempt is independent with constant probability\n✓ Student continues until success occurs\n\nNotation: \\(X \\sim \\text{Geometric}(p = 0.7)\\)\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Distribution Identification Summary\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nDecision Framework Visualization\n\n\n\n\n                            \n                                            \n\n\nFigure 1: Decision Framework for Distribution Identification\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuick Reference Guide Ask these key questions to identify distributions:\nHow many trials?\n\nOne trial → Bernoulli\nFixed number → Binomial (if counting successes)\nUntil first success → Geometric\n\nWhat are we counting?\n\nSuccesses in fixed trials → Binomial\nTrials until success → Geometric\nEvents over time/space → Poisson\n\nTime component?\n\nEvents at constant rate over time → Poisson\nNo time component → Binomial/Bernoulli/Geometric\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGeometric vs. Binomial: Geometric counts trials until success;\n\nBinomial counts successes in fixed trials\nPoisson parameter: Remember to multiply rate by time period (e.g., 3/minute × 2 minutes = \\(\\lambda\\) = 6)\n\nIndependence assumption: All these distributions require independent trials/events\n\n\n\n\n\n\nGiven distribution:\n\n\n\nX\n1\n2\n3\n4\n5\n\n\n\n\nP(X=k)\n0.1\n0.3\n0.4\na\n0.1\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Probability Mass Function\n\n\n\n\n\n\n\nSolution. Since probabilities must sum to \\(1\\):\n\\(0.1 + 0.3 + 0.4 + a + 0.1 = 1\\)\n\\(0.9 + a = 1\\)\n\\(\\boxed{a = 0.1}\\)\n\n\n\n\n\nSolution. \\(P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3)\\)\n\\(P(X ≤ 3) = 0.1 + 0.3 + 0.4 = \\boxed{0.8}\\)\n\n\n\n\n\n                            \n                                            \n\n\nFigure 3: PMF showing P(X ≤ 3) = 0.8\n\n\n\n\n\n\n\n\nSolution. \\(P(X &gt; 2) = P(X = 3) + P(X = 4) + P(X = 5)\\)\n\\(P(X &gt; 2) = 0.4 + 0.1 + 0.1 = \\boxed{0.6}\\)\n(Check: \\(0.8 + 0.2 = 1\\) and the full PMF sums to 1, so the results are consistent.)\n\n\n\n\n\n                            \n                                            \n\n\nFigure 4: PMF showing P(X &gt; 2) = 0.6\n\n\n\n\nPutting everything together:\n\n\n\n\n\n\nTip\n\n\n\nKey Insights from Visualizations\nDistribution Shape: The PMF shows \\(X = 3\\) has the highest probability (\\(0.4\\)), making it the mode\nCumulative Probability: \\(P(X ≤ 3) = 0.8\\) means \\(80\\%\\) of outcomes are 3 or less\nComplement Relationship: \\(P(X &gt; 2) = 0.6\\) and \\(P(X ≤ 2) = 0.4\\) sum to \\(1\\)\nSymmetry: The distribution has some symmetry around the center, with equal probabilities at the extremes (\\(X = 1 \\quad \\text{and} \\quad X = 5\\) both have \\(P = 0.1\\))\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 5: PMF showing both P(X ≤ 3) and P(X &gt; 2) regions"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html",
    "href": "files/lecture_notes/test_lectures/lecture6.html",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#sec-objectives",
    "href": "files/lecture_notes/test_lectures/lecture6.html#sec-objectives",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#mutually-exclusive-vs.-independent",
    "href": "files/lecture_notes/test_lectures/lecture6.html#mutually-exclusive-vs.-independent",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent",
    "text": "Mutually Exclusive vs. Independent\n\nMutually Exclusive (left): the circles A and B do not overlap, so \\(P(A\\cap B)=0\\).\nIndependent (right): the circles overlap, and we’ve sized the intersection so that \\(P(A\\cap B)=P(A)\\,P(B)\\).\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Create side-by-side Venn diagrams\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# 1) Mutually Exclusive: no overlap\nvenn2(\n    subsets=(1, 1, 0),\n    set_labels=('A', 'B'),\n    ax=axes[0],\n    subset_label_formatter=lambda x: ''\n)\naxes[0].set_title('Mutually Exclusive\\nP(A∩B) = 0')\n\n# 2) Independent: overlap equals product of areas (e.g., 0.5 * 0.5 = 0.25)\n# Scale counts arbitrarily (25, 25, 25) to represent proportions\nvenn2(\n    subsets=(25, 25, 25),\n    set_labels=('A', 'B'),\n    ax=axes[1],\n    subset_label_formatter=lambda x: ''\n)\naxes[1].set_title('Independent\\nP(A∩B) = P(A)P(B)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#mutually-exclusive-vs.-independent-example",
    "href": "files/lecture_notes/test_lectures/lecture6.html#mutually-exclusive-vs.-independent-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent Example",
    "text": "Mutually Exclusive vs. Independent Example\n\n\n\nDraw a single card from a 52-card deck:\n\nLet A={“draw an Ace”}, so P(A)=4/52.\nLet B={“draw a King”}, so P(B)=4/52.\n\nQ: What is \\(P(A\\cap B)\\) ?\n\n\n\n\n\nSolution. They’re disjoint (you can’t draw an Ace and a King), so \\(P(A\\cap B) = 0\\).\nBut \\(P(A)\\,P(B) = \\frac{4}{52}\\times\\frac{4}{52} = \\frac{16}{2704} \\neq 0\\).\nHence, \\(P(A\\cap B)\\neq P(A)P(B)\\), so they’re not independent."
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#multiplication-rule",
    "href": "files/lecture_notes/test_lectures/lecture6.html#multiplication-rule",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nGeneral case: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nIndependent events: \\(P(A \\cap B) = P(A) \\times P(B)\\)\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Define scaled subset sizes for illustration\n# General case: P(A)=40 (10+30), P(B|A)=0.75 so intersection=30, B-only=20 (if P(B)=50)\n# Independent case: P(A)=40, P(B)=50, intersection=P(A)*P(B)=20\ngeneral_subsets = (10, 20, 30)   # (A-only, B-only, intersection)\nindep_subsets = (20, 30, 20)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- General Multiplication Rule ---\nvenn2(subsets=general_subsets, set_labels=('A', 'B'), ax=axes[0])\naxes[0].set_title('General: P(A∩B) = P(A)·P(B|A)', fontsize=14)\n\n# --- Independent Events ---\nvenn2(subsets=indep_subsets, set_labels=('A', 'B'), ax=axes[1])\naxes[1].set_title('Independent: P(A∩B) = P(A)·P(B)', fontsize=14)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#tree-diagrams",
    "href": "files/lecture_notes/test_lectures/lecture6.html#tree-diagrams",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\n\n🎯 Definition Tree diagrams help visualize sequential events and calculate probabilities.\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes\ncoords = {\n    'root': (0.1, 0.5),\n    'A': (0.4, 0.7),\n    'B': (0.4, 0.3),\n    'A_Red': (0.7, 0.8),\n    'A_Blue': (0.7, 0.6),\n    'B_Red': (0.7, 0.4),\n    'B_Blue': (0.7, 0.2),\n}\n\n# Probabilities\np_A = 0.7\np_B = 0.3\np_R_A = 0.6\np_B_A = 0.4\np_R_B = 0.3\np_B_B = 0.7\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\n# Draw nodes\nfor node, (x, y) in coords.items():\n    circle = Circle((x, y), 0.05, edgecolor='black', facecolor='white', linewidth=2)\n    ax.add_patch(circle)\n    label = node.replace('_', '\\n')\n    if node == 'root':\n        label = 'Start'\n    ax.text(x, y, label, ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw arrows and annotate probabilities\ndef draw_branch(start, end, text):\n    x1, y1 = coords[start]\n    x2, y2 = coords[end]\n    arrow = FancyArrowPatch((x1+0.05, y1), (x2-0.05, y2), arrowstyle='-&gt;', mutation_scale=20)\n    ax.add_patch(arrow)\n    ax.text((x1+x2)/2, (y1+y2)/2, text, fontsize=12, backgroundcolor='white', ha='center')\n\ndraw_branch('root', 'A', f'{p_A}')\ndraw_branch('root', 'B', f'{p_B}')\ndraw_branch('A', 'A_Red', f'{p_R_A}')\ndraw_branch('A', 'A_Blue', f'{p_B_A}')\ndraw_branch('B', 'B_Red', f'{p_R_B}')\ndraw_branch('B', 'B_Blue', f'{p_B_B}')\n\n# Title\nax.set_title('Tree Diagram: Drawing from Urn A (70%) or B (30%)', fontsize=14, pad=20)\n\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#tree-diagram-examples",
    "href": "files/lecture_notes/test_lectures/lecture6.html#tree-diagram-examples",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagram Examples",
    "text": "Tree Diagram Examples\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes in the full tree\ncoords = {\n    'root':       (0.1, 0.5),\n    'A':          (0.35, 0.75),\n    'NotA':       (0.35, 0.25),\n    'A_B':        (0.7, 0.8),\n    'A_notB':     (0.7, 0.65),\n    'NotA_B':     (0.7, 0.35),\n    'NotA_notB':  (0.7, 0.2)\n}\n\n# Probabilities (example values)\npA     = 0.4\npNotA  = 0.6\npB_A   = 0.75\npNotB_A= 0.25\npB_notA= 0.333\npNotB_notA=0.667\n\n# Colors\ncolor_intersection = '#1f77b4'  # blue\ncolor_B            = '#ff7f0e'  # orange\ngray               = 'lightgray'\n\ndef draw_tree(highlight_branches, branch_color, node_to_color):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n\n    # Draw nodes\n    node_patches = {}\n    for node, (x, y) in coords.items():\n        circ = Circle((x, y), 0.035, edgecolor='black', facecolor='white', lw=2)\n        ax.add_patch(circ)\n        node_patches[node] = circ\n        label = {\n            'root': 'Start',\n            'A': 'A',\n            'NotA': '¬A',\n            'A_B': 'B',\n            'A_notB': '¬B',\n            'NotA_B': 'B',\n            'NotA_notB': '¬B'\n        }[node]\n        ax.text(x, y, label, ha='center', va='center', fontsize=12)\n\n    # Function to draw a branch\n    def draw_branch(start, end, text, color, lw=1.5):\n        x1, y1 = coords[start]\n        x2, y2 = coords[end]\n        arr = FancyArrowPatch((x1+0.035, y1), (x2-0.035, y2),\n                              arrowstyle='-|&gt;', mutation_scale=15,\n                              lw=lw, color=color)\n        ax.add_patch(arr)\n        ax.text((x1+x2)/2, (y1+y2)/2, text, ha='center', va='center',\n                backgroundcolor='white', fontsize=10)\n\n    # Draw all branches in gray\n    branches = [\n        ('root','A', f'{pA}'),\n        ('root','NotA', f'{pNotA}'),\n        ('A','A_B', f'{pB_A}'),\n        ('A','A_notB', f'{pNotB_A}'),\n        ('NotA','NotA_B', f'{pB_notA:.3f}'),\n        ('NotA','NotA_notB', f'{pNotB_notA:.3f}')\n    ]\n    for b in branches:\n        draw_branch(*b, color=gray)\n\n    # Highlight requested branches\n    for b in highlight_branches:\n        # find text label for branch\n        prob = {\n            ('root','A'): f'{pA}',\n            ('root','NotA'): f'{pNotA}',\n            ('A','A_B'): f'{pB_A}',\n            ('A','A_notB'): f'{pNotB_A}',\n            ('NotA','NotA_B'): f'{pB_notA:.3f}',\n            ('NotA','NotA_notB'): f'{pNotB_notA:.3f}'\n        }[b]\n        draw_branch(b[0], b[1], prob, color=branch_color, lw=3)\n    \n    # Color the end-node if desired\n    for node in node_to_color:\n        node_patches[node].set_facecolor(branch_color)\n\n    return fig, ax\n\n# Tree 1: highlight only the intersection branch root-&gt;A-&gt;A_B in blue\nfig1, ax1 = draw_tree(highlight_branches=[('root','A'),('A','A_B')],\n                      branch_color=color_intersection,\n                      node_to_color=['A_B'])\nax1.set_title('Intersection Only (P(A∩B))', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Tree 2: highlight both B branches (A-&gt;B and ¬A-&gt;B) in orange\nfig2, ax2 = draw_tree(highlight_branches=[('root','A'),('A','A_B'),\n                                          ('root','NotA'),('NotA','NotA_B')],\n                      branch_color=color_B,\n                      node_to_color=['A_B','NotA_B'])\nax2.set_title('Event B Only (P(B))', fontsize=14)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-2",
    "href": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-2",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\n\n\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement.\n\nWhat’s the probability both balls are red?\nWhat’s the probability the first is red and second is blue?\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.lines import Line2D\n\n# Set up figure\nfig, ax = plt.subplots(figsize=(10, 6))\nax.axis('off')\n\n# Node positions\npositions = {\n    'root': (0.1, 0.5),\n    'R': (0.35, 0.7),\n    'B': (0.35, 0.3),\n    'RR': (0.7, 0.8),\n    'RB': (0.7, 0.6),\n    'BR': (0.7, 0.4),\n    'BB': (0.7, 0.2),\n}\n\n# Define colors\ncolor_root = '#ecf0f1'\ncolor_R = '#e74c3c'       # red for R or RR\ncolor_B = '#3498db'       # blue for B or BB\ncolor_mixed = '#ff7f0e'   # orange for mixed (RB, BR)\nedge_color = 'black'\nlinewidth = 2\ntext_fs = 12\n\n# Draw nodes with colored backgrounds and bold labels\nnode_colors = {\n    'root': color_root,\n    'R': color_R,\n    'B': color_B,\n    'RR': color_R,\n    'RB': color_mixed,\n    'BR': color_mixed,\n    'BB': color_B\n}\n\nfor name, (x, y) in positions.items():\n    circ = Circle((x, y), 0.05, facecolor=node_colors[name],\n                  edgecolor=edge_color, linewidth=linewidth, zorder=2)\n    ax.add_patch(circ)\n    ax.text(x, y, name, ha='center', va='center', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Function to draw an arrow and label\ndef draw_arrow(src, dst, label):\n    x1, y1 = positions[src]\n    x2, y2 = positions[dst]\n    ax.annotate('', xy=(x2-0.05, y2), xytext=(x1+0.05, y1),\n                arrowprops=dict(arrowstyle='-&gt;', color=node_colors[dst], lw=linewidth), zorder=1)\n    ax.text((x1+x2)/2, (y1+y2)/2, label, ha='center', va='center',\n            backgroundcolor='white', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Draw branches with probabilities\ndraw_arrow('root', 'R', '5/8')\ndraw_arrow('root', 'B', '3/8')\ndraw_arrow('R', 'RR', '4/7')\ndraw_arrow('R', 'RB', '3/7')\ndraw_arrow('B', 'BR', '5/7')\ndraw_arrow('B', 'BB', '2/7')\n\n# Label leaves with final probabilities in bold\nleaf_probs = {\n    'RR': '20/56',\n    'RB': '15/56',\n    'BR': '15/56',\n    'BB': '6/56'\n}\nfor leaf, prob in leaf_probs.items():\n    x, y = positions[leaf]\n    ax.text(x+0.12, y, prob, ha='left', va='center', fontsize=text_fs, fontweight='bold')\n\n# Legend\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', label='RR / R',\n           markerfacecolor=color_R, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='BB / B',\n           markerfacecolor=color_B, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='RB / BR',\n           markerfacecolor=color_mixed, markersize=12, markeredgecolor=edge_color),\n]\nax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.02, 0.95))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{both red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)\n\\(P(\\text{red then blue}) = \\frac{5}{8} \\times \\frac{3}{7} = \\frac{15}{56}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#law-of-total-probability",
    "href": "files/lecture_notes/test_lectures/lecture6.html#law-of-total-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n🎯 Definition\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space, then:\n\\[P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\\]\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Ellipse, Rectangle\n\n# Example probabilities\nP_B = [0.3, 0.5, 0.2]\nP_A_given_B = [0.2, 0.6, 0.5]\nP_A_and_B = [P_B[i] * P_A_given_B[i] for i in range(3)]\n\n# Create figure\nfig, ax = plt.subplots(figsize=(9, 5))\nax.set_aspect('equal')\nax.axis('off')\n\n# Draw sample space rectangle\nrect = Rectangle((-1, -2), width=8, height=4, edgecolor='black', facecolor='none', linewidth=2)\nax.add_patch(rect)\nax.text(-0.9, 1.7, r'$\\Omega$', fontsize=14, fontweight='bold', va='top', ha='left')\n\n# Draw B partitions as circles\npositions_B = [(1, 0), (3, 0), (5, 0)]\ncolors_B = ['#ffcc00', '#ff6600', '#ff0066']\n\nfor i, (x, y) in enumerate(positions_B):\n    circle = Circle((x, y), radius=1, facecolor=colors_B[i], edgecolor='black', alpha=0.3)\n    ax.add_patch(circle)\n    ax.text(x, y-1.3, f'$P(B_{i+1})={P_B[i]:.2f}$', ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw A as a large ellipse overlapping all B's\nellipse = Ellipse((3, 0), width=8, height=3, angle=0, facecolor='#1f77b4', edgecolor='black', alpha=0.2)\nax.add_patch(ellipse)\nax.text(3, 1.2, '$A$', ha='center', va='center', fontsize=14, fontweight='bold')\n\n# Annotate intersections P(A ∩ Bi)\nfor i, (x, y) in enumerate(positions_B):\n    ax.text(x, 0, f'$P(A\\\\cap B_{i+1})={P_A_and_B[i]:.2f}$', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n\nax.set_xlim(-1, 7)\nax.set_ylim(-2, 2)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#law-of-total-probability-example",
    "href": "files/lecture_notes/test_lectures/lecture6.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\n\nA factory has two machines:\n\nMachine 1: Produces 60% of items, 5% defective\nMachine 2: Produces 40% of items, 3% defective\n\nQ: What’s the overall probability an item is defective?\n\n\n\nSolution. \\(P(\\text{defective}) = P(D|M_1)P(M_1) + P(D|M_2)P(M_2)\\)\n\\(= 0.05 \\times 0.6 + 0.03 \\times 0.4 = 0.03 + 0.012 = 0.042\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem",
    "href": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n🎯 Definition \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nThis allows us to “reverse” conditional probabilities\nNamed after Thomas Bayes (1701-1761)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-components",
    "href": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-components",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\n\\(A,B\\): Events\n\\(P(A|B)\\): Posterior probability - what we want to find\n\\(P(B|A)\\): Likelihood - given \\(A\\), probability of observing \\(B\\)\n\\(P(A)\\): Prior probability - initial probability of \\(A\\)\n\\(P(B)\\): Marginal probability - total probability of \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-example",
    "href": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Example",
    "text": "Bayes’ Theorem Example\n\n\n\nMedical test for a disease: 1\n\nDisease affects 1% of population\nTest is 95% accurate for sick people\nTest is 90% accurate for healthy people\n\nQ:If someone tests positive, what’s the probability they have the disease?\n\n\n\nimport plotly.express as px\nimport pandas as pd\n\n# Given probabilities\nP_D = 0.01\nP_notD = 0.99\nP_pos_given_D = 0.95\nP_pos_given_notD = 0.10\n\n# Joint probabilities\nP_D_and_pos = P_D * P_pos_given_D       # True positives\nP_notD_and_pos = P_notD * P_pos_given_notD  # False positives\nP_D_and_neg = P_D * (1 - P_pos_given_D)     # False negatives\nP_notD_and_neg = P_notD * (1 - P_pos_given_notD)  # True negatives\n\n# Prepare DataFrame for treemap (mosaic)\ndf = pd.DataFrame({\n    'Test Result': ['Positive', 'Positive', 'Negative', 'Negative'],\n    'Condition':   ['Disease',   'No Disease', 'Disease',   'No Disease'],\n    'Probability': [P_D_and_pos, P_notD_and_pos, P_D_and_neg, P_notD_and_neg]\n})\n\n# Create treemap mosaic plot\nfig = px.treemap(\n    df,\n    path=['Test Result', 'Condition'],\n    values='Probability',\n    color='Condition',\n    color_discrete_map={'Disease':'tomato', 'No Disease':'skyblue'}\n)\nfig.update_traces(textinfo='label+percent entry')\nfig.update_layout(\n    title='Bayes’ Theorem: Distribution by Test Result and Condition',\n    margin=dict(t=50, l=25, r=25, b=25)\n)\nfig.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-solution",
    "href": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-solution",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution",
    "text": "Bayes’ Theorem Solution\nLet:\n\n\\(D\\): Person has disease\n\\(T^+\\): Test is positive\n\nGiven:\n\n\\(P(D) = 0.01\\)\n\\(P(T^+|D) = 0.95\\)\n\\(P(T^-|D^c) = 0.90\\), so \\(P(T^+|D^c) = 0.10\\)\n\n\n\nSolution. \\(P(T^+) = P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\)\n\\(= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.1085\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-solution-cont.",
    "href": "files/lecture_notes/test_lectures/lecture6.html#bayes-theorem-solution-cont.",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution (cont.)",
    "text": "Bayes’ Theorem Solution (cont.)\n\\[P(D|T^+) = \\frac{P(T^+|D) \\times P(D)}{P(T^+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\]\n\nimport matplotlib.pyplot as plt\n\n# Posterior probabilities given a positive test\nP_D_and_pos = 0.0095\nP_notD_and_pos = 0.099\nP_positive = P_D_and_pos + P_notD_and_pos\n\nP_D_given_pos = P_D_and_pos / P_positive\nP_notD_given_pos = P_notD_and_pos / P_positive\n\n# Pie chart\nlabels = ['Disease (P≈8.8%)', 'No Disease (P≈91.2%)']\nsizes = [P_D_given_pos, P_notD_given_pos]\ncolors = ['tomato', 'skyblue']\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(\n    sizes, labels=labels, colors=colors, autopct='%.1f%%',\n    startangle=90, textprops={'fontsize': 14, 'fontweight': 'bold'}\n)\nax.set_title('Posterior Probability Given Positive Test', fontsize=16, fontweight='bold')\nax.axis('equal')  # Equal aspect ensures pie is circular.\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSurprising result: Even with a positive test, there’s only an 8.8% chance of having the disease!\nThis is due to the low base rate of the disease"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#common-probability-mistakes",
    "href": "files/lecture_notes/test_lectures/lecture6.html#common-probability-mistakes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nConfusing \\(P(A|B)\\) with \\(P(B|A)\\)\n\n\nProsecutor’s fallacy is a specific error in interpreting conditional probabilities. Confusing\n\\(P(\\text{Evidence}\\mid\\text{Innocent})\n\\quad\\text{with}\\quad\nP(\\text{Innocent}\\mid\\text{Evidence})\\).\nEx: OJ Simpson Case 2"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#common-probability-mistakes-1",
    "href": "files/lecture_notes/test_lectures/lecture6.html#common-probability-mistakes-1",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nAssuming independence when events are dependent\nIgnoring base rates (as in the medical test example)\n\n\nBase rate fallacy is when you ignore or underweight the prior probability \\(P(H)\\) of a hypothesis, focusing only on the new evidence \\(E\\).\n\n\nDouble counting in union calculations"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-3",
    "href": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-3",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nTwo fair dice are rolled. Find:\n\n\\(P(\\text{sum} = 7)\\)\n\\(P(\\text{sum} = 7 | \\text{first die shows 3})\\)\nAre these events independent?\n\n\n\n\nSolution. \n\n6 ways out of 36: \\(P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\nGiven first die is 3, need second die to be 4: \\(P(\\text{sum} = 7 | \\text{first} = 3) = \\frac{1}{6}\\)\nYes, they’re independent since \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#counting-and-probability",
    "href": "files/lecture_notes/test_lectures/lecture6.html#counting-and-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\nSometimes we need to count outcomes:\n\nMultiplication Principle: If task 1 can be done in \\(m\\) ways and task 2 in \\(n\\) ways, both can be done in \\(m \\times n\\) ways\n\n\nPermutations: Arrangements where order matters \\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\nCombinations: Selections where order doesn’t matter \\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#counting-example",
    "href": "files/lecture_notes/test_lectures/lecture6.html#counting-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting Example",
    "text": "Counting Example\n\nQ: How many ways can you arrange 5 people in a row?\n\n\n\nSolution. This is a permutation: \\(P(5,5) = 5! = 120\\) ways\n\n\n\n\n\nQ:How many ways can you choose 3 people from 5 for a committee?\n\n\n\n\nSolution. This is a combination: \\(C(5,3) = \\binom{5}{3} = \\frac{5!}{3!2!} = 10\\) ways"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#probability-with-counting",
    "href": "files/lecture_notes/test_lectures/lecture6.html#probability-with-counting",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Probability with Counting",
    "text": "Probability with Counting\n\nExample: A committee of 3 people is chosen from 8 people (5 women, 3 men). What’s the probability all 3 are women?\n\n\n\nSolution. Total ways to choose 3 from 8: \\(\\binom{8}{3} = 56\\)\nWays to choose 3 women from 5: \\(\\binom{5}{3} = 10\\)\nProbability: \\(\\frac{10}{56} = \\frac{5}{28}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#real-world-applications",
    "href": "files/lecture_notes/test_lectures/lecture6.html#real-world-applications",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Diagnosis: Using Bayes’ theorem for test interpretation\nQuality Control: Probability of defective items\nFinance: Risk assessment and portfolio theory\nSports: Probability of wins, fantasy sports\nInsurance: Calculating premiums based on risk"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#key-formulas-summary",
    "href": "files/lecture_notes/test_lectures/lecture6.html#key-formulas-summary",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\nBasic probability: \\(P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\)\nComplement: \\(P(A^c) = 1 - P(A)\\)\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nConditional: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nIndependence: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nBayes’: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#problem-solving-strategy",
    "href": "files/lecture_notes/test_lectures/lecture6.html#problem-solving-strategy",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\nIdentify the sample space and events\nDetermine if events are independent or mutually exclusive\nChoose the appropriate rule or formula\nCalculate step by step\nCheck if your answer makes sense"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-4",
    "href": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-4",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA bag contains 4 red, 3 blue, and 2 green marbles. Three marbles are drawn without replacement.\nFind the probability that: a) All three are red b) No two are the same color c) At least one is blue"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-4-solutions",
    "href": "files/lecture_notes/test_lectures/lecture6.html#practice-problem-4-solutions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4 Solutions",
    "text": "Practice Problem 4 Solutions\n\nSolution. \n\nAll red: \\(\\frac{4}{9} \\times \\frac{3}{8} \\times \\frac{2}{7} = \\frac{24}{504} = \\frac{1}{21}\\)\nDifferent colors: \\(\\frac{4 \\times 3 \\times 2}{9 \\times 8 \\times 7} \\times 3! = \\frac{24 \\times 6}{504} = \\frac{144}{504} = \\frac{2}{7}\\)\nAt least one blue: \\(1 - P(\\text{no blue}) = 1 - \\frac{6 \\times 5 \\times 4}{9 \\times 8 \\times 7} = 1 - \\frac{120}{504} = \\frac{384}{504} = \\frac{16}{21}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#common-questions",
    "href": "files/lecture_notes/test_lectures/lecture6.html#common-questions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Questions",
    "text": "Common Questions\n\nQ1.: “Why isn’t \\(P(A \\cup B) = P(A) + P(B)\\) always?”\nA: We’d double-count outcomes in both events\nQ2.: “How do I know if events are independent?”\nA: Check if \\(P(A|B) = P(A)\\) or if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nQ3.: “When do I use Bayes’ theorem?”\nA: When you want to “reverse” a conditional probability\n\n\nQ3 note (Bayes Example)\n\nForward: I know my test picks up disease 95% of the time ⇒ \\(P(+\\mid D)=0.95\\).\nReverse: I want the chance I really have the disease when the test is positive ⇒ \\(P(D\\mid +)\\)."
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#looking-ahead",
    "href": "files/lecture_notes/test_lectures/lecture6.html#looking-ahead",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture:\n\nRandom Variables and Probability Distributions\nDiscrete vs. continuous random variables\nExpected value and variance"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#final-thoughts",
    "href": "files/lecture_notes/test_lectures/lecture6.html#final-thoughts",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nProbability is the foundation of statistics:\n\nHelps us quantify uncertainty\nProvides tools for making decisions with incomplete information\nEssential for understanding statistical inference\n\n\n\nPractice: The key to mastering probability is working through many problems!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#questions",
    "href": "files/lecture_notes/test_lectures/lecture6.html#questions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Random Variables and Distributions"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#resources",
    "href": "files/lecture_notes/test_lectures/lecture6.html#resources",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture6.html#footnotes",
    "href": "files/lecture_notes/test_lectures/lecture6.html#footnotes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)↩︎\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent.↩︎"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-combinations-formula",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-combinations-formula",
    "title": "PSTAT 5A: Counting",
    "section": "Combinations Formula",
    "text": "Combinations Formula\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(C(n,r)\\) or \\(\\binom{n}{r}\\): Number of ways to choose \\(r\\) objects from \\(n\\) distinct objects (order doesn’t matter)\n\\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]\n\\(\\binom{n}{r}\\) reads “\\(n\\) choose \\(r\\)”\n\n\n\n\n\nHow many ways can we choose 3 people from a group of 8 for a committee?\n\nSolution. \\(C(8,3) = \\frac{8!}{3!(8-3)!} = \\frac{8!}{3! \\times 5!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 56\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#relationship-permutations-vs-combinations",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#relationship-permutations-vs-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Relationship: Permutations vs Combinations",
    "text": "Relationship: Permutations vs Combinations\n\n\n\n\n\n\n\n\n\nRelationship\n\n\n\n\\(P(n,r) = \\frac{n!}{(n-r)!} \\quad (1)\\)\n\nand,\n\n\\(C(n,r) = \\frac{n!}{(n-r)!} \\quad (2)\\)\n\nPlugging (2) into (1) and multiplying by \\(r\\) which represents the number of arrangements we get :\n\\(P(n,r) = C(n,r) \\times r!\\) (multiply by arrangements)\nWhy? For each combination of \\(n\\) objects, there are \\(r!\\) ways to arrange them\nSimilarly,\n\\(C(n,r) = P(n,r)/ r!\\) (divide out arrangements)\nWhy? diving by \\(r\\) removes the arrangements from our formula and leaves us with the number of selection possible from \\(n\\) objects.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPermutations = Combinations × Arrangements\n\nCombinations answer: “How many ways can I choose?”\n\nArrangements answer: “How many ways can I order what I chose?”*\n\nPermutations answer: “How many ways can I choose AND order?”\n\n\\(\\text{Choose} \\times \\text{Arrange} = \\text{Choose and Arrange}\\)\n\\(C(n,r)×r!=P(n,r)C(n,r) \\times r! = P(n,r)C(n,r)×r!=P(n,r)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-permutation-vs-combination",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-permutation-vs-combination",
    "title": "PSTAT 5A: Counting",
    "section": "Key Decision: Permutation or Combination?",
    "text": "Key Decision: Permutation or Combination?\n\n\n\n\n\n\n\n\n\nHow to Decide\n\n\nAsk yourself: Does order matter?\nOrder matters → Use Permutations\n\nArrangements, sequences, rankings\n\nOrder doesn’t matter → Use Combinations\n\nSelections, groups, subsets\n\n\n\n\n\n\n\n\nNote\n\n\n✅ \\(P(n,r)\\) = counts both selection & arrangement → grows faster\n✅ \\(C(n,r)\\) = counts only selection → grows slower\n✅ The difference comes from \\(r!\\), which is big even for modest \\(r\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#practice-3",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#practice-3",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nFrom a class of 20 students:\n\nHow many ways to choose 5 students for a study group?\nHow many ways to choose a president, vice-president, and secretary?\n\n\n\nSolution. \n\n\\(C(20,5) = \\frac{20!}{5! \\times 15!} = 15,504\\) (order doesn’t matter)\n\\(P(20,3) = \\frac{20!}{17!} = 6,840\\) (order matters)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#properties-of-combinations",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#properties-of-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Properties of Combinations",
    "text": "Properties of Combinations\n\n\n\n\n\n\n\nProperties\n\n\n\nSymmetry: \\(\\binom{n}{r} = \\binom{n}{n-r}\\)\nPascal’s Identity: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nBoundary conditions: \\(\\binom{n}{0} = \\binom{n}{n} = 1\\)\n\n\n\n\n\n\n\\(\\binom{8}{3} = \\binom{8}{5} = 56\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#pascals-triangle",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#pascals-triangle",
    "title": "PSTAT 5A: Counting",
    "section": "Pascal’s Triangle",
    "text": "Pascal’s Triangle\n\n\n\n\n\n\n\n\n\n\nPascal’s Triangle\n\n\n       1                    ← (x + y)⁰\n     1   1                  ← (x + y)¹\n   1   2   1                ← (x + y)²\n 1   3   3   1              ← (x + y)³\n1   4   6   4   1           ← (x + y)⁴\n1  5  10  10  5  1          ← (x + y)⁵\nEach number equals \\(\\binom{n}{r}\\) where \\(n\\) is the row number and \\(r\\) is the position from the left (starting at 0).\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nPattern: Each number is the sum of the two numbers above it.\nFormula: \\(\\binom{n}{r} = \\binom{n-1}{r-1} + \\binom{n-1}{r}\\)\nExample: \\(\\binom{4}{2} = 6\\) (row 4, position 2)\n\n\n\n\n\nRow 3:       1   3   3   1\n            ↙ ↘ ↙ ↘ ↙ ↘ ↙ ↘\nRow 4:     1   4   6   4   1"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#binomial-theorem",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#binomial-theorem",
    "title": "PSTAT 5A: Counting",
    "section": "Binomial Theorem",
    "text": "Binomial Theorem\n\n\n\n\n\n\n\n\n\nKey Formula\n\n\n\\[(x + y)^n = \\sum_{r=0}^{n} \\binom{n}{r} x^{n-r} y^r\\]\nThis formula tells us how to expand \\((x + y)\\) raised to any positive integer power \\(n\\).\n\n\n\n\n\nEXAMPLE\n\\((x + y)^3 = \\binom{3}{0}x^3 + \\binom{3}{1}x^2y + \\binom{3}{2}xy^2 + \\binom{3}{3}y^3\\)\n\\(= x^3 + 3x^2y + 3xy^2 + y^3\\)\n\n\n\n\n\n\n\n\n\nKey Insights\n\n\nThe General Pattern\nPowers decrease and increase systematically:\n\nPowers of \\(x\\): \\(n, n-1, n-2, \\ldots, 1, 0\\)\nPowers of \\(y\\): \\(0, 1, 2, \\ldots, n-1, n\\)\nSum of powers in each term: always equals \\(n\\)\n\nCoefficients come from Pascal’s Triangle:\n\nCoefficient of \\(x^{n-r}y^r\\) is \\(\\binom{n}{r}\\)\nRead directly from row \\(n\\) of Pascal’s Triangle\n\nSymmetry in coefficients:\nFirst and last terms have coefficient 1 Coefficients are symmetric: \\(\\binom{n}{0} = \\binom{n}{n}\\), \\(\\binom{n}{1} = \\binom{n}{n-1}\\), etc."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-counting-probability",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-counting-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\n\n\nA committee of 4 people is chosen from 7 women and 5 men. What’s the probability that exactly 2 are women?\n\nTotal ways to choose 4 from 12: \\(\\binom{12}{4} = 495\\)\nWays to choose 2 women from 7: \\(\\binom{7}{2} = 21\\)\nWays to choose 2 men from 5: \\(\\binom{5}{2} = 10\\)\n\n\n\nSolution. Favorable outcomes: \\(\\binom{7}{2} \\times \\binom{5}{2} = 21 \\times 10 = 210\\)\nProbability: \\(\\frac{210}{495} = \\frac{14}{33}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#practice-4",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#practice-4",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\n\n\nA standard deck has 52 cards. What’s the probability that a 5-card hand contains:\n\nExactly 3 aces? \\(P(\\text{exactly 3 aces in 5 cards})\\)\n\n\n\nAt least 1 ace? \\(P(\\text{at least one ace})\\)\n\n\n\n\n\nSolution. \n\nWays to pick 5 cards with zero aces All 5 come from the 48 non-aces: \\[\\binom{48}{5}\n= \\frac{48\\cdot47\\cdot46\\cdot45\\cdot44}{5\\cdot4\\cdot3\\cdot2\\cdot1}\n= \\frac{205{,}476{,}480}{120}\n= 1{,}712{,}304\\]\nProbability of no aces \\[\nP(\\text{no aces})\n= \\frac{\\binom{48}{5}}{\\binom{52}{5}}\n= \\frac{1{,}712{,}304}{2{,}598{,}960}\n\\approx 0.659 \\quad \\text{or} 65.9 \\% \\]\nSubtract from 1 \\[P(\\text{at least one ace})\n= 1 - P(\\text{no aces})\n= 1 - 0.659\n\\approx 0.341\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#practice-problem-4-continued",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#practice-problem-4-continued",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 4 (continued)",
    "text": "Practice Problem 4 (continued)\n\n\n\n\n\n\n\n\nTip\n\n\n\nHypergeometric formula: \\[\nP(\\text{exactly }k\\text{ successes})\n= \\frac{\\binom{K}{k}\\,\\binom{N-K}{n-k}}{\\binom{N}{n}} \\]\n\nwhere \\(N=52\\), \\(K=4\\) aces, \\(n=5\\) draws, and \\(k\\) is the number of aces you want.\n\nComplement trick: \\(\\;P(\\ge1\\text{ ace}) = 1 - P(0\\text{ aces})\\).\n\nThis structure makes it easy to plug in any “number of successes” you need, or to use the complement when you want “at least one.”\n\n\n\n\n\nHypergeometric in one formula\nFor part (a) you could also write directly:\n\\[P(k=3)\n= \\frac{\\binom{4}{3}\\,\\binom{48}{2}}{\\binom{52}{5}}\n= \\frac{4\\cdot1{,}128}{2{,}598{,}960}\n\\approx 0.001735\\]\nAnd for part (b):\n\\[\nP(\\ge1)\n= 1 - \\frac{\\binom{48}{5}}{\\binom{52}{5}}\n= 1 - 0.6590\n= 0.3410\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-with-restrictions",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-with-restrictions",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Restrictions",
    "text": "Permutations with Restrictions\n\nHow many 6-letter “words” can be formed from the letters A, B, C, D, E, F if:\n\nNo letter is repeated\nA and B must be adjacent\n\n\n\nSolution. Treat AB as a single unit\n\n5 units to arrange: (AB), C, D, E, F → \\(5! = 120\\) ways\nA and B can be arranged within their unit: \\(2! = 2\\) ways\nTotal: \\(5! \\times 2! = 240\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-inclusion-exclusion",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-inclusion-exclusion",
    "title": "PSTAT 5A: Counting",
    "section": "The Inclusion-Exclusion Principle",
    "text": "The Inclusion-Exclusion Principle\n\n\n\nThe Principle of Inclusion–Exclusion lets you count (or find the probability of) the union of several sets by alternately adding and subtracting the sizes of their intersections.\n\n\nFor two sets \\(A\\) and \\(B\\): \\[|A \\cup B| = |A| + |B| - |A \\cap B|\\]\nFor three sets \\(A\\), \\(B\\), and \\(C\\): \\[|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| \\\\\n- |A \\cap C| - |B \\cap C| + |A \\cap B \\cap C|\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#inclusion-exclusion-example",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#inclusion-exclusion-example",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Example",
    "text": "Inclusion-Exclusion Example\n\nHow many integers from 1 to 100 are divisible by 2, 3, or 5?\n\nLet:\n\n\\(A\\) = divisible by 2: \\(|A| = 50\\)\n\\(B\\) = divisible by 3: \\(|B| = 33\\)\n\\(C\\) = divisible by 5: \\(|C| = 20\\)\n\n\n\n\n\n\n\nNote\n\n\n\\(|A \\cap B| = 16\\) (divisible by 6)\n\\(|A \\cap C| = 10\\) (divisible by 10)\n\\(|B \\cap C| = 6\\) (divisible by 15)\n\\(|A \\cap B \\cap C| = 3\\) (divisible by 30)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#inclusion-exclusion-solution",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#inclusion-exclusion-solution",
    "title": "PSTAT 5A: Counting",
    "section": "Inclusion-Exclusion Solution",
    "text": "Inclusion-Exclusion Solution\n\nSolution. \\[|A \\cup B \\cup C| = 50 + 33 + 20 - 16 - 10 - 6 + 3 = 74\\]\nAnswer: 74 integers from 1 to 100 are divisible by at least one of 2, 3, or 5"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#multinomial-coefficients",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#multinomial-coefficients",
    "title": "PSTAT 5A: Counting",
    "section": "Multinomial Coefficients",
    "text": "Multinomial Coefficients\n\n\n\n\n\n\n\nMultinomial Coefficient\n\n\nNumber of ways to divide \\(n\\) objects into groups of sizes \\(n_1, n_2, \\ldots, n_k\\):\n\\[\\binom{n}{n_1, n_2, \\ldots, n_k} = \\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\]\n\n\n\n\n\nHow many ways can 12 people be divided into 3 teams of 4?\n\\(\\binom{12}{4,4,4} = \\frac{12!}{4! \\times 4! \\times 4!} = 34,650\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-problem-solving-strategy",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-problem-solving-strategy",
    "title": "PSTAT 5A: Counting",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\n\n\n\n\n\nStrategy\n\n\n\nRead carefully: What exactly are we counting?\nIdentify the type: Permutation, combination, or other?\nCheck for restrictions: Are there constraints?\nDoes order matter?: This determines permutation vs combination\nBreak down complex problems: Use multiplication principle\nVerify your answer: Does it make sense?"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#common-mistakes-to-avoid",
    "title": "PSTAT 5A: Counting",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\n\n\n\n\n\n\nCommon Mistakes\n\n\n\nConfusing permutations and combinations\n\nAlways ask: “Does order matter?”\n\nForgetting about restrictions\n\nRead the problem carefully\n\nDouble counting\n\nMake sure you’re not counting the same arrangement twice\n\nNot considering the complement\n\nSometimes “at least” problems are easier using complements"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#practice-6",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#practice-6",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\n\nA class has 15 students: 8 women and 7 men. How many ways can we:\n\nForm a committee of 5 people with exactly 3 women?\nArrange 6 people in a row with alternating genders (starting with a woman)?\n\n\n\nSolution. \n\n\\(\\binom{8}{3} \\times \\binom{7}{2} = 56 \\times 21 = 1,176\\)\nChoose 3 women from 8: \\(P(8,3) = 336\\) Choose 3 men from 7: \\(P(7,3) = 210\\) Total: \\(336 \\times 210 = 70,560\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#counting-in-computer-science",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#counting-in-computer-science",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Computer Science",
    "text": "Counting in Computer Science\n\nPassword Security:\n\n8-character password with letters, digits, symbols\n\\((26 + 26 + 10 + 32)^8 = 94^8 \\approx 6.1 \\times 10^{15}\\)\n\nHash Functions:\n\nDistributing data into buckets\nCollision probability calculations\n\nAlgorithm Analysis:\n\nCounting operations, comparisons\nBig O notation foundations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#counting-in-genetics",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#counting-in-genetics",
    "title": "PSTAT 5A: Counting",
    "section": "Counting in Genetics",
    "text": "Counting in Genetics\n\nDNA Sequences:\n\n4 bases (A, T, G, C)\nGene of length \\(n\\): \\(4^n\\) possible sequences\n\nProtein Folding:\n\nNumber of possible conformations\nCombinatorial explosion\n\nPopulation Genetics:\n\nHardy-Weinberg calculations\nAllele combinations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#real-world-applications",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#real-world-applications",
    "title": "PSTAT 5A: Counting",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nLottery:\n\nPowerball: Choose 5 from 69, then 1 from 26\nOdds: \\(\\frac{1}{\\binom{69}{5} \\times 26} \\approx \\frac{1}{292,000,000}\\)\n\nCryptography:\n\nKey space size determines security\nRSA encryption relies on large number factorization\n\nSports Tournaments:\n\nMarch Madness bracket: \\(2^{63}\\) possible outcomes\nRound-robin tournaments: \\(\\binom{n}{2}\\) games"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#key-formulas-summary",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#key-formulas-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\n\n\n\n\n\n\n\nSummary of Key Formulas\n\n\n\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\nWith repetition: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)\nInclusion-Exclusion: \\(|A \\cup B| = |A| + |B| - |A \\cap B|\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#technology-and-counting",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#technology-and-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Technology and Counting",
    "text": "Technology and Counting\n\n\n\n\n\n\n\nTools\n\n\nCalculators:\n\nUse nPr and nCr functions\nBe careful with large numbers\n\nSoftware:\n\nR: factorial(), choose(), combn()\nPython: math.factorial(), math.comb()\nExcel: FACT(), COMBIN(), PERMUT()\n\nOnline Tools:\n\nWolfram Alpha for complex calculations\nCombination/permutation calculators"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#practice-7",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#practice-7",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\n\nA standard deck of cards is shuffled. What’s the probability that:\n\nThe top 4 cards are all hearts?\nIn a 13-card hand, you get exactly one card from each rank?\n\n\n\nSolution. \n\n\\(\\frac{13}{52} \\times \\frac{12}{51} \\times \\frac{11}{50} \\times \\frac{10}{49} = \\frac{13 \\times 12 \\times 11 \\times 10}{52 \\times 51 \\times 50 \\times 49} \\approx 0.0026\\)\nChoose 1 card from each of 13 ranks: \\(4^{13}\\) Total 13-card hands: \\(\\binom{52}{13}\\) Probability: \\(\\frac{4^{13}}{\\binom{52}{13}} \\approx 6.3 \\times 10^{-6}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#extending-to-probability",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#extending-to-probability",
    "title": "PSTAT 5A: Counting",
    "section": "Extending to Probability",
    "text": "Extending to Probability\n\n\n\n\n\n\n\nDistributions\n\n\nHypergeometric Distribution:\n\nDrawing without replacement\nUses combinations: \\(P(X = k) = \\frac{\\binom{K}{k}\\binom{N-K}{n-k}}{\\binom{N}{n}}\\)\n\nBinomial Distribution:\n\nDrawing with replacement\nUses combinations: \\(P(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\\)\n\nWe’ll explore these distributions in detail in future lectures"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#historical-note",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#historical-note",
    "title": "PSTAT 5A: Counting",
    "section": "Historical Note",
    "text": "Historical Note\n\n\n\n\n\n\n\nHistory\n\n\nBlaise Pascal (1623-1662) and Pierre de Fermat (1601-1665): - Founded probability theory through gambling problems - Pascal’s triangle and combinations\nLeonhard Euler (1707-1783): - Advanced combinatorics - Graph theory connections\nModern applications span computer science, biology, physics, and economics"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#common-student-questions",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#common-student-questions",
    "title": "PSTAT 5A: Counting",
    "section": "Common Student Questions",
    "text": "Common Student Questions\n\nQ: “When do I use permutations vs combinations?”\nA: Ask “Does order matter?” Order matters → permutation\nQ: “How do I handle restrictions?”\nA: Break the problem into cases or use complementary counting\nQ: “What if objects are identical?”\nA: Use the formula for permutations with repetition\nQ: “How do I check my answer?”\nA: Verify with small examples or use different methods"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#looking-ahead",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#looking-ahead",
    "title": "PSTAT 5A: Counting",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\n\n\n\n\n\n\nNext Lecture\n\n\nNext lecture: Discrete Probability Distributions - Binomial distribution (using combinations!)\n\nHypergeometric distribution\nGeometric distribution\nExpected value and variance\n\nConnection: Today’s counting techniques are essential for probability calculations"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#study-tips",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#study-tips",
    "title": "PSTAT 5A: Counting",
    "section": "Study Tips",
    "text": "Study Tips\n\n\n\n\n\n\n\nTips\n\n\n\nPractice, practice, practice: Work through many examples\nIdentify patterns: Learn to recognize problem types\nStart simple: Build up to complex problems\nCheck your work: Use different approaches when possible\nUnderstand concepts: Don’t just memorize formulas"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#learning-summary",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#learning-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Learning Objectives Summary",
    "text": "Learning Objectives Summary\n\n\n\n\n\n\n\nWhat We’ve Covered\n\n\nIn this lecture, we’ve addressed all the learning objectives:\n\n✅ Calculate combinations and understand when to use them: Covered in Section 3 and Section 4\n✅ Distinguish between permutations and combinations: Covered in Section 7\n✅ Use counting techniques to solve probability problems: Covered in Section 12\n✅ Apply the inclusion-exclusion principle: Covered in Section 17\n✅ Solve complex counting problems systematically: Covered in Section 21"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#final-thoughts",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#final-thoughts",
    "title": "PSTAT 5A: Counting",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nCounting is fundamental to:\n\nProbability calculations\nStatistical inference\nComputer algorithms\nScientific modeling\n\n\n\n\n\n\n\n\nKnow the Basics\n\n\nPermutations and combinations are the building blocks for advanced statistical concepts"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#questions",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#questions",
    "title": "PSTAT 5A: Counting",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html",
    "title": "PSTAT 5A: Counting",
    "section": "",
    "text": "The art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#welcome-to-lecture-7",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#learning-objectives",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#learning-objectives",
    "title": "PSTAT 5A: Counting",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\n\n\n\n\n\n\n\nLearning Objectives\n\n\nBy the end of this lecture, you will be able to:\n\nApply the fundamental counting principles (Section 4)\nCalculate permutations with and without repetition (Section 8, Section 11, Section 15)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#why-study-counting",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#why-study-counting",
    "title": "PSTAT 5A: Counting",
    "section": "Why Study Counting?",
    "text": "Why Study Counting?\n\n\n\n\n\nCounting helps us:\n\nCalculate probabilities for complex events\n\nSolve optimization problems\n\nUnderstand combinations in genetics, computer science\n\nAnalyze algorithms and data structures\n\nMake decisions involving arrangements and selections\n\n\n\nReal-world applications of counting include:\n\nCryptography: Password strength and encryption key space\nGenetics: DNA sequence analysis and gene combinations\n\nTournament brackets: March Madness and sports competitions\nLottery odds: Probability calculations for games of chance\nPassword security: Character combinations and brute force protection"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#sec-fundamental-counting",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#sec-fundamental-counting",
    "title": "PSTAT 5A: Counting",
    "section": "The Fundamental Counting Principle",
    "text": "The Fundamental Counting Principle\n\n\n\n\n\n\n\n\n\nMultiplication Rule\n\n\nIf a procedure consists of \\(k\\) separate tasks where:\n\nTask 1 can be performed in \\(n_1\\) ways\nTask 2 can be performed in \\(n_2\\) ways\n…\nTask \\(k\\) can be performed in \\(n_k\\) ways\n\nThen, the entire procedure can be performed in \\(n_1 \\times n_2 \\times \\cdots \\times n_k\\) ways\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {'flowchart': {'nodeSpacing': 20, 'rankSpacing': 50}}}%%\nflowchart TD\n    Start([🟢 Start]) --&gt; T1[📋 Task 1&lt;br/&gt;n₁ ways]\n    T1 --&gt; C1[Choice 1]\n    T1 --&gt; C2[Choice 2]\n    T1 --&gt; Cn1[Choice n₁]\n    \n    C1 --&gt; T2[📋 Task 2&lt;br/&gt;n₂ ways]\n    C2 --&gt; T2\n    Cn1 --&gt; T2\n    \n    T2 --&gt; C21[Choice 1]\n    T2 --&gt; C22[Choice 2]\n    T2 --&gt; C2n[Choice n₂]\n    \n    C21 --&gt; Total[🎯 Total ways&lt;br/&gt;n₁ × n₂ × ... × nₖ]\n    C22 --&gt; Total\n    C2n --&gt; Total\n    \n    classDef start fill:#d4edda,stroke:#155724,stroke-width:3px\n    classDef task fill:#d1ecf1,stroke:#0c5460,stroke-width:2px\n    classDef choice fill:#fff3cd,stroke:#856404,stroke-width:1px\n    classDef total fill:#f8d7da,stroke:#721c24,stroke-width:3px\n    \n    class Start start\n    class T1,T2 task\n    class C1,C2,Cn1,C21,C22,C2n choice\n    class Total total"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#simple-counting-example",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#simple-counting-example",
    "title": "PSTAT 5A: Counting",
    "section": "Simple Counting Example",
    "text": "Simple Counting Example\n\n\n\nFormat: ABC-123\n\\[\n\\underbrace{A \\; B \\; \\_ \\; \\ - \\_ \\; \\_ \\; \\_}_{positions}\n\\]\n\nFirst position: 26 letters\nSecond position: 26 letters\nThird position: 26 letters\nFourth position: 10 digits\nFifth position: 10 digits\nSixth position: 10 digits\n\n\n\n\n\nSolution. Total possibilities: \\(26 \\times 26 \\times 26 \\times 10 \\times 10 \\times 10 = 26^3 \\times 10^3 = 17,576,000\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#restaurant-menu-example",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#restaurant-menu-example",
    "title": "PSTAT 5A: Counting",
    "section": "Restaurant Menu Example",
    "text": "Restaurant Menu Example\n\nA restaurant offers:\n🍤 Appetizers: 4\n🍲 Main Courses: 6\n🍰 Desserts: 3\nHow many different three-course meals are possible?\n\n\nSolution. \\(4 \\times 6 \\times 3 = 72\\) different meals"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#practice-1",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#practice-1",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\n\nA password must contain:\n\nExactly 8 characters\nEach character is either a letter (26 possibilities) or digit (10 possibilities)\n\nHow many possible passwords are there?\n\n\n\n\n\n\n\nSolution. Each position has \\(26 + 10 = 36\\) choices.\nTotal: \\(36^8 = 2,821,109,907,456\\) passwords"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Permutations?",
    "text": "What Are Permutations?\n\n\n\n\n\n\n\n\n\nPermutation\n\n\nAn arrangement of objects where order matters\n\n\n\n\n\n\n\n\n\n\n\nOrder Matters\n\n\n\nRace finish positions (1st, 2nd, 3rd)\nSeating arrangements\nPasswords\nDNA sequences\n\n\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll permutations of ABC:\n1. ABC\n2. ACB\n3. BAC\n4. BCA\n5. CAB\n6. CBA"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#permutations-of-n-distinct-objects",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#permutations-of-n-distinct-objects",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(n\\) Distinct Objects",
    "text": "Permutations of \\(n\\) Distinct Objects\n\n\n\n\n\n\n\n\n\nKey Formula\n\n\nThe number of ways to arrange \\(n\\) distinct objects is:\n\\[n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\]\n\n\n\n\n\n\n\n\n\n\n\nSeating Process:\n1st position: 5 choices (Alice, Bob, Carol, David, Eve)\n2nd position: 4 choices (whoever is left)\n3rd position: 3 choices (whoever is left)\n4th position: 2 choices (whoever is left)\n5th position: 1 choice (last person)\nHow many ways can 5 people sit in a row?\n\n\nSolution. \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\) ways"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#factorial-values",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#factorial-values",
    "title": "PSTAT 5A: Counting",
    "section": "Factorial Values",
    "text": "Factorial Values\n\n\n\n\n\n\\(n\\)\n\\(n!\\)\n\n\n\n\n0\n1\n\n\n1\n1\n\n\n2\n2\n\n\n3\n6\n\n\n4\n24\n\n\n5\n120\n\n\n10\n3,628,800\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nNote\n\n\n\\(0! = 1\\) by definition"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations-r-from-n",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations-r-from-n",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations of \\(r\\) Objects from \\(n\\)",
    "text": "Permutations of \\(r\\) Objects from \\(n\\)\n\n\n\n\n\n\n\n\n\nKey Formula\n\n\n\\(P(n,r)\\) or \\(_nP_r\\): Number of ways to arrange \\(r\\) objects selected from \\(n\\) distinct objects\n\\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\\[\nP_{k,n} =\n\\frac{n!}{(n-k)!}\n=\n\\]\n\\[\n= \\frac{\n  n(n-1)\\cdots(n-k+1)\\,\n  \\overbrace{(n-k)(n-k-1)\\cdots3\\cdot2\\cdot1}^{(n-k)!}\n}{\n  (n-k)!\n}\n\\]\n\\[\n= \\underbrace{\nn (n-1) (n-2) \\cdots (n-k+1)\n}_{k \\text{ terms}}\n\\]\nFill in \\(k\\) slots with no repetitions\n\\[\n\\underbrace{n \\; (n-1) \\; \\_ \\; \\_ \\; \\cdots}_{k}\n\\]\nNote that if we allowed repetitions we would get \\(n^k\\) \\[\n\\underbrace{n \\; n \\; n \\; \\cdots \\; n}_{k}\n\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#understanding-pnr",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#understanding-pnr",
    "title": "PSTAT 5A: Counting",
    "section": "Understanding \\(P(n,r)\\)",
    "text": "Understanding \\(P(n,r)\\)\nWhy is \\(P(n,r) = \\frac{n!}{(n-r)!}\\)?\n\nFirst position: \\(n\\) choices\nSecond position: \\((n-1)\\) choices\n\nThird position: \\((n-2)\\) choices\n…\n\\(r\\)-th position: \\((n-r+1)\\) choices\n\n\nTotal: \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times (n-r+1) = \\frac{n!}{(n-r)!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#practice-2",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#practice-2",
    "title": "PSTAT 5A: Counting",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\nA baseball team has 15 players. How many ways can the coach:\n\nArrange all 15 players in a line?\nChoose and arrange 9 players for the starting lineup (batting order matters)?\n\n\n\n\nSolution. \n\n\\(15! = 1,307,674,368,000\\)\n\\(P(15,9) = \\frac{15!}{6!} = 1,816,214,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations-repetition",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#sec-permutations-repetition",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition",
    "text": "Permutations with Repetition\n\n\n\n\n\n\n\nPermutations with Repetition\n\n\nWhen some objects are identical, we have fewer distinct arrangements\nIf we have \\(n\\) objects where:\n\n\\(n_1\\) are of type 1\n\\(n_2\\) are of type 2\n…\n\\(n_k\\) are of type \\(k\\)\n\nNumber of distinct arrangements: \\(\\frac{n!}{n_1! \\times n_2! \\times \\cdots \\times n_k!}\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#permutations-with-repetition-example",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#permutations-with-repetition-example",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations with Repetition Example",
    "text": "Permutations with Repetition Example\n\n\n\nHow many distinct arrangements are there of the letters in “STATISTICS”? \n\n\n\n\n\n\n\nTip\n\n\nS-T-A-T-I-S-T-I-C-S\n\nTotal letters: 10\nS appears 3 times\nT appears 3 times\nA appears 1 time\nI appears 2 times\nC appears 1 time\n\n\n\n\n\n\nSolution. \\(\\frac{10!}{3! \\times 3! \\times 1! \\times 2! \\times 1!} = \\frac{3,628,800}{6 \\times 6 \\times 1 \\times 2 \\times 1} = \\frac{3,628,800}{72} = 50,400\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#sec-combinations",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#sec-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\n\n\n\n\n\n\n\n\n\nCombination\n\n\nA selection of objects where order does NOT matter\n\n\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\n\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\nKey Point: Order doesn't matter for combinations\n\n\n\nChoosing committee members\nSelecting pizza toppings\nForming study groups\nLottery number selection\n\n\n\n\n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#questions",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#questions",
    "title": "PSTAT 5A: Counting",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Counting continued"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#sec-objectives",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#sec-objectives",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine probability and understand its basic properties\nIdentify sample spaces and events\nApply fundamental probability rules\nCalculate conditional probabilities\nDetermine when events are independent\nUse Bayes’ theorem in simple applications"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#mutually-exclusive-vs.-independent",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#mutually-exclusive-vs.-independent",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent",
    "text": "Mutually Exclusive vs. Independent\n\nMutually Exclusive (left): the circles A and B do not overlap, so \\(P(A\\cap B)=0\\).\nIndependent (right): the circles overlap, and we’ve sized the intersection so that \\(P(A\\cap B)=P(A)\\,P(B)\\).\n\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Create side-by-side Venn diagrams\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# 1) Mutually Exclusive: no overlap\nvenn2(\n    subsets=(1, 1, 0),\n    set_labels=('A', 'B'),\n    ax=axes[0],\n    subset_label_formatter=lambda x: ''\n)\naxes[0].set_title('Mutually Exclusive\\nP(A∩B) = 0')\n\n# 2) Independent: overlap equals product of areas (e.g., 0.5 * 0.5 = 0.25)\n# Scale counts arbitrarily (25, 25, 25) to represent proportions\nvenn2(\n    subsets=(25, 25, 25),\n    set_labels=('A', 'B'),\n    ax=axes[1],\n    subset_label_formatter=lambda x: ''\n)\naxes[1].set_title('Independent\\nP(A∩B) = P(A)P(B)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#mutually-exclusive-vs.-independent-example",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#mutually-exclusive-vs.-independent-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Mutually Exclusive vs. Independent Example",
    "text": "Mutually Exclusive vs. Independent Example\n\n\n\nDraw a single card from a 52-card deck:\n\nLet A={“draw an Ace”}, so P(A)=4/52.\nLet B={“draw a King”}, so P(B)=4/52.\n\nQ: What is \\(P(A\\cap B)\\) ?\n\n\n\n\n\nSolution. They’re disjoint (you can’t draw an Ace and a King), so \\(P(A\\cap B) = 0\\).\nBut \\(P(A)\\,P(B) = \\frac{4}{52}\\times\\frac{4}{52} = \\frac{16}{2704} \\neq 0\\).\nHence, \\(P(A\\cap B)\\neq P(A)P(B)\\), so they’re not independent."
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#multiplication-rule",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#multiplication-rule",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\nGeneral case: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nIndependent events: \\(P(A \\cap B) = P(A) \\times P(B)\\)\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib_venn import venn2\n\n# Define scaled subset sizes for illustration\n# General case: P(A)=40 (10+30), P(B|A)=0.75 so intersection=30, B-only=20 (if P(B)=50)\n# Independent case: P(A)=40, P(B)=50, intersection=P(A)*P(B)=20\ngeneral_subsets = (10, 20, 30)   # (A-only, B-only, intersection)\nindep_subsets = (20, 30, 20)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- General Multiplication Rule ---\nvenn2(subsets=general_subsets, set_labels=('A', 'B'), ax=axes[0])\naxes[0].set_title('General: P(A∩B) = P(A)·P(B|A)', fontsize=14)\n\n# --- Independent Events ---\nvenn2(subsets=indep_subsets, set_labels=('A', 'B'), ax=axes[1])\naxes[1].set_title('Independent: P(A∩B) = P(A)·P(B)', fontsize=14)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#tree-diagrams",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#tree-diagrams",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\n\n🎯 Definition Tree diagrams help visualize sequential events and calculate probabilities.\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes\ncoords = {\n    'root': (0.1, 0.5),\n    'A': (0.4, 0.7),\n    'B': (0.4, 0.3),\n    'A_Red': (0.7, 0.8),\n    'A_Blue': (0.7, 0.6),\n    'B_Red': (0.7, 0.4),\n    'B_Blue': (0.7, 0.2),\n}\n\n# Probabilities\np_A = 0.7\np_B = 0.3\np_R_A = 0.6\np_B_A = 0.4\np_R_B = 0.3\np_B_B = 0.7\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8, 6))\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.axis('off')\n\n# Draw nodes\nfor node, (x, y) in coords.items():\n    circle = Circle((x, y), 0.05, edgecolor='black', facecolor='white', linewidth=2)\n    ax.add_patch(circle)\n    label = node.replace('_', '\\n')\n    if node == 'root':\n        label = 'Start'\n    ax.text(x, y, label, ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw arrows and annotate probabilities\ndef draw_branch(start, end, text):\n    x1, y1 = coords[start]\n    x2, y2 = coords[end]\n    arrow = FancyArrowPatch((x1+0.05, y1), (x2-0.05, y2), arrowstyle='-&gt;', mutation_scale=20)\n    ax.add_patch(arrow)\n    ax.text((x1+x2)/2, (y1+y2)/2, text, fontsize=12, backgroundcolor='white', ha='center')\n\ndraw_branch('root', 'A', f'{p_A}')\ndraw_branch('root', 'B', f'{p_B}')\ndraw_branch('A', 'A_Red', f'{p_R_A}')\ndraw_branch('A', 'A_Blue', f'{p_B_A}')\ndraw_branch('B', 'B_Red', f'{p_R_B}')\ndraw_branch('B', 'B_Blue', f'{p_B_B}')\n\n# Title\nax.set_title('Tree Diagram: Drawing from Urn A (70%) or B (30%)', fontsize=14, pad=20)\n\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#tree-diagram-examples",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#tree-diagram-examples",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Tree Diagram Examples",
    "text": "Tree Diagram Examples\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, FancyArrowPatch\n\n# Coordinates for nodes in the full tree\ncoords = {\n    'root':       (0.1, 0.5),\n    'A':          (0.35, 0.75),\n    'NotA':       (0.35, 0.25),\n    'A_B':        (0.7, 0.8),\n    'A_notB':     (0.7, 0.65),\n    'NotA_B':     (0.7, 0.35),\n    'NotA_notB':  (0.7, 0.2)\n}\n\n# Probabilities (example values)\npA     = 0.4\npNotA  = 0.6\npB_A   = 0.75\npNotB_A= 0.25\npB_notA= 0.333\npNotB_notA=0.667\n\n# Colors\ncolor_intersection = '#1f77b4'  # blue\ncolor_B            = '#ff7f0e'  # orange\ngray               = 'lightgray'\n\ndef draw_tree(highlight_branches, branch_color, node_to_color):\n    fig, ax = plt.subplots(figsize=(6, 4))\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.axis('off')\n\n    # Draw nodes\n    node_patches = {}\n    for node, (x, y) in coords.items():\n        circ = Circle((x, y), 0.035, edgecolor='black', facecolor='white', lw=2)\n        ax.add_patch(circ)\n        node_patches[node] = circ\n        label = {\n            'root': 'Start',\n            'A': 'A',\n            'NotA': '¬A',\n            'A_B': 'B',\n            'A_notB': '¬B',\n            'NotA_B': 'B',\n            'NotA_notB': '¬B'\n        }[node]\n        ax.text(x, y, label, ha='center', va='center', fontsize=12)\n\n    # Function to draw a branch\n    def draw_branch(start, end, text, color, lw=1.5):\n        x1, y1 = coords[start]\n        x2, y2 = coords[end]\n        arr = FancyArrowPatch((x1+0.035, y1), (x2-0.035, y2),\n                              arrowstyle='-|&gt;', mutation_scale=15,\n                              lw=lw, color=color)\n        ax.add_patch(arr)\n        ax.text((x1+x2)/2, (y1+y2)/2, text, ha='center', va='center',\n                backgroundcolor='white', fontsize=10)\n\n    # Draw all branches in gray\n    branches = [\n        ('root','A', f'{pA}'),\n        ('root','NotA', f'{pNotA}'),\n        ('A','A_B', f'{pB_A}'),\n        ('A','A_notB', f'{pNotB_A}'),\n        ('NotA','NotA_B', f'{pB_notA:.3f}'),\n        ('NotA','NotA_notB', f'{pNotB_notA:.3f}')\n    ]\n    for b in branches:\n        draw_branch(*b, color=gray)\n\n    # Highlight requested branches\n    for b in highlight_branches:\n        # find text label for branch\n        prob = {\n            ('root','A'): f'{pA}',\n            ('root','NotA'): f'{pNotA}',\n            ('A','A_B'): f'{pB_A}',\n            ('A','A_notB'): f'{pNotB_A}',\n            ('NotA','NotA_B'): f'{pB_notA:.3f}',\n            ('NotA','NotA_notB'): f'{pNotB_notA:.3f}'\n        }[b]\n        draw_branch(b[0], b[1], prob, color=branch_color, lw=3)\n    \n    # Color the end-node if desired\n    for node in node_to_color:\n        node_patches[node].set_facecolor(branch_color)\n\n    return fig, ax\n\n# Tree 1: highlight only the intersection branch root-&gt;A-&gt;A_B in blue\nfig1, ax1 = draw_tree(highlight_branches=[('root','A'),('A','A_B')],\n                      branch_color=color_intersection,\n                      node_to_color=['A_B'])\nax1.set_title('Intersection Only (P(A∩B))', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Tree 2: highlight both B branches (A-&gt;B and ¬A-&gt;B) in orange\nfig2, ax2 = draw_tree(highlight_branches=[('root','A'),('A','A_B'),\n                                          ('root','NotA'),('NotA','NotA_B')],\n                      branch_color=color_B,\n                      node_to_color=['A_B','NotA_B'])\nax2.set_title('Event B Only (P(B))', fontsize=14)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-2",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-2",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\n\n\n\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement.\n\nWhat’s the probability both balls are red?\nWhat’s the probability the first is red and second is blue?\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.lines import Line2D\n\n# Set up figure\nfig, ax = plt.subplots(figsize=(10, 6))\nax.axis('off')\n\n# Node positions\npositions = {\n    'root': (0.1, 0.5),\n    'R': (0.35, 0.7),\n    'B': (0.35, 0.3),\n    'RR': (0.7, 0.8),\n    'RB': (0.7, 0.6),\n    'BR': (0.7, 0.4),\n    'BB': (0.7, 0.2),\n}\n\n# Define colors\ncolor_root = '#ecf0f1'\ncolor_R = '#e74c3c'       # red for R or RR\ncolor_B = '#3498db'       # blue for B or BB\ncolor_mixed = '#ff7f0e'   # orange for mixed (RB, BR)\nedge_color = 'black'\nlinewidth = 2\ntext_fs = 12\n\n# Draw nodes with colored backgrounds and bold labels\nnode_colors = {\n    'root': color_root,\n    'R': color_R,\n    'B': color_B,\n    'RR': color_R,\n    'RB': color_mixed,\n    'BR': color_mixed,\n    'BB': color_B\n}\n\nfor name, (x, y) in positions.items():\n    circ = Circle((x, y), 0.05, facecolor=node_colors[name],\n                  edgecolor=edge_color, linewidth=linewidth, zorder=2)\n    ax.add_patch(circ)\n    ax.text(x, y, name, ha='center', va='center', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Function to draw an arrow and label\ndef draw_arrow(src, dst, label):\n    x1, y1 = positions[src]\n    x2, y2 = positions[dst]\n    ax.annotate('', xy=(x2-0.05, y2), xytext=(x1+0.05, y1),\n                arrowprops=dict(arrowstyle='-&gt;', color=node_colors[dst], lw=linewidth), zorder=1)\n    ax.text((x1+x2)/2, (y1+y2)/2, label, ha='center', va='center',\n            backgroundcolor='white', fontsize=text_fs, fontweight='bold', zorder=3)\n\n# Draw branches with probabilities\ndraw_arrow('root', 'R', '5/8')\ndraw_arrow('root', 'B', '3/8')\ndraw_arrow('R', 'RR', '4/7')\ndraw_arrow('R', 'RB', '3/7')\ndraw_arrow('B', 'BR', '5/7')\ndraw_arrow('B', 'BB', '2/7')\n\n# Label leaves with final probabilities in bold\nleaf_probs = {\n    'RR': '20/56',\n    'RB': '15/56',\n    'BR': '15/56',\n    'BB': '6/56'\n}\nfor leaf, prob in leaf_probs.items():\n    x, y = positions[leaf]\n    ax.text(x+0.12, y, prob, ha='left', va='center', fontsize=text_fs, fontweight='bold')\n\n# Legend\nlegend_elements = [\n    Line2D([0], [0], marker='o', color='w', label='RR / R',\n           markerfacecolor=color_R, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='BB / B',\n           markerfacecolor=color_B, markersize=12, markeredgecolor=edge_color),\n    Line2D([0], [0], marker='o', color='w', label='RB / BR',\n           markerfacecolor=color_mixed, markersize=12, markeredgecolor=edge_color),\n]\nax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.02, 0.95))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution. \n\n\\(P(\\text{both red}) = \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)\n\\(P(\\text{red then blue}) = \\frac{5}{8} \\times \\frac{3}{7} = \\frac{15}{56}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#law-of-total-probability",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#law-of-total-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n🎯 Definition\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space, then:\n\\[P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \\cdots + P(A|B_n)P(B_n)\\]\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Ellipse, Rectangle\n\n# Example probabilities\nP_B = [0.3, 0.5, 0.2]\nP_A_given_B = [0.2, 0.6, 0.5]\nP_A_and_B = [P_B[i] * P_A_given_B[i] for i in range(3)]\n\n# Create figure\nfig, ax = plt.subplots(figsize=(9, 5))\nax.set_aspect('equal')\nax.axis('off')\n\n# Draw sample space rectangle\nrect = Rectangle((-1, -2), width=8, height=4, edgecolor='black', facecolor='none', linewidth=2)\nax.add_patch(rect)\nax.text(-0.9, 1.7, r'$\\Omega$', fontsize=14, fontweight='bold', va='top', ha='left')\n\n# Draw B partitions as circles\npositions_B = [(1, 0), (3, 0), (5, 0)]\ncolors_B = ['#ffcc00', '#ff6600', '#ff0066']\n\nfor i, (x, y) in enumerate(positions_B):\n    circle = Circle((x, y), radius=1, facecolor=colors_B[i], edgecolor='black', alpha=0.3)\n    ax.add_patch(circle)\n    ax.text(x, y-1.3, f'$P(B_{i+1})={P_B[i]:.2f}$', ha='center', va='center', fontsize=12, fontweight='bold')\n\n# Draw A as a large ellipse overlapping all B's\nellipse = Ellipse((3, 0), width=8, height=3, angle=0, facecolor='#1f77b4', edgecolor='black', alpha=0.2)\nax.add_patch(ellipse)\nax.text(3, 1.2, '$A$', ha='center', va='center', fontsize=14, fontweight='bold')\n\n# Annotate intersections P(A ∩ Bi)\nfor i, (x, y) in enumerate(positions_B):\n    ax.text(x, 0, f'$P(A\\\\cap B_{i+1})={P_A_and_B[i]:.2f}$', ha='center', va='center', fontsize=12, color='black', fontweight='bold')\n\nax.set_xlim(-1, 7)\nax.set_ylim(-2, 2)\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#law-of-total-probability-example",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\n\nA factory has two machines:\n\nMachine 1: Produces 60% of items, 5% defective\nMachine 2: Produces 40% of items, 3% defective\n\nQ: What’s the overall probability an item is defective?\n\n\n\nSolution. \\(P(\\text{defective}) = P(D|M_1)P(M_1) + P(D|M_2)P(M_2)\\)\n\\(= 0.05 \\times 0.6 + 0.03 \\times 0.4 = 0.03 + 0.012 = 0.042\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\n\n🎯 Definition \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\nThis allows us to “reverse” conditional probabilities\nNamed after Thomas Bayes (1701-1761)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-components",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-components",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\n\\(A,B\\): Events\n\\(P(A|B)\\): Posterior probability - what we want to find\n\\(P(B|A)\\): Likelihood - given \\(A\\), probability of observing \\(B\\)\n\\(P(A)\\): Prior probability - initial probability of \\(A\\)\n\\(P(B)\\): Marginal probability - total probability of \\(B\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-example",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Example",
    "text": "Bayes’ Theorem Example\n\n\n\nMedical test for a disease: 1\n\nDisease affects 1% of population\nTest is 95% accurate for sick people\nTest is 90% accurate for healthy people\n\nQ:If someone tests positive, what’s the probability they have the disease?\n\n\n\nimport plotly.express as px\nimport pandas as pd\n\n# Given probabilities\nP_D = 0.01\nP_notD = 0.99\nP_pos_given_D = 0.95\nP_pos_given_notD = 0.10\n\n# Joint probabilities\nP_D_and_pos = P_D * P_pos_given_D       # True positives\nP_notD_and_pos = P_notD * P_pos_given_notD  # False positives\nP_D_and_neg = P_D * (1 - P_pos_given_D)     # False negatives\nP_notD_and_neg = P_notD * (1 - P_pos_given_notD)  # True negatives\n\n# Prepare DataFrame for treemap (mosaic)\ndf = pd.DataFrame({\n    'Test Result': ['Positive', 'Positive', 'Negative', 'Negative'],\n    'Condition':   ['Disease',   'No Disease', 'Disease',   'No Disease'],\n    'Probability': [P_D_and_pos, P_notD_and_pos, P_D_and_neg, P_notD_and_neg]\n})\n\n# Create treemap mosaic plot\nfig = px.treemap(\n    df,\n    path=['Test Result', 'Condition'],\n    values='Probability',\n    color='Condition',\n    color_discrete_map={'Disease':'tomato', 'No Disease':'skyblue'}\n)\nfig.update_traces(textinfo='label+percent entry')\nfig.update_layout(\n    title='Bayes’ Theorem: Distribution by Test Result and Condition',\n    margin=dict(t=50, l=25, r=25, b=25)\n)\nfig.show()"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-solution",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-solution",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution",
    "text": "Bayes’ Theorem Solution\nLet:\n\n\\(D\\): Person has disease\n\\(T^+\\): Test is positive\n\nGiven:\n\n\\(P(D) = 0.01\\)\n\\(P(T^+|D) = 0.95\\)\n\\(P(T^-|D^c) = 0.90\\), so \\(P(T^+|D^c) = 0.10\\)\n\n\n\nSolution. \\(P(T^+) = P(T^+|D)P(D) + P(T^+|D^c)P(D^c)\\)\n\\(= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.1085\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-solution-cont.",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#bayes-theorem-solution-cont.",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Bayes’ Theorem Solution (cont.)",
    "text": "Bayes’ Theorem Solution (cont.)\n\\[P(D|T^+) = \\frac{P(T^+|D) \\times P(D)}{P(T^+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\]\n\nimport matplotlib.pyplot as plt\n\n# Posterior probabilities given a positive test\nP_D_and_pos = 0.0095\nP_notD_and_pos = 0.099\nP_positive = P_D_and_pos + P_notD_and_pos\n\nP_D_given_pos = P_D_and_pos / P_positive\nP_notD_given_pos = P_notD_and_pos / P_positive\n\n# Pie chart\nlabels = ['Disease (P≈8.8%)', 'No Disease (P≈91.2%)']\nsizes = [P_D_given_pos, P_notD_given_pos]\ncolors = ['tomato', 'skyblue']\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.pie(\n    sizes, labels=labels, colors=colors, autopct='%.1f%%',\n    startangle=90, textprops={'fontsize': 14, 'fontweight': 'bold'}\n)\nax.set_title('Posterior Probability Given Positive Test', fontsize=16, fontweight='bold')\nax.axis('equal')  # Equal aspect ensures pie is circular.\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSurprising result: Even with a positive test, there’s only an 8.8% chance of having the disease!\nThis is due to the low base rate of the disease"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#common-probability-mistakes",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#common-probability-mistakes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nConfusing \\(P(A|B)\\) with \\(P(B|A)\\)\n\n\nProsecutor’s fallacy is a specific error in interpreting conditional probabilities. Confusing\n\\(P(\\text{Evidence}\\mid\\text{Innocent})\n\\quad\\text{with}\\quad\nP(\\text{Innocent}\\mid\\text{Evidence})\\).\nEx: OJ Simpson Case 2"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#common-probability-mistakes-1",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#common-probability-mistakes-1",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Probability Mistakes",
    "text": "Common Probability Mistakes\n\nAssuming independence when events are dependent\nIgnoring base rates (as in the medical test example)\n\n\nBase rate fallacy is when you ignore or underweight the prior probability \\(P(H)\\) of a hypothesis, focusing only on the new evidence \\(E\\).\n\n\nDouble counting in union calculations"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-3",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-3",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\n\nTwo fair dice are rolled. Find:\n\n\\(P(\\text{sum} = 7)\\)\n\\(P(\\text{sum} = 7 | \\text{first die shows 3})\\)\nAre these events independent?\n\n\n\n\nSolution. \n\n6 ways out of 36: \\(P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}\\)\nGiven first die is 3, need second die to be 4: \\(P(\\text{sum} = 7 | \\text{first} = 3) = \\frac{1}{6}\\)\nYes, they’re independent since \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#counting-and-probability",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#counting-and-probability",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting and Probability",
    "text": "Counting and Probability\n\nSometimes we need to count outcomes:\n\nMultiplication Principle: If task 1 can be done in \\(m\\) ways and task 2 in \\(n\\) ways, both can be done in \\(m \\times n\\) ways\n\n\nPermutations: Arrangements where order matters \\[P(n,r) = \\frac{n!}{(n-r)!}\\]\n\n\nCombinations: Selections where order doesn’t matter \\[C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#counting-example",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#counting-example",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Counting Example",
    "text": "Counting Example\n\nQ: How many ways can you arrange 5 people in a row?\n\n\n\nSolution. This is a permutation: \\(P(5,5) = 5! = 120\\) ways\n\n\n\n\n\nQ:How many ways can you choose 3 people from 5 for a committee?\n\n\n\n\nSolution. This is a combination: \\(C(5,3) = \\binom{5}{3} = \\frac{5!}{3!2!} = 10\\) ways"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#probability-with-counting",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#probability-with-counting",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Probability with Counting",
    "text": "Probability with Counting\n\nExample: A committee of 3 people is chosen from 8 people (5 women, 3 men). What’s the probability all 3 are women?\n\n\n\nSolution. Total ways to choose 3 from 8: \\(\\binom{8}{3} = 56\\)\nWays to choose 3 women from 5: \\(\\binom{5}{3} = 10\\)\nProbability: \\(\\frac{10}{56} = \\frac{5}{28}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#real-world-applications",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#real-world-applications",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Diagnosis: Using Bayes’ theorem for test interpretation\nQuality Control: Probability of defective items\nFinance: Risk assessment and portfolio theory\nSports: Probability of wins, fantasy sports\nInsurance: Calculating premiums based on risk"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#key-formulas-summary",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#key-formulas-summary",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\n\nBasic probability: \\(P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\)\nComplement: \\(P(A^c) = 1 - P(A)\\)\nAddition: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nConditional: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nIndependence: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nBayes’: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#problem-solving-strategy",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#problem-solving-strategy",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\n\nIdentify the sample space and events\nDetermine if events are independent or mutually exclusive\nChoose the appropriate rule or formula\nCalculate step by step\nCheck if your answer makes sense"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-4",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-4",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\n\nA bag contains 4 red, 3 blue, and 2 green marbles. Three marbles are drawn without replacement.\nFind the probability that: a) All three are red b) No two are the same color c) At least one is blue"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-4-solutions",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#practice-problem-4-solutions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Practice Problem 4 Solutions",
    "text": "Practice Problem 4 Solutions\n\nSolution. \n\nAll red: \\(\\frac{4}{9} \\times \\frac{3}{8} \\times \\frac{2}{7} = \\frac{24}{504} = \\frac{1}{21}\\)\nDifferent colors: \\(\\frac{4 \\times 3 \\times 2}{9 \\times 8 \\times 7} \\times 3! = \\frac{24 \\times 6}{504} = \\frac{144}{504} = \\frac{2}{7}\\)\nAt least one blue: \\(1 - P(\\text{no blue}) = 1 - \\frac{6 \\times 5 \\times 4}{9 \\times 8 \\times 7} = 1 - \\frac{120}{504} = \\frac{384}{504} = \\frac{16}{21}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#common-questions",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#common-questions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Common Questions",
    "text": "Common Questions\n\nQ1.: “Why isn’t \\(P(A \\cup B) = P(A) + P(B)\\) always?”\nA: We’d double-count outcomes in both events\nQ2.: “How do I know if events are independent?”\nA: Check if \\(P(A|B) = P(A)\\) or if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nQ3.: “When do I use Bayes’ theorem?”\nA: When you want to “reverse” a conditional probability\n\n\nQ3 note (Bayes Example)\n\nForward: I know my test picks up disease 95% of the time ⇒ \\(P(+\\mid D)=0.95\\).\nReverse: I want the chance I really have the disease when the test is positive ⇒ \\(P(D\\mid +)\\)."
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#looking-ahead",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#looking-ahead",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture:\n\nRandom Variables and Probability Distributions\nDiscrete vs. continuous random variables\nExpected value and variance"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#final-thoughts",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#final-thoughts",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Final Thoughts",
    "text": "Final Thoughts\n\nProbability is the foundation of statistics:\n\nHelps us quantify uncertainty\nProvides tools for making decisions with incomplete information\nEssential for understanding statistical inference\n\n\n\nPractice: The key to mastering probability is working through many problems!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#questions",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#questions",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: Thursday’s 11 AM On Zoom (Link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Random Variables and Distributions"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#resources",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#resources",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Resources",
    "text": "Resources\n\nRead Chapter 3 in course textbook\nElements of Set Theory for Probability\nProbability Models and Axioms"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture5-6.html#footnotes",
    "href": "files/lecture_notes/test_lectures/lecture5-6.html#footnotes",
    "title": "PSTAT 5A: Conditional Probability Continued & Bayes’ Theorem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(P(\\neg D \\,\\wedge\\, \\text{Negative})\n\\;=\\;P(\\neg D)\\times P(\\text{Negative}\\mid \\neg D)\n\\;=\\;0.99\\times0.90\n\\;=\\;0.891\\;(89.1\\%)\\)↩︎\nthe DNA evidence in the O. J. Simpson trial are a classic example of the prosecutor’s fallacy. Prosecutors highlighted that the chance of a random person matching the crime-scene DNA was “one in 170 million,” then implied (or let the jury infer) that Simpson therefore had a 1 in 170 million chance of being innocent.↩︎"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html",
    "title": "PSTAT 5A: Counting",
    "section": "",
    "text": "The art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#welcome-to-lecture-6-part-2",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#welcome-to-lecture-6-part-2",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 6 (Part 2)",
    "text": "Welcome to Lecture 6 (Part 2)\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#learning-objectives",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#learning-objectives",
    "title": "PSTAT 5A: Counting",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\n\n\n\n\n\n\n\nLearning Objectives\n\n\nBy the end of this lecture, you will be able to:\n\nCalculate combinations and understand when to use them (Section 3, Section 4)\nDistinguish between permutations and combinations (Section 7)\nUse counting techniques to solve probability problems (Section 12)\nApply the inclusion-exclusion principle (Section 17)\nSolve complex counting problems systematically (Section 21)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6.html#welcome-to-lecture-6-part-1",
    "href": "files/lecture_notes/lecture6/lecture6.html#welcome-to-lecture-6-part-1",
    "title": "PSTAT 5A: Counting",
    "section": "",
    "text": "The art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#sec-combinations",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#sec-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "What Are Combinations?",
    "text": "What Are Combinations?\n\n\n\n\n\n\n\n\n\nCombination\n\n\nA selection of objects where order does NOT matter\n\n\n\n\n\nCommittee Selection:\nABC, BAC, CAB → Same committee!\nRace Results:\nABC, BAC, CAB → Different outcomes!\n\n\n\n\n\n\nNote\n\n\nKey Point: Order doesn’t matter for combinations\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nAll combinations of 4 items taken 2 at a time:\n1. A, B\n2. A, C\n3. A, D\n4. B, C\n5. B, D\n6. C, D\n\n\n\n\nMore Examples: - Choosing committee members - Selecting pizza toppings - Forming study groups - Lottery number selection"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-vs-combinations",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-vs-combinations",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations vs Combinations",
    "text": "Permutations vs Combinations\n\n\n\n\n\n\n\n\n\nRelationship\n\n\n\n\\(P(n,r) = \\frac{n!}{(n-r)!} \\quad (1)\\)\n\nand,\n\n\\(C(n,r) = \\frac{n!}{(n-r)!} \\quad (2)\\)\n\nPlugging (2) into (1) and multiplying by \\(r\\) which represents the number of arrangements we get :\n\\(P(n,r) = C(n,r) \\times r!\\) (multiply by arrangements)\nWhy? For each combination of \\(r\\) objects, there are \\(r!\\) ways to arrange them\nSimilarly,\n\\(C(n,r) = P(n,r)/ r!\\) (divide out arrangements)\nWhy? diving by \\(r\\) removes the arrangements from our formula and leaves us with the number of selection possible from \\(n\\) objects.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPermutations = Combinations × Arrangements\nCombinations answer: “How many ways can I choose?”\nArrangements answer: “How many ways can I order what I chose?”*\nPermutations answer: “How many ways can I choose AND order?”\n\n\\(\\text{Choose} \\times \\text{Arrange} = \\text{Choose and Arrange}\\)\n\\[\n\\underbrace{\\binom{n}{r}}{\\substack{\\text{choose which}\\\\r\\text{ elements}}}\n\\;\\times\\;\n\\underbrace{r!}{\\substack{\\text{order those}\\\\r\\text{ elements}}}\n\\;=\\;\n\\frac{n!}{r!(n-r)!}\\;\\times\\;r!\n\\;=\\;\n\\frac{n!}{(n-r)!}\n\\;=\\;\nP(n,r).\n\\]\nThus,\n\\(C(n,r)×r!= P(n,r)\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-vs-combinations-example",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#permutations-vs-combinations-example",
    "title": "PSTAT 5A: Counting",
    "section": "Permutations vs Combinations Example",
    "text": "Permutations vs Combinations Example\n\nSelect 3 people from a group of 5 for different purposes.\n\nFor a ranked competition (order matters):\n1st place, 2nd place, 3rd place matter\nUse permutations: \\(P(5,3) = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} = \\frac{120}{2} = 60\\) ways\nFor a committee (order doesn’t matter):\nJust need 3 people, no specific roles\nUse combinations: \\(C(5,3) = \\frac{5!}{3!(5-3)!} = \\frac{5!}{3! \\cdot 2!} = \\frac{120}{6 \\cdot 2} = 10\\) ways\n\nVerifying the relationship:\n\\(P(5,3)=C(5,3)×3!\\)\n\\(60=10 \\times 6 \\checkmark\\)\nFor each of the 10 combinations, there are \\(3! = 6\\) ways to arrange them, giving us the 60 permutations."
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#learning-summary",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#learning-summary",
    "title": "PSTAT 5A: Counting",
    "section": "Learning Objectives Summary",
    "text": "Learning Objectives Summary\n\n\n\n\n\n\n\nWhat We’ve Covered\n\n\nIn this lecture, we’ve addressed all the learning objectives:\n\n✅ Apply the fundamental counting principles: Covered in Section 4\n✅ Calculate permutations with and without repetition: Covered in Section 8, Section 11, and Section 15"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part2.html#welcome-to-lecture-7-part-2",
    "href": "files/lecture_notes/lecture6/lecture6-part2.html#welcome-to-lecture-7-part-2",
    "title": "PSTAT 5A: Counting",
    "section": "Welcome to Lecture 7 (Part 2)",
    "text": "Welcome to Lecture 7 (Part 2)\nThe art and science of systematic enumeration"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#perm",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#perm",
    "title": "PSTAT 5A: Counting",
    "section": "Perm",
    "text": "Perm\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary?\n\n\nSolution. \\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture6/lecture6-part1.html#example",
    "href": "files/lecture_notes/lecture6/lecture6-part1.html#example",
    "title": "PSTAT 5A: Counting",
    "section": "Example",
    "text": "Example\n\nHow many ways can we select and arrange 3 people from a group of 8 for president, vice-president, and secretary? \n\n\nSolution. \\(P(8,3) = \\frac{8!}{(8-3)!} = \\frac{8!}{5!} = 8 \\times 7 \\times 6 = 336\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#what-is-a-random-variable",
    "href": "files/lecture_notes/lecture7/lecture7.html#what-is-a-random-variable",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\n\nDefinition: A random variable is a function that assigns a numerical value to each outcome of a random experiment.\nNotation: Usually denoted by capital letters \\(X\\), \\(Y\\), \\(Z\\)\nKey insight: Random variables transform outcomes into numbers, making statistical analysis possible\n\n\n\nTreena - Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#why-use-random-variables",
    "href": "files/lecture_notes/lecture7/lecture7.html#why-use-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Why Use Random Variables?",
    "text": "Why Use Random Variables?\n\n\nRandom variables allow us to:\n\nQuantify outcomes numerically\nCalculate means, variances, and other statistics\nModel real-world phenomena\nMake predictions and decisions\nCompare different random processes\n\nExamples: Height, test scores, number of defects, wait times, stock prices\n\n\n\nTreena - Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#types-of-random-variables",
    "href": "files/lecture_notes/lecture7/lecture7.html#types-of-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\n\n\nToday we focus on discrete random variables - notice there are gaps between possible values!\n\n\n\nDiscrete vs. Continuous: Demystifying the type of Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#examples-of-discrete-random-variables",
    "href": "files/lecture_notes/lecture7/lecture7.html#examples-of-discrete-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Examples of Discrete Random Variables",
    "text": "Examples of Discrete Random Variables\n\\(X\\) = Number of heads in 3 coin flips - Possible values: \\(\\{0, 1, 2, 3\\}\\)\n\\(Y\\) = Number of customers in an hour - Possible values: \\(\\{0, 1, 2, 3, \\ldots\\}\\)\n\\(Z\\) = Score on a 10-question quiz - Possible values: \\(\\{0, 1, 2, \\ldots, 10\\}\\)\n Figure: Example of a discrete random variable: number of heads in coin flips.\n\nNotice: All values are integers with gaps between them"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#probability-mass-function-pmf",
    "href": "files/lecture_notes/lecture7/lecture7.html#probability-mass-function-pmf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\n\n\nDefinition: The Probability Mass Function (PMF) of a discrete random variable \\(X\\) is:\n\n\n\\[P(X = x) = \\text{probability that } X \\text{ takes the value } x\\]\n\n\nProperties of PMF:\n\n\n\n\\(P(X = x) \\geq 0\\) for all \\(x\\)\n\n\n\\(\\sum_{\\text{all } x} P(X = x) = 1\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#pmf-example-fair-die",
    "href": "files/lecture_notes/lecture7/lecture7.html#pmf-example-fair-die",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Fair Die",
    "text": "PMF Example: Fair Die\nLet \\(X\\) = outcome of rolling a fair six-sided die\n\\[P(X = x) = \\begin{cases}\n\\frac{1}{6} & \\text{if } x \\in \\{1, 2, 3, 4, 5, 6\\} \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n Figure: Bar chart of PMF for a fair die.\n\nVerification: - All probabilities ≥ 0 ✓ - Sum = \\(6 \\times \\frac{1}{6} = 1\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#pmf-example-two-coin-flips",
    "href": "files/lecture_notes/lecture7/lecture7.html#pmf-example-two-coin-flips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Two Coin Flips",
    "text": "PMF Example: Two Coin Flips\n\nTwo Coin Flips - Number of Heads\nLet \\(X\\) = number of heads in two coin flips\nSample Space: \\(\\{HH, HT, TH, TT\\}\\)\n\n\n\nH T H T\n\n\nCurrent outcome: H T H T\n\nClick coins to flip them!\n\nTable summarizes by number of heads, not the exact sequence.\n\n\n\n\n\n\\(x\\) (heads)\n\n\nOutcomes\n\n\n\\(P(X = x)\\)\n\n\nEmpirical\n\n\n\n\n\n\n0\n\n\nTT\n\n\n0.25\n\n\n0\n\n\n\n\n1\n\n\nHT, TH\n\n\n0.50\n\n\n0\n\n\n\n\n2\n\n\nHH\n\n\n0.25\n\n\n0\n\n\n\n\n\nEmpirical probability updates as you flip!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#cumulative-distribution-function-cdf",
    "href": "files/lecture_notes/lecture7/lecture7.html#cumulative-distribution-function-cdf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\n\n\n\nThe cumulative distribution function of a random variable \\(X\\) is:\n\\[F(x) = P(X \\leq x)\\]\nProperties of CDF: 1. \\(F(x)\\) is non-decreasing 2. \\(\\lim_{x \\to -\\infty} F(x) = 0\\) 3. \\(\\lim_{x \\to \\infty} F(x) = 1\\) 4. \\(F(x)\\) is right-continuous"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#expected-value-mean",
    "href": "files/lecture_notes/lecture7/lecture7.html#expected-value-mean",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value (Mean)",
    "text": "Expected Value (Mean)\nThe expected value of a discrete random variable \\(X\\) is:\n\\[E[X] = \\mu = \\sum_{\\text{all } x} x \\cdot P(X = x)\\]\n Figure: Expected value as the long-run average of repeated coin flips."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#variance",
    "href": "files/lecture_notes/lecture7/lecture7.html#variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Variance",
    "text": "Variance\nThe variance of a random variable \\(X\\) measures spread around the mean:\n\\[\\text{Var}(X) = \\sigma^2 = E[(X - \\mu)^2]\\]\n Figure: Variance measures the spread of possible values around the mean."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#bernoulli-distribution",
    "href": "files/lecture_notes/lecture7/lecture7.html#bernoulli-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\nA Bernoulli random variable models a single trial with two outcomes:\n\nSuccess (1) with probability \\(p\\)\nFailure (0) with probability \\(1-p\\)\n\n\n\n\n\n\nbar\n    title Bernoulli PMF (p=0.7)\n    x-axis 0 1\n    y-axis Probability\n    0: 0.3\n    1: 0.7"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#binomial-distribution",
    "href": "files/lecture_notes/lecture7/lecture7.html#binomial-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\nA binomial random variable counts successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\)\n Figure: Binomial experiment: sequence of independent trials.\n\n\n\n\n\nbar\n    title Binomial PMF (n=5, p=0.5)\n    x-axis 0 1 2 3 4 5\n    y-axis Probability\n    0: 0.031\n    1: 0.156\n    2: 0.312\n    3: 0.312\n    4: 0.156\n    5: 0.031"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#geometric-distribution",
    "href": "files/lecture_notes/lecture7/lecture7.html#geometric-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nA geometric random variable counts the number of trials until the first success\n Figure: Geometric experiment: waiting for the first success.\n\n\n\n\n\nbar\n    title Geometric PMF (p=0.3)\n    x-axis 1 2 3 4 5\n    y-axis Probability\n    1: 0.3\n    2: 0.21\n    3: 0.147\n    4: 0.1029\n    5: 0.072"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#poisson-distribution",
    "href": "files/lecture_notes/lecture7/lecture7.html#poisson-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\nA Poisson random variable counts events occurring in a fixed interval when events happen at a constant average rate\n Figure: Poisson process: random events in time.\n\n\n\n\n\nbar\n    title Poisson PMF (λ=3)\n    x-axis 0 1 2 3 4 5 6\n    y-axis Probability\n    0: 0.0498\n    1: 0.1494\n    2: 0.2240\n    3: 0.2240\n    4: 0.1680\n    5: 0.1008\n    6: 0.0504"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#hypergeometric-distribution",
    "href": "files/lecture_notes/lecture7/lecture7.html#hypergeometric-distribution",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Hypergeometric Distribution",
    "text": "Hypergeometric Distribution\nA hypergeometric random variable counts successes when sampling without replacement from a finite population\n Figure: Hypergeometric sampling: drawing balls from a box without replacement."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#law-of-large-numbers",
    "href": "files/lecture_notes/lecture7/lecture7.html#law-of-large-numbers",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\nAs \\(n\\) increases, the sample mean approaches the expected value:\n\\[\\bar{X}_n = \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\to E[X]\\]\n\n\n\n\n\nline\n    title Law of Large Numbers Simulation\n    x-axis 0 100 200 300 400 500 600 700 800 900 1000\n    y-axis Sample Mean\n    0: 0.0\n    100: 0.48\n    200: 0.51\n    300: 0.49\n    400: 0.50\n    500: 0.51\n    600: 0.50\n    700: 0.50\n    800: 0.49\n    900: 0.50\n    1000: 0.50\n\n\n\n\n\n\nFigure: As the number of trials increases, the sample mean converges to the expected value."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#simulationmonte-carlo",
    "href": "files/lecture_notes/lecture7/lecture7.html#simulationmonte-carlo",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Simulation/Monte Carlo",
    "text": "Simulation/Monte Carlo\nMonte Carlo Method: Use computer simulation to estimate probabilities\n Figure: Monte Carlo simulation: repeated random experiments to estimate probabilities."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#properties-of-expected-value",
    "href": "files/lecture_notes/lecture7/lecture7.html#properties-of-expected-value",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Properties of Expected Value",
    "text": "Properties of Expected Value\n\n\n\nLinearity of Expectation\n\n\\(E[c] = c\\) (constant)\n\\(E[cX] = c \\cdot E[X]\\) (scaling)\n\\(E[X + Y] = E[X] + E[Y]\\) (additivity)\n\\(E[aX + bY + c] = aE[X] + bE[Y] + c\\)\n\n\nVariance Properties\n\n\\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (if \\(X\\) and \\(Y\\) are independent)\n\n\nImportant: Property 3 holds even if \\(X\\) and \\(Y\\) are dependent!"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#expected-value-of-functions",
    "href": "files/lecture_notes/lecture7/lecture7.html#expected-value-of-functions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value of Functions",
    "text": "Expected Value of Functions\nFor a function \\(g(X)\\) of a random variable \\(X\\):\n\\[E[g(X)] = \\sum_{\\text{all } x} g(x) \\cdot P(X = x)\\]\n\nCommon example: \\(g(X) = X^2\\)\n\\[E[X^2] = \\sum_{\\text{all } x} x^2 \\cdot P(X = x)\\]\nNote: Generally \\(E[g(X)] \\neq g(E[X])\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-8",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-8",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 8",
    "text": "Practice Problem 8\nFor the light bulb example:\n\nWhat’s the expected number of defective bulbs in the sample?\nWhat’s the probability of no defective bulbs?\nWhat’s the probability of at least 2 defective bulbs?\n\n\nSolutions: a) \\(E[X] = 5 \\times \\frac{3}{20} = 0.75\\) bulbs b) \\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{17}{5}}{\\binom{20}{5}} = \\frac{6188}{15504} \\approx 0.399\\) c) \\(P(X \\geq 2) = 1 - P(X = 0) - P(X = 1) \\approx 1 - 0.399 - 0.461 = 0.140\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-9",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-9",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 9",
    "text": "Practice Problem 9\nA website receives an average of 2 orders per minute. Assuming orders follow a Poisson process:\n\nWhat’s the probability of exactly 3 orders in a minute?\nWhat’s the probability of no orders in 30 seconds?\nWhat’s the probability of more than 5 orders in 2 minutes?\n\n\nSolutions: a) \\(X \\sim \\text{Poisson}(2)\\): \\(P(X = 3) = \\frac{2^3 e^{-2}}{3!} \\approx 0.180\\) b) \\(Y \\sim \\text{Poisson}(1)\\): \\(P(Y = 0) = e^{-1} \\approx 0.368\\) c) \\(Z \\sim \\text{Poisson}(4)\\): \\(P(Z &gt; 5) = 1 - P(Z \\leq 5) \\approx 0.215\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#central-limit-theorem-preview",
    "href": "files/lecture_notes/lecture7/lecture7.html#central-limit-theorem-preview",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Central Limit Theorem Preview",
    "text": "Central Limit Theorem Preview\nFor large \\(n\\), many discrete distributions approach normal:\nBinomial: \\(X \\sim \\text{Binomial}(n, p)\\) → \\(N(np, np(1-p))\\) when \\(np &gt; 5\\) and \\(n(1-p) &gt; 5\\)\nPoisson: \\(X \\sim \\text{Poisson}(\\lambda)\\) → \\(N(\\lambda, \\lambda)\\) when \\(\\lambda &gt; 10\\)\n\nThis connection will be crucial for hypothesis testing and confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#common-mistakes",
    "href": "files/lecture_notes/lecture7/lecture7.html#common-mistakes",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Mistakes",
    "text": "Common Mistakes\n\nWrong distribution choice: Check assumptions carefully\nParameter confusion: Is it \\(n\\), \\(p\\), or \\(\\lambda\\)?\nInclusion errors: “At least 3” vs “More than 3”\nIndependence assumption: Sampling with/without replacement\nTechnology errors: pdf vs cdf functions"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-10",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-10",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 10",
    "text": "Practice Problem 10\nA multiple choice test has 20 questions with 5 options each. A student knows the answers to 12 questions and guesses on the rest.\n\nWhat’s the expected score?\nWhat’s the probability of scoring at least 15?\nWhat’s the standard deviation of the score?\n\n\nSolution: Let \\(X\\) = known correct, \\(Y\\) = guessed correct - \\(X = 12\\) (deterministic) - \\(Y \\sim \\text{Binomial}(8, 0.2)\\) - Total score \\(S = X + Y = 12 + Y\\)\n\n\\(E[S] = 12 + E[Y] = 12 + 8(0.2) = 13.6\\)\n\\(P(S \\geq 15) = P(Y \\geq 3) \\approx 0.203\\)\n\n\\(\\sigma_S = \\sigma_Y = \\sqrt{8(0.2)(0.8)} = \\sqrt{1.28} \\approx 1.13\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#extensions-and-advanced-topics",
    "href": "files/lecture_notes/lecture7/lecture7.html#extensions-and-advanced-topics",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Extensions and Advanced Topics",
    "text": "Extensions and Advanced Topics\nNegative Binomial: Number of trials to get \\(r\\) successes\nMultinomial: Extension of binomial to more than 2 categories\nCompound Distributions: Sums of random variables\nGenerating Functions: Advanced technique for deriving properties\n\nThese topics appear in advanced probability courses"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#historical-context",
    "href": "files/lecture_notes/lecture7/lecture7.html#historical-context",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Historical Context",
    "text": "Historical Context\nJacob Bernoulli (1654-1705): Bernoulli trials and law of large numbers\nSiméon Denis Poisson (1781-1840): Poisson distribution and approximation\nAbraham de Moivre (1667-1754): Normal approximation to binomial\nModern Applications: Computer science, machine learning, bioinformatics"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#bonus-law-of-large-numbers",
    "href": "files/lecture_notes/lecture7/lecture7.html#bonus-law-of-large-numbers",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bonus: Law of Large Numbers",
    "text": "Bonus: Law of Large Numbers\nAs \\(n\\) increases, the sample mean approaches the expected value:\n\\[\\bar{X}_n = \\frac{X_1 + X_2 + \\cdots + X_n}{n} \\to E[X]\\]\nExample: Flip a coin 1000 times - proportion of heads will be close to 0.5\nThis justifies our interpretation of expected value as “long-run average”"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#bonus-simulation",
    "href": "files/lecture_notes/lecture7/lecture7.html#bonus-simulation",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Bonus: Simulation",
    "text": "Bonus: Simulation\nMonte Carlo Method: Use computer simulation to estimate probabilities\n# Simulate 10,000 binomial random variables\nX &lt;- rbinom(10000, size=10, prob=0.3)\nmean(X)  # Should be close to 10*0.3 = 3\nvar(X)   # Should be close to 10*0.3*0.7 = 2.1\nSimulation helps verify theoretical results and solve complex problems"
  },
  {
    "objectID": "test-minimal.html#test-slide",
    "href": "test-minimal.html#test-slide",
    "title": "Minimal Test",
    "section": "",
    "text": "This is a simple test.\n\nThis is a test div."
  },
  {
    "objectID": "lecture7-minimal.html#welcome-to-lecture-7",
    "href": "lecture7-minimal.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "lecture7-minimal.html#todays-learning-objectives",
    "href": "lecture7-minimal.html#todays-learning-objectives",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine random variables and distinguish discrete from continuous\nWork with probability mass functions (PMFs) and cumulative distribution functions (CDFs)\nCalculate expected values and variances for discrete random variables\nApply properties of expectation and variance\nWork with common discrete distributions (Bernoulli, Binomial, Geometric, Poisson)\nSolve real-world problems using discrete random variables\nUse technology to compute probabilities and parameters"
  },
  {
    "objectID": "lecture7-minimal.html#what-is-a-random-variable",
    "href": "lecture7-minimal.html#what-is-a-random-variable",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\nDefinition: A random variable is a function that assigns a numerical value to each outcome of a random experiment.\nNotation: Usually denoted by capital letters \\(X\\), \\(Y\\), \\(Z\\)\nKey insight: Random variables transform outcomes into numbers, making mathematical analysis possible"
  },
  {
    "objectID": "lecture7-minimal.html#why-use-random-variables",
    "href": "lecture7-minimal.html#why-use-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Why Use Random Variables?",
    "text": "Why Use Random Variables?\nRandom variables allow us to:\n\nQuantify outcomes numerically\nCalculate means, variances, and other statistics\nModel real-world phenomena mathematically\nMake predictions and decisions\nCompare different random processes\n\nExamples: Height, test scores, number of defects, wait times, stock prices"
  },
  {
    "objectID": "lecture7-minimal.html#types-of-random-variables",
    "href": "lecture7-minimal.html#types-of-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\nDiscrete Random Variable\nTakes on a countable number of values\nCan list all possible values\nExamples:\n• Dice rolls: {1, 2, 3, 4, 5, 6}\n• Number of emails: {0, 1, 2, 3, …}\n• Quiz scores: {0, 1, 2, …, 10}\nContinuous Random Variable\nTakes on uncountably many values\nCannot list all possible values\nExamples:\n• Height: Any value in [0, ∞)\n• Time: Any positive real number\n• Temperature: Any real number"
  },
  {
    "objectID": "lecture7-minimal.html#probability-mass-function-pmf",
    "href": "lecture7-minimal.html#probability-mass-function-pmf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nThe probability mass function of a discrete random variable \\(X\\) is:\n\\[P(X = x) = \\text{probability that } X \\text{ takes the value } x\\]\nProperties of PMF: 1. \\(P(X = x) \\geq 0\\) for all \\(x\\) 2. \\(\\sum_{\\text{all } x} P(X = x) = 1\\)"
  },
  {
    "objectID": "lecture7-minimal.html#expected-value-and-variance",
    "href": "lecture7-minimal.html#expected-value-and-variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nThe expected value of a discrete random variable \\(X\\) is:\n\\[E[X] = \\mu = \\sum_{\\text{all } x} x \\cdot P(X = x)\\]\nThe variance of a random variable \\(X\\) measures spread around the mean:\n\\[\\text{Var}(X) = \\sigma^2 = E[(X - \\mu)^2] = E[X^2] - (E[X])^2\\]\nExpected value represents the long-run average if we repeat the experiment many times."
  },
  {
    "objectID": "lecture7-minimal.html#key-takeaways",
    "href": "lecture7-minimal.html#key-takeaways",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRandom variables transform outcomes into numbers for mathematical analysis\nPMF gives probabilities for specific values; CDF gives cumulative probabilities\nExpected value is the long-run average; variance measures spread\nChoose distributions based on the underlying process:\n\nBernoulli for single trials\nBinomial for fixed trials\nGeometric for waiting times\n\nPoisson for rates\n\nLaw of Large Numbers connects theoretical expectations with observed averages"
  },
  {
    "objectID": "lecture7-minimal.html#questions",
    "href": "lecture7-minimal.html#questions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: [Your office hours]\nEmail: [Your email]\nNext Class: Continuous Random Variables"
  },
  {
    "objectID": "lecture7-test.html#welcome-to-lecture-7",
    "href": "lecture7-test.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "lecture7-test.html#todays-learning-objectives",
    "href": "lecture7-test.html#todays-learning-objectives",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine random variables and distinguish discrete from continuous\nWork with probability mass functions (PMFs) and cumulative distribution functions (CDFs)\nCalculate expected values and variances for discrete random variables\nApply properties of expectation and variance\nWork with common discrete distributions (Bernoulli, Binomial, Geometric, Poisson)\nSolve real-world problems using discrete random variables\nUse technology to compute probabilities and parameters"
  },
  {
    "objectID": "lecture7-test.html#what-is-a-random-variable",
    "href": "lecture7-test.html#what-is-a-random-variable",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\nDefinition: A random variable is a function that assigns a numerical value to each outcome of a random experiment.\nNotation: Usually denoted by capital letters \\(X\\), \\(Y\\), \\(Z\\)\nKey insight: Random variables transform outcomes into numbers, making mathematical analysis possible"
  },
  {
    "objectID": "lecture7-test.html#why-use-random-variables",
    "href": "lecture7-test.html#why-use-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Why Use Random Variables?",
    "text": "Why Use Random Variables?\nRandom variables allow us to:\n\nQuantify outcomes numerically\nCalculate means, variances, and other statistics\nModel real-world phenomena mathematically\nMake predictions and decisions\nCompare different random processes\n\nExamples: Height, test scores, number of defects, wait times, stock prices"
  },
  {
    "objectID": "lecture7-test.html#types-of-random-variables",
    "href": "lecture7-test.html#types-of-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\nDiscrete Random Variable\nTakes on a countable number of values\nCan list all possible values\nExamples:\n• Dice rolls: {1, 2, 3, 4, 5, 6}\n• Number of emails: {0, 1, 2, 3, …}\n• Quiz scores: {0, 1, 2, …, 10}\nContinuous Random Variable\nTakes on uncountably many values\nCannot list all possible values\nExamples:\n• Height: Any value in [0, ∞)\n• Time: Any positive real number\n• Temperature: Any real number"
  },
  {
    "objectID": "lecture7-test.html#probability-mass-function-pmf",
    "href": "lecture7-test.html#probability-mass-function-pmf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nThe probability mass function of a discrete random variable \\(X\\) is:\n\\[P(X = x) = \\text{probability that } X \\text{ takes the value } x\\]\nProperties of PMF: 1. \\(P(X = x) \\geq 0\\) for all \\(x\\) 2. \\(\\sum_{\\text{all } x} P(X = x) = 1\\)\nInteractive PMF Demo: Fair Die\n\nRoll Die 1000 Times"
  },
  {
    "objectID": "lecture7-test.html#pmf-example-two-coin-flips",
    "href": "lecture7-test.html#pmf-example-two-coin-flips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Two Coin Flips",
    "text": "PMF Example: Two Coin Flips\nTwo Coin Flips - Number of Heads\nLet \\(X\\) = number of heads in two coin flips\nSample Space: \\(\\{HH, HT, TH, TT\\}\\)\n\n\n\n\\(x\\) (heads)\nOutcomes\n\\(P(X = x)\\)\n\n\n\n\n0\nTT\n1/4 = 0.25\n\n\n1\nHT, TH\n2/4 = 0.50\n\n\n2\nHH\n1/4 = 0.25"
  },
  {
    "objectID": "lecture7-test.html#practice-problem-1-red-and-blue-balls",
    "href": "lecture7-test.html#practice-problem-1-red-and-blue-balls",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 1: Red and Blue Balls",
    "text": "Practice Problem 1: Red and Blue Balls\nA box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let \\(X\\) = number of red balls drawn. Find the PMF of \\(X\\).\nSolution:\n\\(X\\) can take values 0, 1, or 2.\n\\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{2}{2}}{\\binom{5}{2}} = \\frac{1 \\times 1}{10} = \\frac{1}{10}\\)\n\\(P(X = 1) = \\frac{\\binom{3}{1}\\binom{2}{1}}{\\binom{5}{2}} = \\frac{3 \\times 2}{10} = \\frac{6}{10}\\)\n\\(P(X = 2) = \\frac{\\binom{3}{2}\\binom{2}{0}}{\\binom{5}{2}} = \\frac{3 \\times 1}{10} = \\frac{3}{10}\\)\nCheck: \\(\\frac{1}{10} + \\frac{6}{10} + \\frac{3}{10} = 1\\) ✓"
  },
  {
    "objectID": "lecture7-clean.html#welcome-to-lecture-7",
    "href": "lecture7-clean.html#welcome-to-lecture-7",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 7",
    "text": "Welcome to Lecture 7\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "lecture7-clean.html#todays-learning-objectives",
    "href": "lecture7-clean.html#todays-learning-objectives",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine random variables and distinguish discrete from continuous\nWork with probability mass functions (PMFs) and cumulative distribution functions (CDFs)\nCalculate expected values and variances for discrete random variables\nApply properties of expectation and variance\nWork with common discrete distributions (Bernoulli, Binomial, Geometric, Poisson)\nSolve real-world problems using discrete random variables\nUse technology to compute probabilities and parameters"
  },
  {
    "objectID": "lecture7-clean.html#what-is-a-random-variable",
    "href": "lecture7-clean.html#what-is-a-random-variable",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\nDefinition: A random variable is a function that assigns a numerical value to each outcome of a random experiment.\nNotation: Usually denoted by capital letters \\(X\\), \\(Y\\), \\(Z\\)\nKey insight: Random variables transform outcomes into numbers, making mathematical analysis possible\nDie Roll Example\nSample Space (Outcomes): ⚀, ⚁, ⚂, ⚃, ⚄, ⚅\nRandom Variable X: Maps each die face to its numerical value (1, 2, 3, 4, 5, 6)"
  },
  {
    "objectID": "lecture7-clean.html#why-use-random-variables",
    "href": "lecture7-clean.html#why-use-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Why Use Random Variables?",
    "text": "Why Use Random Variables?\nRandom variables allow us to:\n\nQuantify outcomes numerically\nCalculate means, variances, and other statistics\nModel real-world phenomena mathematically\nMake predictions and decisions\nCompare different random processes\n\nExamples: Height, test scores, number of defects, wait times, stock prices"
  },
  {
    "objectID": "lecture7-clean.html#types-of-random-variables",
    "href": "lecture7-clean.html#types-of-random-variables",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables\nDiscrete Random Variable\nTakes on a countable number of values\nCan list all possible values\nExamples:\n• Dice rolls: {1, 2, 3, 4, 5, 6}\n• Number of emails: {0, 1, 2, 3, …}\n• Quiz scores: {0, 1, 2, …, 10}\nContinuous Random Variable\nTakes on uncountably many values\nCannot list all possible values\nExamples:\n• Height: Any value in [0, ∞)\n• Time: Any positive real number\n• Temperature: Any real number"
  },
  {
    "objectID": "lecture7-clean.html#probability-mass-function-pmf",
    "href": "lecture7-clean.html#probability-mass-function-pmf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nThe probability mass function of a discrete random variable \\(X\\) is:\n\\[P(X = x) = \\text{probability that } X \\text{ takes the value } x\\]\nProperties of PMF: 1. \\(P(X = x) \\geq 0\\) for all \\(x\\) 2. \\(\\sum_{\\text{all } x} P(X = x) = 1\\)\nInteractive PMF Demo: Fair Die\n\nRoll Die 1000 Times"
  },
  {
    "objectID": "lecture7-clean.html#pmf-example-two-coin-flips",
    "href": "lecture7-clean.html#pmf-example-two-coin-flips",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "PMF Example: Two Coin Flips",
    "text": "PMF Example: Two Coin Flips\nTwo Coin Flips - Number of Heads\nLet \\(X\\) = number of heads in two coin flips\nSample Space: \\(\\{HH, HT, TH, TT\\}\\)\n\n\n\n\\(x\\) (heads)\nOutcomes\n\\(P(X = x)\\)\n\n\n\n\n0\nTT\n1/4 = 0.25\n\n\n1\nHT, TH\n2/4 = 0.50\n\n\n2\nHH\n1/4 = 0.25"
  },
  {
    "objectID": "lecture7-clean.html#cumulative-distribution-function-cdf",
    "href": "lecture7-clean.html#cumulative-distribution-function-cdf",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe cumulative distribution function of a random variable \\(X\\) is:\n\\[F(x) = P(X \\leq x)\\]\nProperties of CDF: 1. \\(F(x)\\) is non-decreasing 2. \\(\\lim_{x \\to -\\infty} F(x) = 0\\) 3. \\(\\lim_{x \\to \\infty} F(x) = 1\\) 4. \\(F(x)\\) is right-continuous"
  },
  {
    "objectID": "lecture7-clean.html#expected-value-and-variance",
    "href": "lecture7-clean.html#expected-value-and-variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nThe expected value of a discrete random variable \\(X\\) is:\n\\[E[X] = \\mu = \\sum_{\\text{all } x} x \\cdot P(X = x)\\]\nThe variance of a random variable \\(X\\) measures spread around the mean:\n\\[\\text{Var}(X) = \\sigma^2 = E[(X - \\mu)^2] = E[X^2] - (E[X])^2\\]\nExpected value represents the long-run average if we repeat the experiment many times."
  },
  {
    "objectID": "lecture7-clean.html#law-of-large-numbers-demo",
    "href": "lecture7-clean.html#law-of-large-numbers-demo",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Law of Large Numbers Demo",
    "text": "Law of Large Numbers Demo\nLaw of Large Numbers Demo\n\nRun Simulation\n\n 100 trials 500 trials 1000 trials 5000 trials \n\n\nWatch how the sample mean converges to the expected value!"
  },
  {
    "objectID": "lecture7-clean.html#common-discrete-distributions",
    "href": "lecture7-clean.html#common-discrete-distributions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBernoulli\nSingle trial, two outcomes\nParameters: \\(p\\) (success probability)\nPMF: \\(P(X = 1) = p\\), \\(P(X = 0) = 1-p\\)\nMean: \\(p\\)\nVariance: \\(p(1-p)\\)\nBinomial\n\\(n\\) independent Bernoulli trials\nParameters: \\(n\\) (trials), \\(p\\) (success prob.)\nPMF: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nMean: \\(np\\)\nVariance: \\(np(1-p)\\)\nGeometric\nTrials until first success\nParameters: \\(p\\) (success probability)\nPMF: \\(P(X = k) = (1-p)^{k-1} p\\)\nMean: \\(1/p\\)\nVariance: \\((1-p)/p^2\\)\nPoisson\nEvents in fixed interval\nParameters: \\(\\lambda\\) (average rate)\nPMF: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nMean: \\(\\lambda\\)\nVariance: \\(\\lambda\\)"
  },
  {
    "objectID": "lecture7-clean.html#interactive-distribution-explorer",
    "href": "lecture7-clean.html#interactive-distribution-explorer",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Interactive Distribution Explorer",
    "text": "Interactive Distribution Explorer\nDistribution Visualizer\n Binomial Geometric Poisson  Parameter 1:  Parameter 2:"
  },
  {
    "objectID": "lecture7-clean.html#practice-problem-1-red-and-blue-balls",
    "href": "lecture7-clean.html#practice-problem-1-red-and-blue-balls",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 1: Red and Blue Balls",
    "text": "Practice Problem 1: Red and Blue Balls\nA box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let \\(X\\) = number of red balls drawn. Find the PMF of \\(X\\).\nSolution:\n\\(X\\) can take values 0, 1, or 2.\n\\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{2}{2}}{\\binom{5}{2}} = \\frac{1 \\times 1}{10} = \\frac{1}{10}\\)\n\\(P(X = 1) = \\frac{\\binom{3}{1}\\binom{2}{1}}{\\binom{5}{2}} = \\frac{3 \\times 2}{10} = \\frac{6}{10}\\)\n\\(P(X = 2) = \\frac{\\binom{3}{2}\\binom{2}{0}}{\\binom{5}{2}} = \\frac{3 \\times 1}{10} = \\frac{3}{10}\\)\nCheck: \\(\\frac{1}{10} + \\frac{6}{10} + \\frac{3}{10} = 1\\) ✓"
  },
  {
    "objectID": "lecture7-clean.html#practice-problem-2-expected-value",
    "href": "lecture7-clean.html#practice-problem-2-expected-value",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 2: Expected Value",
    "text": "Practice Problem 2: Expected Value\nUsing the red balls example from Problem 1, find \\(E[X]\\) and \\(\\text{Var}(X)\\).\nSolution:\nExpected Value: \\[E[X] = 0 \\times \\frac{1}{10} + 1 \\times \\frac{6}{10} + 2 \\times \\frac{3}{10} = 0 + \\frac{6}{10} + \\frac{6}{10} = 1.2\\]\nVariance: \\[E[X^2] = 0^2 \\times \\frac{1}{10} + 1^2 \\times \\frac{6}{10} + 2^2 \\times \\frac{3}{10} = 0 + \\frac{6}{10} + \\frac{12}{10} = 1.8\\]\n\\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 1.8 - (1.2)^2 = 1.8 - 1.44 = 0.36\\]\nStandard Deviation: \\(\\sigma = \\sqrt{0.36} = 0.6\\)"
  },
  {
    "objectID": "lecture7-clean.html#practice-problem-3-binomial-application",
    "href": "lecture7-clean.html#practice-problem-3-binomial-application",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 3: Binomial Application",
    "text": "Practice Problem 3: Binomial Application\nA student takes a 10-question multiple choice quiz with 4 options per question. If the student guesses randomly, what’s the probability of getting exactly 3 correct?\nSolution:\nThis is a binomial distribution with \\(n = 10\\), \\(p = 1/4 = 0.25\\)\n\\[P(X = 3) = \\binom{10}{3} \\times (0.25)^3 \\times (0.75)^7\\]\n\\[P(X = 3) = 120 \\times 0.015625 \\times 0.1335 \\approx 0.2503\\]\nSo there’s about a 25% chance of getting exactly 3 correct by guessing."
  },
  {
    "objectID": "lecture7-clean.html#properties-of-expected-value",
    "href": "lecture7-clean.html#properties-of-expected-value",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Properties of Expected Value",
    "text": "Properties of Expected Value\nLinearity of Expectation: 1. \\(E[c] = c\\) (constant) 2. \\(E[cX] = c \\cdot E[X]\\) (scaling) 3. \\(E[X + Y] = E[X] + E[Y]\\) (additivity) 4. \\(E[aX + bY + c] = aE[X] + bE[Y] + c\\)\nImportant: Property 3 holds even if \\(X\\) and \\(Y\\) are dependent!\nVariance Properties: - \\(\\text{Var}(aX + b) = a^2 \\text{Var}(X)\\) - \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\) (if \\(X\\) and \\(Y\\) are independent)"
  },
  {
    "objectID": "lecture7-clean.html#key-takeaways",
    "href": "lecture7-clean.html#key-takeaways",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRandom variables transform outcomes into numbers for mathematical analysis\nPMF gives probabilities for specific values; CDF gives cumulative probabilities\nExpected value is the long-run average; variance measures spread\nChoose distributions based on the underlying process:\n\nBernoulli for single trials\nBinomial for fixed trials\nGeometric for waiting times\n\nPoisson for rates\n\nLaw of Large Numbers connects theoretical expectations with observed averages"
  },
  {
    "objectID": "lecture7-clean.html#looking-ahead",
    "href": "lecture7-clean.html#looking-ahead",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture: Continuous Random Variables - Probability density functions (PDFs) - Normal distribution - Exponential distribution\n- Central Limit Theorem applications\nConnection: Discrete distributions often approximate continuous ones, and vice versa"
  },
  {
    "objectID": "lecture7-clean.html#questions",
    "href": "lecture7-clean.html#questions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: [Your office hours]\nEmail: [Your email]\nNext Class: Continuous Random Variables\nInteractive charts and visualizations will be loaded via the included JavaScript libraries."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#expected-value-and-variance",
    "href": "files/lecture_notes/lecture7/lecture7.html#expected-value-and-variance",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\nThe expected value of a discrete random variable \\(X\\) is:\n\\[E[X] = \\mu = \\sum_{\\text{all } x} x \\cdot P(X = x)\\]\nThe variance of a random variable \\(X\\) measures spread around the mean:\n\\[\\text{Var}(X) = \\sigma^2 = E[(X - \\mu)^2] = E[X^2] - (E[X])^2\\]\n\nExpected value represents the long-run average if we repeat the experiment many times."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#law-of-large-numbers-demo",
    "href": "files/lecture_notes/lecture7/lecture7.html#law-of-large-numbers-demo",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Law of Large Numbers Demo",
    "text": "Law of Large Numbers Demo\n\nLaw of Large Numbers Demo\n\n\nRun Simulation\n\n 100 trials 500 trials 1000 trials 5000 trials \n\n\n\n\n\nWatch how the sample mean converges to the expected value! {.smaller}"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#common-discrete-distributions",
    "href": "files/lecture_notes/lecture7/lecture7.html#common-discrete-distributions",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\n\nBernoulli\nSingle trial, two outcomes\nParameters: \\(p\\) (success probability)\nPMF: \\(P(X = 1) = p\\), \\(P(X = 0) = 1-p\\)\nMean: \\(p\\)\nVariance: \\(p(1-p)\\)\n\n\nBinomial\n\\(n\\) independent Bernoulli trials\nParameters: \\(n\\) (trials), \\(p\\) (success prob.)\nPMF: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\)\nMean: \\(np\\)\nVariance: \\(np(1-p)\\)\n\n\nGeometric\nTrials until first success\nParameters: \\(p\\) (success probability)\nPMF: \\(P(X = k) = (1-p)^{k-1} p\\)\nMean: \\(1/p\\)\nVariance: \\((1-p)/p^2\\)\n\n\nPoisson\nEvents in fixed interval\nParameters: \\(\\lambda\\) (average rate)\nPMF: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\nMean: \\(\\lambda\\)\nVariance: \\(\\lambda\\)\n\n:::"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#interactive-distribution-explorer",
    "href": "files/lecture_notes/lecture7/lecture7.html#interactive-distribution-explorer",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Interactive Distribution Explorer",
    "text": "Interactive Distribution Explorer\n\nDistribution Visualizer\n\n Binomial Geometric Poisson  Parameter 1:  Parameter 2:"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-1-red-and-blue-balls",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-1-red-and-blue-balls",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 1: Red and Blue Balls",
    "text": "Practice Problem 1: Red and Blue Balls\n\nA box contains 3 red balls and 2 blue balls. Two balls are drawn without replacement. Let \\(X\\) = number of red balls drawn. Find the PMF of \\(X\\).\n\nShow Solution\n\n\nSolution. \\(X\\) can take values 0, 1, or 2.\n\\(P(X = 0) = \\frac{\\binom{3}{0}\\binom{2}{2}}{\\binom{5}{2}} = \\frac{1 \\times 1}{10} = \\frac{1}{10}\\)\n\\(P(X = 1) = \\frac{\\binom{3}{1}\\binom{2}{1}}{\\binom{5}{2}} = \\frac{3 \\times 2}{10} = \\frac{6}{10}\\)\n\\(P(X = 2) = \\frac{\\binom{3}{2}\\binom{2}{0}}{\\binom{5}{2}} = \\frac{3 \\times 1}{10} = \\frac{3}{10}\\)\nCheck: \\(\\frac{1}{10} + \\frac{6}{10} + \\frac{3}{10} = 1\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-2-expected-value",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-2-expected-value",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 2: Expected Value",
    "text": "Practice Problem 2: Expected Value\n\nUsing the red balls example from Problem 1, find \\(E[X]\\) and \\(\\text{Var}(X)\\).\n\nShow Solution\n\n\nSolution. Expected Value: \\[E[X] = 0 \\times \\frac{1}{10} + 1 \\times \\frac{6}{10} + 2 \\times \\frac{3}{10} = 0 + \\frac{6}{10} + \\frac{6}{10} = 1.2\\]\nVariance: \\[E[X^2] = 0^2 \\times \\frac{1}{10} + 1^2 \\times \\frac{6}{10} + 2^2 \\times \\frac{3}{10} = 0 + \\frac{6}{10} + \\frac{12}{10} = 1.8\\]\n\\[\\text{Var}(X) = E[X^2] - (E[X])^2 = 1.8 - (1.2)^2 = 1.8 - 1.44 = 0.36\\]\nStandard Deviation: \\(\\sigma = \\sqrt{0.36} = 0.6\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#practice-problem-3-binomial-application",
    "href": "files/lecture_notes/lecture7/lecture7.html#practice-problem-3-binomial-application",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Practice Problem 3: Binomial Application",
    "text": "Practice Problem 3: Binomial Application\n\nA student takes a 10-question multiple choice quiz with 4 options per question. If the student guesses randomly, what’s the probability of getting exactly 3 correct?\n\nShow Solution\n\n\nSolution. This is a binomial distribution with \\(n = 10\\), \\(p = 1/4 = 0.25\\)\n\\[P(X = 3) = \\binom{10}{3} \\times (0.25)^3 \\times (0.75)^7\\]\n\\[P(X = 3) = 120 \\times 0.015625 \\times 0.1335 \\approx 0.2503\\]\nSo there’s about a 25% chance of getting exactly 3 correct by guessing."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#key-takeaways",
    "href": "files/lecture_notes/lecture7/lecture7.html#key-takeaways",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nMain Concepts\n\nRandom variables transform outcomes into numbers for mathematical analysis\nPMF gives probabilities for specific values; CDF gives cumulative probabilities\nExpected value is the long-run average; variance measures spread\n\n\nDistribution Selection\nChoose distributions based on the underlying process:\n\nBernoulli for single trials\nBinomial for fixed trials\nGeometric for waiting times\nPoisson for rates\n\nKey Principle\n\nLaw of Large Numbers connects theoretical expectations with observed averages"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#section",
    "href": "files/lecture_notes/lecture7/lecture7.html#section",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "",
    "text": "Die Roll Example\n\n\nSample Space (Outcomes)\n\n\n⚀\n\n\n⚁\n\n\n⚂\n\n\n⚃\n\n\n⚄\n\n\n⚅\n\n\n\n→\n\n\nRandom Variable X\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\nRandom Variable X maps each die face to its numerical value.\n\n\n:::"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#what-is-a-random-variable-1",
    "href": "files/lecture_notes/lecture7/lecture7.html#what-is-a-random-variable-1",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\n\nDefinition: A random variable is a function that assigns a numerical value to each outcome of a random experiment.\nNotation: Usually denoted by capital letters \\(X\\), \\(Y\\), \\(Z\\)\nKey insight: Random variables transform outcomes into numbers, making statistical analysis possible"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#discrete-random-variable",
    "href": "files/lecture_notes/lecture7/lecture7.html#discrete-random-variable",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Discrete Random Variable",
    "text": "Discrete Random Variable\nTakes on a countable number of values\nCan list all possible values\n\nExamples:\n• Dice rolls: {1, 2, 3, 4, 5, 6}\n• Number of emails: {0, 1, 2, 3, …}\n• Quiz scores: {0, 1, 2, …, 10}"
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-8",
    "href": "files/lecture_notes/lecture7/lecture7.html#welcome-to-lecture-8",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Welcome to Lecture 8",
    "text": "Welcome to Lecture 8\nDiscrete Random Variables\nFrom outcomes to numbers: quantifying randomness"
  },
  {
    "objectID": "test-minimal.html",
    "href": "test-minimal.html",
    "title": "Minimal Test",
    "section": "",
    "text": "This is a simple test.\n\nThis is a test div."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#die-roll-example-mapping-outcomes-to-numbers",
    "href": "files/lecture_notes/lecture7/lecture7.html#die-roll-example-mapping-outcomes-to-numbers",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Die Roll Example: Mapping Outcomes to Numbers",
    "text": "Die Roll Example: Mapping Outcomes to Numbers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\nRandom Variable X maps each die face to its numerical value."
  },
  {
    "objectID": "files/lecture_notes/lecture7/lecture7.html#resources",
    "href": "files/lecture_notes/lecture7/lecture7.html#resources",
    "title": "PSTAT 5A: Discrete Random Variables",
    "section": "Resources",
    "text": "Resources\n\n\n\n Read OpenIntro Statistics Chapter 3 section 3.4\n\n\n Random Variable - Treena Courses\n\n\n Random Variables and Probability Functions\n\n\n Random Variables - Distribution and Expectation\n\n\n Khan Academy - Unit9: Random Variables"
  },
  {
    "objectID": "resources.html#week-3-conditional-probability-counting-discrete-random-variables",
    "href": "resources.html#week-3-conditional-probability-counting-discrete-random-variables",
    "title": "Week 4: Continuous Random Variables & Confidence Intervals",
    "section": "Week 3: Conditional Probability, Counting & Discrete Random Variables",
    "text": "Week 3: Conditional Probability, Counting & Discrete Random Variables"
  },
  {
    "objectID": "resources.html#advanced-probability-discrete-distributions",
    "href": "resources.html#advanced-probability-discrete-distributions",
    "title": "Course Resources",
    "section": "Advanced Probability & Discrete Distributions",
    "text": "Advanced Probability & Discrete Distributions\nThis week deepens your understanding of conditional probability and Bayes’ theorem, introduces counting principles (permutations and combinations), and explores discrete random variables including their probability mass functions, expected values, and common distributions."
  },
  {
    "objectID": "files/labs/lab4/lab4.html",
    "href": "files/labs/lab4/lab4.html",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "",
    "text": "Welcome to Lab 4! Today we’re going to learn about probability using Python. Don’t worry if you’re new to this - we’ll go step by step and you’ll be guided through everything!"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#probability-mass-function-pmf",
    "href": "files/labs/lab4/lab4.html#probability-mass-function-pmf",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nFor a discrete random variable X, the Probability Mass Function P(X = k) gives the probability that X takes the value k.\nKey properties of a PMF: - P(X = k) ≥ 0 for all k - Σ P(X = k) = 1 (sum over all possible values)\nLet’s start with a simple example:\n\n# Simple discrete random variable: rolling a fair die\n# X can take values 1, 2, 3, 4, 5, 6 each with probability 1/6\n\ndie_values = [1, 2, 3, 4, 5, 6]\ndie_probabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n\nprint(\"Die Values:\", die_values)\nprint(\"Probabilities:\", die_probabilities)\nprint(\"Sum of probabilities:\", sum(die_probabilities))\n\nDie Values: [1, 2, 3, 4, 5, 6]\nProbabilities: [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\nSum of probabilities: 1.0\n\n\nLet’s visualize this distribution:\n\nplt.figure(figsize=(8, 5))\nplt.bar(die_values, die_probabilities, alpha=0.7, color='skyblue', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Probability')\nplt.title('Probability Mass Function - Fair Die')\nplt.ylim(0, 0.25)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-1",
    "href": "files/labs/lab4/lab4.html#task-1",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 1",
    "text": "Task 1\n\n⏱️ Estimated time: 5 minutes\n\nConsider a biased coin where P(Heads) = 0.7 and P(Tails) = 0.3. Let X be a random variable where X = 1 for Heads and X = 0 for Tails.\n\nCreate lists for the values and probabilities of X\nVerify that the probabilities sum to 1\nCreate a bar plot showing the PMF"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#expected-value-mean",
    "href": "files/labs/lab4/lab4.html#expected-value-mean",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Expected Value (Mean)",
    "text": "Expected Value (Mean)\nThe expected value of a discrete random variable X is:\n\n\\[E[X] = \\mu = \\sum_{k} k \\cdot P(X = k)\\]"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#variance",
    "href": "files/labs/lab4/lab4.html#variance",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Variance",
    "text": "Variance\nThe variance of X is:\n\n\\[\\text{Var}(X) = \\sigma^2 = E[X^2] - (E[X])^2 = \\sum_{k} k^2 \\cdot P(X = k) - \\mu^2\\]\n\nLet’s calculate these for our fair die example:\n\n# Expected value of a fair die\nexpected_value = sum(k * p for k, p in zip(die_values, die_probabilities))\nprint(f\"Expected value of fair die: {expected_value}\")\n\n# Variance calculation\n# First calculate E[X^2]\nexpected_x_squared = sum(k**2 * p for k, p in zip(die_values, die_probabilities))\nvariance = expected_x_squared - expected_value**2\n\nprint(f\"E[X^2]: {expected_x_squared}\")\nprint(f\"Variance: {variance}\")\nprint(f\"Standard deviation: {np.sqrt(variance)}\")\n\nExpected value of fair die: 3.5\nE[X^2]: 15.166666666666666\nVariance: 2.916666666666666\nStandard deviation: 1.707825127659933"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-2",
    "href": "files/labs/lab4/lab4.html#task-2",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 2",
    "text": "Task 2\n\n⏱️ Estimated time: 4 minutes\n\nCalculate the expected value and variance for the biased coin from Task 1 (where X = 1 for Heads with probability 0.7, and X = 0 for Tails with probability 0.3).\nShow your calculations step by step."
  },
  {
    "objectID": "files/labs/lab4/lab4.html#bernoulli-distribution",
    "href": "files/labs/lab4/lab4.html#bernoulli-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\n⏱️ Estimated time: 6 minutes\n\nA Bernoulli distribution models a single trial with two outcomes: success (1) or failure (0).\n\nParameter: p (probability of success)\nPMF: P(X = 1) = p, P(X = 0) = 1-p\nE[X] = p\nVar(X) = p(1-p)\n\n\n# Using scipy.stats for Bernoulli distribution\np = 0.3  # probability of success\n\n# Create Bernoulli distribution object\nbern = stats.bernoulli(p)\n\n# Calculate probabilities\nprint(f\"P(X = 0) = {bern.pmf(0)}\")\nprint(f\"P(X = 1) = {bern.pmf(1)}\")\n\n# Expected value and variance\nprint(f\"Expected value: {bern.mean()}\")\nprint(f\"Variance: {bern.var()}\")\n\nP(X = 0) = 0.6999999999999997\nP(X = 1) = 0.3\nExpected value: 0.3\nVariance: 0.21\n\n\nLet’s visualize several Bernoulli distributions:\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\np_values = [0.2, 0.5, 0.8]\n\nfor i, p in enumerate(p_values):\n    bern = stats.bernoulli(p)\n    x_vals = [0, 1]\n    y_vals = [bern.pmf(x) for x in x_vals]\n    \n    axes[i].bar(x_vals, y_vals, alpha=0.7, color='lightcoral', edgecolor='black')\n    axes[i].set_xlabel('Value')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Bernoulli(p={p})')\n    axes[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#binomial-distribution",
    "href": "files/labs/lab4/lab4.html#binomial-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n⏱️ Estimated time: 8 minutes\n\nA Binomial distribution models the number of successes in n independent Bernoulli trials.\n\nParameters: n (number of trials), p (probability of success)\nPMF: P(X = k) = C(n,k) × p^k × (1-p)^(n-k)\nE[X] = np\nVar(X) = np(1-p)\n\n\n# Binomial distribution example: 10 coin flips with p = 0.5\nn = 10\np = 0.5\n\nbinom = stats.binom(n, p)\n\n# Calculate probabilities for different numbers of successes\nk_values = range(0, n+1)\nprobabilities = [binom.pmf(k) for k in k_values]\n\n# Display some key probabilities\nprint(f\"P(X = 5) = {binom.pmf(5):.4f}\")\nprint(f\"P(X ≤ 3) = {binom.cdf(3):.4f}\")\nprint(f\"P(X ≥ 7) = {1 - binom.cdf(6):.4f}\")\n\nprint(f\"\\nExpected value: {binom.mean()}\")\nprint(f\"Variance: {binom.var()}\")\nprint(f\"Standard deviation: {binom.std()}\")\n\nP(X = 5) = 0.2461\nP(X ≤ 3) = 0.1719\nP(X ≥ 7) = 0.1719\n\nExpected value: 5.0\nVariance: 2.5\nStandard deviation: 1.5811388300841898\n\n\nLet’s visualize the binomial distribution:\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Successes (k)')\nplt.ylabel('Probability')\nplt.title(f'Binomial Distribution (n={n}, p={p})')\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-3",
    "href": "files/labs/lab4/lab4.html#task-3",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 3",
    "text": "Task 3\n\n⏱️ Estimated time: 6 minutes\n\nA basketball player makes 70% of their free throws. They take 15 free throws.\n\nWhat is the probability they make exactly 10 free throws?\nWhat is the probability they make at least 12 free throws?\nWhat is the expected number of free throws made?\nCreate a bar plot showing the PMF for this scenario"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#geometric-distribution",
    "href": "files/labs/lab4/lab4.html#geometric-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\n⏱️ Estimated time: 6 minutes\n\nA Geometric distribution models the number of trials needed to get the first success.\n\nParameter: p (probability of success)\nPMF: P(X = k) = (1-p)^(k-1) × p\nE[X] = 1/p\nVar(X) = (1-p)/p²\n\n\n# Geometric distribution: rolling a die until we get a 6\np = 1/6  # probability of rolling a 6\n\ngeom = stats.geom(p)\n\n# Calculate probabilities for first few trials\nk_values = range(1, 21)  # trials 1 to 20\nprobabilities = [geom.pmf(k) for k in k_values]\n\nprint(f\"P(X = 1) = {geom.pmf(1):.4f}\")  # Success on first trial\nprint(f\"P(X = 6) = {geom.pmf(6):.4f}\")  # Success on sixth trial\nprint(f\"P(X ≤ 10) = {geom.cdf(10):.4f}\")  # Success within 10 trials\n\nprint(f\"\\nExpected value: {geom.mean():.2f}\")\nprint(f\"Variance: {geom.var():.2f}\")\n\nP(X = 1) = 0.1667\nP(X = 6) = 0.0670\nP(X ≤ 10) = 0.8385\n\nExpected value: 6.00\nVariance: 30.00\n\n\nVisualizing the geometric distribution:\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='orange', edgecolor='black')\nplt.xlabel('Trial Number (k)')\nplt.ylabel('Probability')\nplt.title(f'Geometric Distribution (p={p:.3f})')\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#poisson-distribution",
    "href": "files/labs/lab4/lab4.html#poisson-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n⏱️ Estimated time: 8 minutes\n\nA Poisson distribution models the number of events occurring in a fixed interval when events occur independently at a constant average rate.\n\nParameter: λ (lambda, average rate)\nPMF: P(X = k) = (λ^k × e^(-λ)) / k!\nE[X] = λ\nVar(X) = λ\n\n\n# Poisson distribution: number of customers arriving per hour\nlam = 3.5  # average 3.5 customers per hour\n\npoisson = stats.poisson(lam)\n\n# Calculate probabilities\nk_values = range(0, 15)\nprobabilities = [poisson.pmf(k) for k in k_values]\n\nprint(f\"P(X = 0) = {poisson.pmf(0):.4f}\")  # No customers\nprint(f\"P(X = 3) = {poisson.pmf(3):.4f}\")  # Exactly 3 customers\nprint(f\"P(X ≤ 5) = {poisson.cdf(5):.4f}\")  # At most 5 customers\nprint(f\"P(X ≥ 6) = {1 - poisson.cdf(5):.4f}\")  # At least 6 customers\n\nprint(f\"\\nExpected value: {poisson.mean()}\")\nprint(f\"Variance: {poisson.var()}\")\n\nP(X = 0) = 0.0302\nP(X = 3) = 0.2158\nP(X ≤ 5) = 0.8576\nP(X ≥ 6) = 0.1424\n\nExpected value: 3.5\nVariance: 3.5\n\n\nVisualizing different Poisson distributions:\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nlambda_values = [1, 3.5, 8]\n\nfor i, lam in enumerate(lambda_values):\n    poisson = stats.poisson(lam)\n    k_vals = range(0, int(lam + 4*np.sqrt(lam)))\n    probs = [poisson.pmf(k) for k in k_vals]\n    \n    axes[i].bar(k_vals, probs, alpha=0.7, color='purple', edgecolor='black')\n    axes[i].set_xlabel('Number of Events (k)')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Poisson(λ={lam})')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-4",
    "href": "files/labs/lab4/lab4.html#task-4",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 4",
    "text": "Task 4\n\n⏱️ Estimated time: 7 minutes\n\nA call center receives an average of 5 calls per minute.\n\nWhat is the probability of receiving exactly 7 calls in a minute?\nWhat is the probability of receiving no calls in a minute?\nWhat is the probability of receiving more than 8 calls in a minute?\nPlot the PMF for k = 0 to 15 calls"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-5",
    "href": "files/labs/lab4/lab4.html#task-5",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 5",
    "text": "Task 5\n\n⏱️ Estimated time: 8 minutes\n\nDistribution Identification Practice\nFor each scenario below, identify the appropriate distribution and calculate the requested probability:\n\nScenario A: You flip a fair coin 20 times. What’s the probability of getting exactly 12 heads?\nScenario B: You keep rolling a die until you get a 6. What’s the probability it takes exactly 4 rolls?\nScenario C: A website gets an average of 2 visitors per minute. What’s the probability of getting exactly 3 visitors in a given minute?\nScenario D: A quality control inspector tests items where 5% are defective. What’s the probability the first defective item is found on the 8th test?"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#task-6",
    "href": "files/labs/lab4/lab4.html#task-6",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 6",
    "text": "Task 6\n\n⏱️ Estimated time: 8 minutes\n\nSimulation Project:\nSimulate the basketball free throw scenario from Task 3 (15 shots, 70% success rate):\n\nSimulate this scenario 1000 times\nCalculate the proportion of simulations where the player made exactly 10 shots\nCompare this to the theoretical probability you calculated earlier\nCreate a histogram of the simulation results and overlay the theoretical PMF"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#final-challenge",
    "href": "files/labs/lab4/lab4.html#final-challenge",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Final Challenge",
    "text": "Final Challenge\n\n⏱️ Estimated time: 10 minutes\n\nReal-World Application:\nA customer service center has the following characteristics: - 20% of calls result in a sale (Bernoulli process) - Calls arrive at an average rate of 4 per hour (Poisson process) - Agents keep working until they make their first sale of the day (Geometric process)\nCalculate: 1. In a day with 8 hours of operation, what’s the expected number of calls? 2. What’s the probability that exactly 2 of the next 10 calls result in sales? 3. What’s the expected number of calls an agent needs to handle to make their first sale? 4. Create a comprehensive visualization showing all three distributions"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html",
    "href": "files/labs/lab4/lab4_sln.html",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "",
    "text": "# Load our tools (libraries)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport pandas as pd\n\n# Make our graphs look nice\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\nprint(\"✅ All tools loaded successfully!\")\n\n✅ All tools loaded successfully!"
  },
  {
    "objectID": "files/labs/lab4/test.html",
    "href": "files/labs/lab4/test.html",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "",
    "text": "Welcome to Lab 4! Today we’re going to learn about probability using Python. Don’t worry if you’re new to this - we’ll go step by step and you’ll be guided through everything!"
  },
  {
    "objectID": "files/labs/lab4/test.html#step-1-a-fair-coin",
    "href": "files/labs/lab4/test.html#step-1-a-fair-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Step 1: A Fair Coin",
    "text": "Step 1: A Fair Coin\nWhen we flip a fair coin: - Heads happens 50% of the time - Tails happens 50% of the time\nLet’s tell Python about this:\n\n# A fair coin - just run this code!\noutcomes = [\"Tails\", \"Heads\"]\nprobabilities = [0.5, 0.5]  # 50% each\n\nprint(\"Possible outcomes:\", outcomes)\nprint(\"Probabilities:\", probabilities)\nprint(\"Total probability:\", sum(probabilities))  # Should be 1.0\n\nPossible outcomes: ['Tails', 'Heads']\nProbabilities: [0.5, 0.5]\nTotal probability: 1.0\n\n\nLet’s make a plot of this:\n\n# Make a bar chart - just run this code!\nplt.figure(figsize=(8, 5))\nplt.bar(outcomes, probabilities, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('Fair Coin: 50% Heads, 50% Tails')\nplt.ylabel('Probability')\nplt.ylim(0, 0.6)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/test.html#step-2-your-turn---a-biased-coin",
    "href": "files/labs/lab4/test.html#step-2-your-turn---a-biased-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Step 2: Your Turn - A Biased Coin",
    "text": "Step 2: Your Turn - A Biased Coin\nNow let’s try a biased coin (not fair). This coin lands on Heads 70% of the time!\n\nTask 1: Fill in the Biased Coin\n\n⏱️ Estimated time: 3 minutes\n\nInstructions: Replace the ___ with the correct numbers.\n\n# A biased coin that lands Heads 70% of the time\nbiased_outcomes = [\"Tails\", \"Heads\"]\nbiased_probabilities = [___, ___]  # Hint: If Heads is 70%, what is Tails?\n\nprint(\"Biased coin outcomes:\", biased_outcomes)\nprint(\"Biased coin probabilities:\", biased_probabilities)\nprint(\"Total probability:\", sum(biased_probabilities))\n\n# Check your answer: this should print 1.0\n\nNow make a graph of your biased coin:\n\n# Copy the graphing code from above and modify the title\nplt.figure(figsize=(8, 5))\nplt.bar(___, ___, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('___')  # Change this title to describe your biased coin\nplt.ylabel('Probability')\nplt.ylim(0, 0.8)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/test.html#example-expected-value-of-a-fair-coin",
    "href": "files/labs/lab4/test.html#example-expected-value-of-a-fair-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: Expected Value of a Fair Coin",
    "text": "Example: Expected Value of a Fair Coin\nLet’s say Heads = 1 point and Tails = 0 points.\n\n# Expected value calculation for fair coin\n# Formula: Expected Value = (Value1 × Probability1) + (Value2 × Probability2)\n\ncoin_values = [0, 1]  # Tails = 0, Heads = 1\ncoin_probs = [0.5, 0.5]\n\nexpected_value = 0 * 0.5 + 1 * 0.5\nprint(f\"Expected value of fair coin: {expected_value}\")\n\n# This means: if we flip many times, we expect about 0.5 points per flip on average\n\nExpected value of fair coin: 0.5\n\n\n\nTask 2: Expected Value of Your Biased Coin\n\n⏱️ Estimated time: 4 minutes\n\nCalculate the expected value for your biased coin (Heads 70%, Tails 30%).\nStep 1: Fill in the calculation\n\n# Expected value for biased coin\n# Values: Tails = 0, Heads = 1\n# Probabilities: Tails = 30%, Heads = 70%\n\nexpected_biased = 0 * ___ + 1 * ___\nprint(f\"Expected value of biased coin: {expected_biased}\")\n\nStep 2: What does this mean?\n\nprint(f\"This means: if we flip the biased coin many times,\")\nprint(f\"we expect about {expected_biased} points per flip on average.\")\nprint(f\"Since this is closer to 1 than 0.5, the coin favors ___\")  # Fill in: Heads or Tails?"
  },
  {
    "objectID": "files/labs/lab4/test.html#bernoulli-distribution-single-coin-flip",
    "href": "files/labs/lab4/test.html#bernoulli-distribution-single-coin-flip",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Bernoulli Distribution (Single Coin Flip)",
    "text": "Bernoulli Distribution (Single Coin Flip)\nA Bernoulli distribution is just a fancy name for one coin flip.\n\n# Create a Bernoulli distribution for a fair coin\nfair_coin = stats.bernoulli(0.5)  # 0.5 = 50% chance of success (Heads)\n\n# Ask for probabilities\nprob_tails = fair_coin.pmf(0)  # pmf = \"probability mass function\"\nprob_heads = fair_coin.pmf(1)\n\nprint(f\"Probability of Tails (0): {prob_tails}\")\nprint(f\"Probability of Heads (1): {prob_heads}\")\n\n# Get expected value automatically!\nprint(f\"Expected value: {fair_coin.mean()}\")\n\nProbability of Tails (0): 0.4999999999999999\nProbability of Heads (1): 0.5\nExpected value: 0.5\n\n\n\nTask 3: Your Biased Coin with Python Tools\n\n⏱️ Estimated time: 4 minutes\n\nCreate a Bernoulli distribution for your biased coin (70% Heads).\n\n# Create Bernoulli distribution for biased coin\nbiased_coin = stats.bernoulli(___)  # Fill in: what's the probability of Heads?\n\n# Calculate probabilities\nprob_tails_biased = biased_coin.pmf(0)\nprob_heads_biased = biased_coin.pmf(1)\n\nprint(f\"Biased coin - Probability of Tails: {prob_tails_biased}\")\nprint(f\"Biased coin - Probability of Heads: {prob_heads_biased}\")\nprint(f\"Expected value: {biased_coin.mean()}\")\n\nQuestion: Does this match what you calculated by hand in Task 2? ___"
  },
  {
    "objectID": "files/labs/lab4/test.html#visualizing-different-coins",
    "href": "files/labs/lab4/test.html#visualizing-different-coins",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Visualizing Different Coins",
    "text": "Visualizing Different Coins\nLet’s compare three different coins:\n\n# Compare three coins with different bias\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Three different coins\ncoin_types = [\n    {\"prob\": 0.2, \"name\": \"Mostly Tails\"},\n    {\"prob\": 0.5, \"name\": \"Fair Coin\"}, \n    {\"prob\": 0.8, \"name\": \"Mostly Heads\"}\n]\n\nfor i, coin in enumerate(coin_types):\n    # Create the distribution\n    distribution = stats.bernoulli(coin[\"prob\"])\n    \n    # Get probabilities\n    probs = [distribution.pmf(0), distribution.pmf(1)]\n    \n    # Make bar chart\n    axes[i].bar([0, 1], probs, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{coin[\"name\"]}\\n(p = {coin[\"prob\"]})')\n    axes[i].set_xlabel('Outcome')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_xticks([0, 1])\n    axes[i].set_xticklabels(['Tails', 'Heads'])\n    axes[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/test.html#example-5-fair-coin-flips",
    "href": "files/labs/lab4/test.html#example-5-fair-coin-flips",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: 5 Fair Coin Flips",
    "text": "Example: 5 Fair Coin Flips\n\n# Binomial distribution: 5 fair coin flips\nn_flips = 5        # number of flips\np_heads = 0.5      # probability of heads each time\n\nfive_flips = stats.binom(n_flips, p_heads)\n\nprint(\"Flipping 5 fair coins...\")\nprint(f\"Probability of 0 heads: {five_flips.pmf(0):.4f}\")\nprint(f\"Probability of 1 head:  {five_flips.pmf(1):.4f}\")\nprint(f\"Probability of 2 heads: {five_flips.pmf(2):.4f}\")\nprint(f\"Probability of 3 heads: {five_flips.pmf(3):.4f}\")\nprint(f\"Probability of 4 heads: {five_flips.pmf(4):.4f}\")\nprint(f\"Probability of 5 heads: {five_flips.pmf(5):.4f}\")\n\nprint(f\"\\nExpected number of heads: {five_flips.mean()}\")\n\nFlipping 5 fair coins...\nProbability of 0 heads: 0.0312\nProbability of 1 head:  0.1562\nProbability of 2 heads: 0.3125\nProbability of 3 heads: 0.3125\nProbability of 4 heads: 0.1562\nProbability of 5 heads: 0.0312\n\nExpected number of heads: 2.5\n\n\nLet’s make a graph:\n\n# Graph showing all possibilities\npossible_heads = [0, 1, 2, 3, 4, 5]\nprobabilities = [five_flips.pmf(k) for k in possible_heads]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_heads, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.title('5 Fair Coin Flips: How Many Heads?')\nplt.axvline(x=five_flips.mean(), color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {five_flips.mean()}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nTask 4: Basketball Free Throws\n\n⏱️ Estimated time: 8 minutes\n\nA basketball player makes 60% of their free throws. They shoot 10 free throws. How many will they make?\nStep 1: Set up the problem\n\n# Basketball free throws\nn_shots = ___      # How many shots? (Fill in)\np_make = ___       # Probability of making each shot? (Fill in as decimal)\n\nbasketball = stats.binom(___, ___)  # Create the distribution\n\nStep 2: Answer these questions\n\n# a) What's the probability of making exactly 6 shots?\nprob_exactly_6 = basketball.pmf(___)\nprint(f\"Probability of exactly 6 makes: {prob_exactly_6:.4f}\")\n\n# b) How many shots do we expect them to make?\nexpected_makes = basketball.mean()\nprint(f\"Expected number of makes: {expected_makes}\")\n\n# c) What's the probability of making 8 or more shots?\nprob_8_or_more = (basketball.pmf(8) + basketball.pmf(9) + basketball.pmf(10))\nprint(f\"Probability of 8+ makes: {prob_8_or_more:.4f}\")\n\nStep 3: Make a graph (fill in the blanks)\n\n# Create bar chart\npossible_makes = list(range(0, ___))  # 0 to 10 makes\nprobabilities = [basketball.pmf(k) for k in possible_makes]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_makes, probabilities, alpha=0.7, color='___', edgecolor='black')\nplt.xlabel('___')\nplt.ylabel('___')\nplt.title('___')  # Give it a descriptive title\nplt.axvline(x=expected_makes, color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {expected_makes}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/test.html#example-store-customers",
    "href": "files/labs/lab4/test.html#example-store-customers",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: Store Customers",
    "text": "Example: Store Customers\n\n# On average, 4 customers enter the store per hour\naverage_customers = 4\n\nstore_customers = stats.poisson(average_customers)\n\nprint(\"Store customer arrivals per hour:\")\nprint(f\"Probability of 0 customers: {store_customers.pmf(0):.4f}\")\nprint(f\"Probability of 2 customers: {store_customers.pmf(2):.4f}\")\nprint(f\"Probability of 4 customers: {store_customers.pmf(4):.4f}\")\nprint(f\"Probability of 6 customers: {store_customers.pmf(6):.4f}\")\n\nprint(f\"\\nExpected customers per hour: {store_customers.mean()}\")\n\nStore customer arrivals per hour:\nProbability of 0 customers: 0.0183\nProbability of 2 customers: 0.1465\nProbability of 4 customers: 0.1954\nProbability of 6 customers: 0.1042\n\nExpected customers per hour: 4.0\n\n\nLet’s visualize this:\n\n# Graph customer arrivals\npossible_customers = list(range(0, 12))  # 0 to 11 customers\nprobabilities = [store_customers.pmf(k) for k in possible_customers]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_customers, probabilities, alpha=0.7, color='purple', edgecolor='black')\nplt.xlabel('Number of Customers per Hour')\nplt.ylabel('Probability')\nplt.title('Store Customer Arrivals (Average = 4 per hour)')\nplt.axvline(x=average_customers, color='red', linestyle='--', linewidth=2, \n            label=f'Average = {average_customers}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nTask 5: Your Email Inbox\n\n⏱️ Estimated time: 6 minutes\n\nYour email inbox receives an average of 3 emails per hour.\nStep 1: Set up the Poisson distribution\n\n# Email arrivals\naverage_emails = ___  # Fill in the average\nemail_arrivals = stats.poisson(___)\n\nStep 2: Answer these questions\n\n# a) Probability of getting exactly 3 emails in an hour\nprob_exactly_3 = email_arrivals.pmf(___)\nprint(f\"Probability of exactly 3 emails: {prob_exactly_3:.4f}\")\n\n# b) Probability of getting no emails (quiet hour!)\nprob_no_emails = email_arrivals.pmf(___)\nprint(f\"Probability of no emails: {prob_no_emails:.4f}\")\n\n# c) Expected number of emails per hour\nexpected_emails = email_arrivals.___()  # Fill in the method\nprint(f\"Expected emails per hour: {expected_emails}\")\n\nStep 3: Make a graph (copy and modify the code from the store example)\n\n# Your graph code here (copy from above and modify)"
  },
  {
    "objectID": "files/labs/lab4/test.html#whats-next",
    "href": "files/labs/lab4/test.html#whats-next",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn future labs, we’ll learn about: - Continuous probability distributions (like heights, weights) - Hypothesis testing (is a coin really fair?) - Confidence intervals (estimating with uncertainty)\nGreat job completing your first probability lab! 🎯"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#step-1-a-fair-coin",
    "href": "files/labs/lab4/lab4_sln.html#step-1-a-fair-coin",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Step 1: A Fair Coin",
    "text": "Step 1: A Fair Coin\n\n# A fair coin - just run this code!\noutcomes = [\"Tails\", \"Heads\"]\nprobabilities = [0.5, 0.5]  # 50% each\n\nprint(\"Possible outcomes:\", outcomes)\nprint(\"Probabilities:\", probabilities)\nprint(\"Total probability:\", sum(probabilities))  # Should be 1.0\n\nPossible outcomes: ['Tails', 'Heads']\nProbabilities: [0.5, 0.5]\nTotal probability: 1.0\n\n\n\n# Make a bar chart - just run this code!\nplt.figure(figsize=(8, 5))\nplt.bar(outcomes, probabilities, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('Fair Coin: 50% Heads, 50% Tails')\nplt.ylabel('Probability')\nplt.ylim(0, 0.6)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-1-solution-fill-in-the-biased-coin",
    "href": "files/labs/lab4/lab4_sln.html#task-1-solution-fill-in-the-biased-coin",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 1 Solution: Fill in the Biased Coin",
    "text": "Task 1 Solution: Fill in the Biased Coin\n\n# A biased coin that lands Heads 70% of the time\nbiased_outcomes = [\"Tails\", \"Heads\"]\nbiased_probabilities = [0.3, 0.7]  # If Heads is 70%, then Tails is 30%\n\nprint(\"Biased coin outcomes:\", biased_outcomes)\nprint(\"Biased coin probabilities:\", biased_probabilities)\nprint(\"Total probability:\", sum(biased_probabilities))\n\n# Check your answer: this should print 1.0\n\nBiased coin outcomes: ['Tails', 'Heads']\nBiased coin probabilities: [0.3, 0.7]\nTotal probability: 1.0\n\n\n\n# Graph of biased coin\nplt.figure(figsize=(8, 5))\nplt.bar(biased_outcomes, biased_probabilities, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('Biased Coin: 30% Tails, 70% Heads')\nplt.ylabel('Probability')\nplt.ylim(0, 0.8)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#example-expected-value-of-a-fair-coin",
    "href": "files/labs/lab4/lab4_sln.html#example-expected-value-of-a-fair-coin",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Example: Expected Value of a Fair Coin",
    "text": "Example: Expected Value of a Fair Coin\n\n# Expected value calculation for fair coin\ncoin_values = [0, 1]  # Tails = 0, Heads = 1\ncoin_probs = [0.5, 0.5]\n\nexpected_value = 0 * 0.5 + 1 * 0.5\nprint(f\"Expected value of fair coin: {expected_value}\")\n\nExpected value of fair coin: 0.5"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-2-solution-expected-value-of-biased-coin",
    "href": "files/labs/lab4/lab4_sln.html#task-2-solution-expected-value-of-biased-coin",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 2 Solution: Expected Value of Biased Coin",
    "text": "Task 2 Solution: Expected Value of Biased Coin\n\n# Expected value for biased coin\n# Values: Tails = 0, Heads = 1\n# Probabilities: Tails = 30%, Heads = 70%\n\nexpected_biased = 0 * 0.3 + 1 * 0.7\nprint(f\"Expected value of biased coin: {expected_biased}\")\n\nExpected value of biased coin: 0.7\n\n\n\nprint(f\"This means: if we flip the biased coin many times,\")\nprint(f\"we expect about {expected_biased} points per flip on average.\")\nprint(f\"Since this is closer to 1 than 0.5, the coin favors Heads\")\n\nThis means: if we flip the biased coin many times,\nwe expect about 0.7 points per flip on average.\nSince this is closer to 1 than 0.5, the coin favors Heads"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#bernoulli-distribution-example",
    "href": "files/labs/lab4/lab4_sln.html#bernoulli-distribution-example",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Bernoulli Distribution Example",
    "text": "Bernoulli Distribution Example\n\n# Create a Bernoulli distribution for a fair coin\nfair_coin = stats.bernoulli(0.5)  # 0.5 = 50% chance of success (Heads)\n\n# Ask for probabilities\nprob_tails = fair_coin.pmf(0)  # pmf = \"probability mass function\"\nprob_heads = fair_coin.pmf(1)\n\nprint(f\"Probability of Tails (0): {prob_tails}\")\nprint(f\"Probability of Heads (1): {prob_heads}\")\n\n# Get expected value automatically!\nprint(f\"Expected value: {fair_coin.mean()}\")\n\nProbability of Tails (0): 0.4999999999999999\nProbability of Heads (1): 0.5\nExpected value: 0.5"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-3-solution-biased-coin-with-python-tools",
    "href": "files/labs/lab4/lab4_sln.html#task-3-solution-biased-coin-with-python-tools",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 3 Solution: Biased Coin with Python Tools",
    "text": "Task 3 Solution: Biased Coin with Python Tools\n\n# Create Bernoulli distribution for biased coin\nbiased_coin = stats.bernoulli(0.7)  # 70% probability of Heads\n\n# Calculate probabilities\nprob_tails_biased = biased_coin.pmf(0)\nprob_heads_biased = biased_coin.pmf(1)\n\nprint(f\"Biased coin - Probability of Tails: {prob_tails_biased}\")\nprint(f\"Biased coin - Probability of Heads: {prob_heads_biased}\")\nprint(f\"Expected value: {biased_coin.mean()}\")\n\nBiased coin - Probability of Tails: 0.29999999999999993\nBiased coin - Probability of Heads: 0.7\nExpected value: 0.7\n\n\nAnswer: Yes, this matches what we calculated by hand in Task 2 (0.7)"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#visualizing-different-coins",
    "href": "files/labs/lab4/lab4_sln.html#visualizing-different-coins",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Visualizing Different Coins",
    "text": "Visualizing Different Coins\n\n# Compare three coins with different bias\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Three different coins\ncoin_types = [\n    {\"prob\": 0.2, \"name\": \"Mostly Tails\"},\n    {\"prob\": 0.5, \"name\": \"Fair Coin\"}, \n    {\"prob\": 0.8, \"name\": \"Mostly Heads\"}\n]\n\nfor i, coin in enumerate(coin_types):\n    # Create the distribution\n    distribution = stats.bernoulli(coin[\"prob\"])\n    \n    # Get probabilities\n    probs = [distribution.pmf(0), distribution.pmf(1)]\n    \n    # Make bar chart\n    axes[i].bar([0, 1], probs, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{coin[\"name\"]}\\n(p = {coin[\"prob\"]})')\n    axes[i].set_xlabel('Outcome')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_xticks([0, 1])\n    axes[i].set_xticklabels(['Tails', 'Heads'])\n    axes[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#example-5-fair-coin-flips",
    "href": "files/labs/lab4/lab4_sln.html#example-5-fair-coin-flips",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Example: 5 Fair Coin Flips",
    "text": "Example: 5 Fair Coin Flips\n\n# Binomial distribution: 5 fair coin flips\nn_flips = 5        # number of flips\np_heads = 0.5      # probability of heads each time\n\nfive_flips = stats.binom(n_flips, p_heads)\n\nprint(\"Flipping 5 fair coins...\")\nprint(f\"Probability of 0 heads: {five_flips.pmf(0):.4f}\")\nprint(f\"Probability of 1 head:  {five_flips.pmf(1):.4f}\")\nprint(f\"Probability of 2 heads: {five_flips.pmf(2):.4f}\")\nprint(f\"Probability of 3 heads: {five_flips.pmf(3):.4f}\")\nprint(f\"Probability of 4 heads: {five_flips.pmf(4):.4f}\")\nprint(f\"Probability of 5 heads: {five_flips.pmf(5):.4f}\")\n\nprint(f\"\\nExpected number of heads: {five_flips.mean()}\")\n\nFlipping 5 fair coins...\nProbability of 0 heads: 0.0312\nProbability of 1 head:  0.1562\nProbability of 2 heads: 0.3125\nProbability of 3 heads: 0.3125\nProbability of 4 heads: 0.1562\nProbability of 5 heads: 0.0312\n\nExpected number of heads: 2.5\n\n\n\n# Graph showing all possibilities\npossible_heads = [0, 1, 2, 3, 4, 5]\nprobabilities = [five_flips.pmf(k) for k in possible_heads]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_heads, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.title('5 Fair Coin Flips: How Many Heads?')\nplt.axvline(x=five_flips.mean(), color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {five_flips.mean()}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-4-solution-basketball-free-throws",
    "href": "files/labs/lab4/lab4_sln.html#task-4-solution-basketball-free-throws",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 4 Solution: Basketball Free Throws",
    "text": "Task 4 Solution: Basketball Free Throws\n\n# Basketball free throws\nn_shots = 10       # 10 shots\np_make = 0.6       # 60% chance of making each shot\n\nbasketball = stats.binom(n_shots, p_make)  # Create the distribution\n\n\n# a) What's the probability of making exactly 6 shots?\nprob_exactly_6 = basketball.pmf(6)\nprint(f\"Probability of exactly 6 makes: {prob_exactly_6:.4f}\")\n\n# b) How many shots do we expect them to make?\nexpected_makes = basketball.mean()\nprint(f\"Expected number of makes: {expected_makes}\")\n\n# c) What's the probability of making 8 or more shots?\nprob_8_or_more = (basketball.pmf(8) + basketball.pmf(9) + basketball.pmf(10))\nprint(f\"Probability of 8+ makes: {prob_8_or_more:.4f}\")\n\nProbability of exactly 6 makes: 0.2508\nExpected number of makes: 6.0\nProbability of 8+ makes: 0.1673\n\n\n\n# Create bar chart\npossible_makes = list(range(0, 11))  # 0 to 10 makes\nprobabilities = [basketball.pmf(k) for k in possible_makes]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_makes, probabilities, alpha=0.7, color='orange', edgecolor='black')\nplt.xlabel('Number of Successful Free Throws')\nplt.ylabel('Probability')\nplt.title('Basketball Player: 10 Free Throws (60% Success Rate)')\nplt.axvline(x=expected_makes, color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {expected_makes}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#example-store-customers",
    "href": "files/labs/lab4/lab4_sln.html#example-store-customers",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Example: Store Customers",
    "text": "Example: Store Customers\n\n# On average, 4 customers enter the store per hour\naverage_customers = 4\n\nstore_customers = stats.poisson(average_customers)\n\nprint(\"Store customer arrivals per hour:\")\nprint(f\"Probability of 0 customers: {store_customers.pmf(0):.4f}\")\nprint(f\"Probability of 2 customers: {store_customers.pmf(2):.4f}\")\nprint(f\"Probability of 4 customers: {store_customers.pmf(4):.4f}\")\nprint(f\"Probability of 6 customers: {store_customers.pmf(6):.4f}\")\n\nprint(f\"\\nExpected customers per hour: {store_customers.mean()}\")\n\nStore customer arrivals per hour:\nProbability of 0 customers: 0.0183\nProbability of 2 customers: 0.1465\nProbability of 4 customers: 0.1954\nProbability of 6 customers: 0.1042\n\nExpected customers per hour: 4.0\n\n\n\n# Graph customer arrivals\npossible_customers = list(range(0, 12))  # 0 to 11 customers\nprobabilities = [store_customers.pmf(k) for k in possible_customers]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_customers, probabilities, alpha=0.7, color='purple', edgecolor='black')\nplt.xlabel('Number of Customers per Hour')\nplt.ylabel('Probability')\nplt.title('Store Customer Arrivals (Average = 4 per hour)')\nplt.axvline(x=average_customers, color='red', linestyle='--', linewidth=2, \n            label=f'Average = {average_customers}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-5-solution-email-inbox",
    "href": "files/labs/lab4/lab4_sln.html#task-5-solution-email-inbox",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 5 Solution: Email Inbox",
    "text": "Task 5 Solution: Email Inbox\n\n# Email arrivals\naverage_emails = 3  # 3 emails per hour on average\nemail_arrivals = stats.poisson(average_emails)\n\n\n# a) Probability of getting exactly 3 emails in an hour\nprob_exactly_3 = email_arrivals.pmf(3)\nprint(f\"Probability of exactly 3 emails: {prob_exactly_3:.4f}\")\n\n# b) Probability of getting no emails (quiet hour!)\nprob_no_emails = email_arrivals.pmf(0)\nprint(f\"Probability of no emails: {prob_no_emails:.4f}\")\n\n# c) Expected number of emails per hour\nexpected_emails = email_arrivals.mean()\nprint(f\"Expected emails per hour: {expected_emails}\")\n\nProbability of exactly 3 emails: 0.2240\nProbability of no emails: 0.0498\nExpected emails per hour: 3.0\n\n\n\n# Graph for email arrivals\npossible_emails = list(range(0, 12))  # 0 to 11 emails\nprobabilities = [email_arrivals.pmf(k) for k in possible_emails]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_emails, probabilities, alpha=0.7, color='teal', edgecolor='black')\nplt.xlabel('Number of Emails per Hour')\nplt.ylabel('Probability')\nplt.title('Email Arrivals (Average = 3 per hour)')\nplt.axvline(x=average_emails, color='red', linestyle='--', linewidth=2, \n            label=f'Average = {average_emails}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-6-solution-which-distribution",
    "href": "files/labs/lab4/lab4_sln.html#task-6-solution-which-distribution",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task 6 Solution: Which Distribution?",
    "text": "Task 6 Solution: Which Distribution?\n\n# Scenario A - 8 coin flips, exactly 5 heads\nn = 8          # number of flips\np = 0.5        # probability of heads (fair coin)\nscenario_a = stats.binom(n, p)\n\nanswer_a = scenario_a.pmf(5)  # exactly 5 heads\nprint(f\"Scenario A answer: {answer_a:.4f}\")\n\nScenario A answer: 0.2187\n\n\n\n# Scenario B - Coffee shop customers\naverage_rate = 6  # customers per hour\nscenario_b = stats.poisson(average_rate)\n\nanswer_b = scenario_b.pmf(4)  # exactly 4 customers\nprint(f\"Scenario B answer: {answer_b:.4f}\")\n\nScenario B answer: 0.1339\n\n\n\n# Scenario C - Student homework problem\np_correct = 0.8  # 80% chance of getting it right\nscenario_c = stats.bernoulli(p_correct)\n\nanswer_c = scenario_c.pmf(1)  # gets it right (value = 1)\nprint(f\"Scenario C answer: {answer_c:.4f}\")\n\nScenario C answer: 0.8000"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#task-answers-summary",
    "href": "files/labs/lab4/lab4_sln.html#task-answers-summary",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Task Answers Summary:",
    "text": "Task Answers Summary:\nTask 1: Biased coin probabilities = [0.3, 0.7]\nTask 2: Expected value of biased coin = 0.7\nTask 3: Bernoulli(0.7) gives same expected value = 0.7 ✓\nTask 4: Basketball (10 shots, 60% success rate) - P(exactly 6 makes) = 0.2508 - Expected makes = 6.0 - P(8+ makes) = 0.1673\nTask 5: Email (3 per hour average) - P(exactly 3 emails) = 0.2240 - P(no emails) = 0.0498 - Expected emails = 3.0\nTask 6: Distribution Detective - Scenario A (8 flips, 5 heads): 0.2188 - Scenario B (6 customers/hour, exactly 4): 0.1339 - Scenario C (80% correct, gets it right): 0.8000\nTask 7: Simulation should give approximately 0.2461 for P(5 heads in 10 flips)"
  },
  {
    "objectID": "files/labs/lab4/lab4_sln.html#key-formulas-used",
    "href": "files/labs/lab4/lab4_sln.html#key-formulas-used",
    "title": "Lab 4 Solutions: Introduction to Probability with Python",
    "section": "Key Formulas Used:",
    "text": "Key Formulas Used:\n\nExpected Value: \\(E[X] = \\sum(\\text{value} \\times \\text{probability})\\)\nBernoulli: stats.bernoulli(p)\nBinomial: stats.binom(n, p)\nPoisson: stats.poisson(λ)\nPMF: .pmf(k) gives P(X = k)\nMean: .mean() gives expected value\n\nThis completes lab4 solutions! 🎯"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2_sln.html",
    "href": "files/labs/lab4/lab4_v2_sln.html",
    "title": "Lab 4 Solutions: Discrete Random Variables and Distributions",
    "section": "",
    "text": "Setup\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport pandas as pd\n\n# Set up plotting\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\n\n\n\nTask 1 Solution\nProblem: Consider a biased coin where P(Heads) = 0.7 and P(Tails) = 0.3. Let X be a random variable where X = 1 for Heads and X = 0 for Tails.\n\n# Task 1 Solution\n# 1. Create lists for values and probabilities\ncoin_values = [0, 1]  # 0 for Tails, 1 for Heads\ncoin_probabilities = [0.3, 0.7]  # P(Tails) = 0.3, P(Heads) = 0.7\n\nprint(\"Coin Values (X):\", coin_values)\nprint(\"Probabilities:\", coin_probabilities)\n\n# 2. Verify probabilities sum to 1\nprob_sum = sum(coin_probabilities)\nprint(f\"Sum of probabilities: {prob_sum}\")\nprint(f\"Probabilities sum to 1: {prob_sum == 1.0}\")\n\n# 3. Create bar plot\nplt.figure(figsize=(8, 5))\nplt.bar(coin_values, coin_probabilities, alpha=0.7, color='lightcoral', edgecolor='black', width=0.6)\nplt.xlabel('Value (X)')\nplt.ylabel('Probability')\nplt.title('PMF of Biased Coin (P(Heads) = 0.7)')\nplt.xticks([0, 1], ['Tails (0)', 'Heads (1)'])\nplt.ylim(0, 0.8)\nplt.show()\n\nCoin Values (X): [0, 1]\nProbabilities: [0.3, 0.7]\nSum of probabilities: 1.0\nProbabilities sum to 1: True\n\n\n\n\n\n\n\n\n\n\n\nTask 2 Solution\nProblem: Calculate the expected value and variance for the biased coin from Task 1.\n\n# Task 2 Solution\n# Expected value calculation: E[X] = Σ k * P(X = k)\nexpected_value = sum(k * p for k, p in zip(coin_values, coin_probabilities))\nprint(f\"Expected value E[X]: {expected_value}\")\n\n# Step-by-step calculation\nprint(\"\\nStep-by-step calculation:\")\nprint(f\"E[X] = 0 × P(X=0) + 1 × P(X=1)\")\nprint(f\"E[X] = 0 × 0.3 + 1 × 0.7 = {0*0.3 + 1*0.7}\")\n\n# Variance calculation: Var(X) = E[X²] - (E[X])²\n# First calculate E[X²]\nexpected_x_squared = sum(k**2 * p for k, p in zip(coin_values, coin_probabilities))\nvariance = expected_x_squared - expected_value**2\n\nprint(f\"\\nVariance calculation:\")\nprint(f\"E[X²] = 0² × 0.3 + 1² × 0.7 = {0**2 * 0.3 + 1**2 * 0.7}\")\nprint(f\"Var(X) = E[X²] - (E[X])² = {expected_x_squared} - ({expected_value})² = {variance}\")\nprint(f\"Standard deviation: {np.sqrt(variance):.4f}\")\n\n# Verify using theoretical formula for Bernoulli: Var(X) = p(1-p)\ntheoretical_variance = 0.7 * (1 - 0.7)\nprint(f\"\\nVerification using Bernoulli formula: Var(X) = p(1-p) = 0.7 × 0.3 = {theoretical_variance}\")\n\nExpected value E[X]: 0.7\n\nStep-by-step calculation:\nE[X] = 0 × P(X=0) + 1 × P(X=1)\nE[X] = 0 × 0.3 + 1 × 0.7 = 0.7\n\nVariance calculation:\nE[X²] = 0² × 0.3 + 1² × 0.7 = 0.7\nVar(X) = E[X²] - (E[X])² = 0.7 - (0.7)² = 0.21000000000000002\nStandard deviation: 0.4583\n\nVerification using Bernoulli formula: Var(X) = p(1-p) = 0.7 × 0.3 = 0.21000000000000002\n\n\n\n\nTask 3 Solution\nProblem: A basketball player makes 70% of their free throws. They take 15 free throws.\n\n# Task 3 Solution\n# This is a binomial distribution with n=15, p=0.7\nn = 15\np = 0.7\n\nbinom = stats.binom(n, p)\n\n# 1. Probability of making exactly 10 free throws\nprob_exactly_10 = binom.pmf(10)\nprint(f\"1. P(X = 10) = {prob_exactly_10:.4f}\")\n\n# 2. Probability of making at least 12 free throws\nprob_at_least_12 = 1 - binom.cdf(11)  # P(X ≥ 12) = 1 - P(X ≤ 11)\n# Alternative: prob_at_least_12 = sum(binom.pmf(k) for k in range(12, 16))\nprint(f\"2. P(X ≥ 12) = {prob_at_least_12:.4f}\")\n\n# 3. Expected number of free throws made\nexpected_makes = binom.mean()\nprint(f\"3. Expected number of makes: {expected_makes}\")\nprint(f\"   (Theoretical: n×p = {n}×{p} = {n*p})\")\n\n# 4. Create bar plot showing PMF\nk_values = range(0, n+1)\nprobabilities = [binom.pmf(k) for k in k_values]\n\nplt.figure(figsize=(12, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Successful Free Throws')\nplt.ylabel('Probability')\nplt.title(f'Binomial Distribution: Basketball Free Throws (n={n}, p={p})')\nplt.axvline(x=expected_makes, color='red', linestyle='--', linewidth=2, label=f'Expected Value = {expected_makes}')\nplt.legend()\nplt.show()\n\nprint(f\"\\nSummary Statistics:\")\nprint(f\"Mean: {binom.mean()}\")\nprint(f\"Variance: {binom.var()}\")\nprint(f\"Standard Deviation: {binom.std():.4f}\")\n\n1. P(X = 10) = 0.2061\n2. P(X ≥ 12) = 0.2969\n3. Expected number of makes: 10.5\n   (Theoretical: n×p = 15×0.7 = 10.5)\n\n\n\n\n\n\n\n\n\n\nSummary Statistics:\nMean: 10.5\nVariance: 3.1500000000000012\nStandard Deviation: 1.7748\n\n\n\n\nTask 4 Solution\nProblem: A call center receives an average of 5 calls per minute.\n\n# Task 4 Solution\n# This is a Poisson distribution with λ = 5\nlam = 5\npoisson = stats.poisson(lam)\n\n# 1. Probability of receiving exactly 7 calls\nprob_exactly_7 = poisson.pmf(7)\nprint(f\"1. P(X = 7) = {prob_exactly_7:.4f}\")\n\n# 2. Probability of receiving no calls\nprob_no_calls = poisson.pmf(0)\nprint(f\"2. P(X = 0) = {prob_no_calls:.4f}\")\n\n# 3. Probability of receiving more than 8 calls\nprob_more_than_8 = 1 - poisson.cdf(8)  # P(X &gt; 8) = 1 - P(X ≤ 8)\nprint(f\"3. P(X &gt; 8) = {prob_more_than_8:.4f}\")\n\n# 4. Plot PMF for k = 0 to 15\nk_values = range(0, 16)\nprobabilities = [poisson.pmf(k) for k in k_values]\n\nplt.figure(figsize=(12, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='purple', edgecolor='black')\nplt.xlabel('Number of Calls per Minute')\nplt.ylabel('Probability')\nplt.title(f'Poisson Distribution: Call Center (λ = {lam})')\nplt.axvline(x=lam, color='red', linestyle='--', linewidth=2, label=f'Expected Value = {lam}')\nplt.legend()\nplt.show()\n\nprint(f\"\\nSummary Statistics:\")\nprint(f\"Expected value (λ): {poisson.mean()}\")\nprint(f\"Variance (λ): {poisson.var()}\")\nprint(f\"Standard Deviation: {poisson.std():.4f}\")\n\n1. P(X = 7) = 0.1044\n2. P(X = 0) = 0.0067\n3. P(X &gt; 8) = 0.0681\n\n\n\n\n\n\n\n\n\n\nSummary Statistics:\nExpected value (λ): 5.0\nVariance (λ): 5.0\nStandard Deviation: 2.2361\n\n\n\n\nTask 5 Solution\nProblem: Distribution Identification Practice\n\n# Task 5 Solutions\n\nprint(\"=== SCENARIO A ===\")\nprint(\"Flip a fair coin 20 times. Probability of exactly 12 heads?\")\nprint(\"Distribution: Binomial(n=20, p=0.5)\")\n\nn_a = 20\np_a = 0.5\nbinom_a = stats.binom(n_a, p_a)\nprob_a = binom_a.pmf(12)\nprint(f\"Answer: P(X = 12) = {prob_a:.4f}\")\n\nprint(\"\\n=== SCENARIO B ===\")\nprint(\"Roll a die until you get a 6. Probability it takes exactly 4 rolls?\")\nprint(\"Distribution: Geometric(p=1/6)\")\n\np_b = 1/6\ngeom_b = stats.geom(p_b)\nprob_b = geom_b.pmf(4)\nprint(f\"Answer: P(X = 4) = {prob_b:.4f}\")\n\nprint(\"\\n=== SCENARIO C ===\")\nprint(\"Website gets average 2 visitors per minute. Probability of exactly 3 visitors?\")\nprint(\"Distribution: Poisson(λ=2)\")\n\nlam_c = 2\npoisson_c = stats.poisson(lam_c)\nprob_c = poisson_c.pmf(3)\nprint(f\"Answer: P(X = 3) = {prob_c:.4f}\")\n\nprint(\"\\n=== SCENARIO D ===\")\nprint(\"5% of items are defective. Probability first defective item found on 8th test?\")\nprint(\"Distribution: Geometric(p=0.05)\")\n\np_d = 0.05\ngeom_d = stats.geom(p_d)\nprob_d = geom_d.pmf(8)\nprint(f\"Answer: P(X = 8) = {prob_d:.4f}\")\n\n=== SCENARIO A ===\nFlip a fair coin 20 times. Probability of exactly 12 heads?\nDistribution: Binomial(n=20, p=0.5)\nAnswer: P(X = 12) = 0.1201\n\n=== SCENARIO B ===\nRoll a die until you get a 6. Probability it takes exactly 4 rolls?\nDistribution: Geometric(p=1/6)\nAnswer: P(X = 4) = 0.0965\n\n=== SCENARIO C ===\nWebsite gets average 2 visitors per minute. Probability of exactly 3 visitors?\nDistribution: Poisson(λ=2)\nAnswer: P(X = 3) = 0.1804\n\n=== SCENARIO D ===\n5% of items are defective. Probability first defective item found on 8th test?\nDistribution: Geometric(p=0.05)\nAnswer: P(X = 8) = 0.0349\n\n\n\n\nTask 6 Solution\nProblem: Simulation of basketball free throw scenario\n\n# Task 6 Solution\nnp.random.seed(42)  # For reproducible results\n\n# Parameters from Task 3\nn_shots = 15\np_success = 0.7\nn_simulations = 1000\n\n# Theoretical probability of exactly 10 makes\ntheoretical_prob = stats.binom(n_shots, p_success).pmf(10)\nprint(f\"Theoretical P(X = 10): {theoretical_prob:.4f}\")\n\n# Simulate the scenario 1000 times\nsimulation_results = []\nexactly_10_count = 0\n\nfor i in range(n_simulations):\n    # Simulate 15 free throws (1 = make, 0 = miss)\n    shots = np.random.binomial(1, p_success, n_shots)\n    makes = np.sum(shots)\n    simulation_results.append(makes)\n    \n    if makes == 10:\n        exactly_10_count += 1\n\n# Calculate proportion of simulations with exactly 10 makes\nsimulated_prob = exactly_10_count / n_simulations\nprint(f\"Simulated P(X = 10): {simulated_prob:.4f}\")\nprint(f\"Difference: {abs(theoretical_prob - simulated_prob):.4f}\")\n\n# Create histogram with theoretical PMF overlay\nplt.figure(figsize=(14, 8))\n\n# Histogram of simulation results\nplt.hist(simulation_results, bins=range(0, n_shots+2), alpha=0.7, density=True, \n         color='lightblue', edgecolor='black', label='Simulation Results')\n\n# Theoretical PMF overlay\nbinom_theory = stats.binom(n_shots, p_success)\nk_values = range(0, n_shots+1)\ntheoretical_probs = [binom_theory.pmf(k) for k in k_values]\nplt.plot(k_values, theoretical_probs, 'ro-', linewidth=2, markersize=8, \n         label='Theoretical PMF')\n\nplt.xlabel('Number of Successful Free Throws')\nplt.ylabel('Probability/Density')\nplt.title(f'Simulation vs Theory: Basketball Free Throws\\n({n_simulations} simulations, n={n_shots}, p={p_success})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\nSimulation Summary:\")\nprint(f\"Mean of simulations: {np.mean(simulation_results):.2f}\")\nprint(f\"Theoretical mean: {binom_theory.mean():.2f}\")\nprint(f\"Standard deviation of simulations: {np.std(simulation_results):.2f}\")\nprint(f\"Theoretical standard deviation: {binom_theory.std():.2f}\")\n\nTheoretical P(X = 10): 0.2061\nSimulated P(X = 10): 0.2150\nDifference: 0.0089\n\n\n\n\n\n\n\n\n\n\nSimulation Summary:\nMean of simulations: 10.59\nTheoretical mean: 10.50\nStandard deviation of simulations: 1.74\nTheoretical standard deviation: 1.77\n\n\n\n\nFinal Challenge Solution\nProblem: Customer service center analysis\n\n# Final Challenge Solution\n\nprint(\"=== CUSTOMER SERVICE CENTER ANALYSIS ===\\n\")\n\n# Given information:\n# - 20% of calls result in a sale (Bernoulli process)\n# - Calls arrive at average rate of 4 per hour (Poisson process)  \n# - Agents work until first sale (Geometric process)\n\np_sale = 0.2  # Probability of sale per call\ncalls_per_hour = 4\nhours_per_day = 8\n\nprint(\"Given:\")\nprint(f\"- Probability of sale per call: {p_sale}\")\nprint(f\"- Average calls per hour: {calls_per_hour}\")\nprint(f\"- Hours of operation per day: {hours_per_day}\")\n\n# 1. Expected number of calls in 8-hour day\nexpected_calls_per_day = calls_per_hour * hours_per_day\nprint(f\"\\n1. Expected calls in {hours_per_day}-hour day: {expected_calls_per_day}\")\n\n# 2. Probability that exactly 2 of next 10 calls result in sales\nn_calls = 10\nbinom_sales = stats.binom(n_calls, p_sale)\nprob_2_sales = binom_sales.pmf(2)\nprint(f\"\\n2. P(exactly 2 sales in 10 calls): {prob_2_sales:.4f}\")\n\n# 3. Expected number of calls until first sale\ngeom_first_sale = stats.geom(p_sale)\nexpected_calls_until_sale = geom_first_sale.mean()\nprint(f\"\\n3. Expected calls until first sale: {expected_calls_until_sale:.1f}\")\n\n# 4. Comprehensive visualization\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n# Plot 1: Poisson - Calls per hour\npoisson_calls = stats.poisson(calls_per_hour)\nk_poisson = range(0, 15)\nprob_poisson = [poisson_calls.pmf(k) for k in k_poisson]\n\naxes[0, 0].bar(k_poisson, prob_poisson, alpha=0.7, color='skyblue', edgecolor='black')\naxes[0, 0].set_title(f'Calls per Hour\\nPoisson(λ={calls_per_hour})')\naxes[0, 0].set_xlabel('Number of Calls')\naxes[0, 0].set_ylabel('Probability')\naxes[0, 0].axvline(x=calls_per_hour, color='red', linestyle='--', label=f'Mean = {calls_per_hour}')\naxes[0, 0].legend()\n\n# Plot 2: Binomial - Sales in 10 calls\nk_binom = range(0, n_calls + 1)\nprob_binom = [binom_sales.pmf(k) for k in k_binom]\n\naxes[0, 1].bar(k_binom, prob_binom, alpha=0.7, color='lightgreen', edgecolor='black')\naxes[0, 1].set_title(f'Sales in {n_calls} Calls\\nBinomial(n={n_calls}, p={p_sale})')\naxes[0, 1].set_xlabel('Number of Sales')\naxes[0, 1].set_ylabel('Probability')\naxes[0, 1].axvline(x=binom_sales.mean(), color='red', linestyle='--', \n                   label=f'Mean = {binom_sales.mean():.1f}')\naxes[0, 1].legend()\n\n# Plot 3: Geometric - Calls until first sale\nk_geom = range(1, 21)\nprob_geom = [geom_first_sale.pmf(k) for k in k_geom]\n\naxes[1, 0].bar(k_geom, prob_geom, alpha=0.7, color='orange', edgecolor='black')\naxes[1, 0].set_title(f'Calls Until First Sale\\nGeometric(p={p_sale})')\naxes[1, 0].set_xlabel('Call Number')\naxes[1, 0].set_ylabel('Probability')\naxes[1, 0].axvline(x=expected_calls_until_sale, color='red', linestyle='--', \n                   label=f'Mean = {expected_calls_until_sale:.1f}')\naxes[1, 0].legend()\n\n# Plot 4: Poisson - Calls per day\npoisson_day = stats.poisson(expected_calls_per_day)\nk_day = range(15, 50)  # Focus on reasonable range around mean\nprob_day = [poisson_day.pmf(k) for k in k_day]\n\naxes[1, 1].bar(k_day, prob_day, alpha=0.7, color='purple', edgecolor='black')\naxes[1, 1].set_title(f'Calls per Day\\nPoisson(λ={expected_calls_per_day})')\naxes[1, 1].set_xlabel('Number of Calls')\naxes[1, 1].set_ylabel('Probability')\naxes[1, 1].axvline(x=expected_calls_per_day, color='red', linestyle='--', \n                   label=f'Mean = {expected_calls_per_day}')\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Additional insights\nprint(f\"\\n=== ADDITIONAL INSIGHTS ===\")\nprint(f\"Daily sales expectations:\")\nexpected_daily_sales = expected_calls_per_day * p_sale\nprint(f\"- Expected calls per day: {expected_calls_per_day}\")\nprint(f\"- Expected sales per day: {expected_daily_sales:.1f}\")\n\nprint(f\"\\nProbability calculations:\")\nprint(f\"- P(no sales in 10 calls): {binom_sales.pmf(0):.4f}\")\nprint(f\"- P(at least 1 sale in 10 calls): {1 - binom_sales.pmf(0):.4f}\")\nprint(f\"- P(first sale on call 1): {geom_first_sale.pmf(1):.4f}\")\nprint(f\"- P(first sale within 5 calls): {geom_first_sale.cdf(5):.4f}\")\n\n=== CUSTOMER SERVICE CENTER ANALYSIS ===\n\nGiven:\n- Probability of sale per call: 0.2\n- Average calls per hour: 4\n- Hours of operation per day: 8\n\n1. Expected calls in 8-hour day: 32\n\n2. P(exactly 2 sales in 10 calls): 0.3020\n\n3. Expected calls until first sale: 5.0\n\n\n\n\n\n\n\n\n\n\n=== ADDITIONAL INSIGHTS ===\nDaily sales expectations:\n- Expected calls per day: 32\n- Expected sales per day: 6.4\n\nProbability calculations:\n- P(no sales in 10 calls): 0.1074\n- P(at least 1 sale in 10 calls): 0.8926\n- P(first sale on call 1): 0.2000\n- P(first sale within 5 calls): 0.6723\n\n\n\n\nSummary\nThis lab covered the fundamental concepts of discrete random variables and probability distributions:\n\nBasic Concepts: PMF, expected value, variance\nKey Distributions: Bernoulli, Binomial, Geometric, Poisson\nPython Tools: scipy.stats for probability calculations\nSimulation: Verifying theoretical results with Monte Carlo methods\nReal Applications: Identifying appropriate distributions for real-world scenarios\n\nKey Takeaways: - Always identify the underlying process to choose the right distribution - Use simulation to verify theoretical calculations - Visualizations help understand distribution shapes and parameters - scipy.stats provides powerful tools for probability work"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#step-1-a-fair-coin",
    "href": "files/labs/lab4/lab4.html#step-1-a-fair-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Step 1: A Fair Coin",
    "text": "Step 1: A Fair Coin\nWhen we flip a fair coin:\n\nHeads happens 50% of the time\nTails happens 50% of the time\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat “fair” means: the coin has no bias toward heads or tails, so each side is exactly as likely.\nIn other words, \\(P(\\text{Heads}) = P(\\text{Tails}) = 0.5\\).\n\n\nLet’s tell Python about this:\n\n# A fair coin - just copy and run this code!\noutcomes = [\"Tails\", \"Heads\"]\nprobabilities = [0.5, 0.5]  # 50% each\n\nprint(\"Possible outcomes:\", outcomes)\nprint(\"Probabilities:\", probabilities)\nprint(\"Total probability:\", sum(probabilities))  # Should be 1.0\n\nLet’s make a plot of this:\n\n# Make a bar chart - just copy and run this code!\nplt.figure(figsize=(8, 5))\nplt.bar(outcomes, probabilities, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('Fair Coin: 50% Heads, 50% Tails')\nplt.ylabel('Probability')\nplt.ylim(0, 0.6)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#step-2-your-turn---a-biased-coin",
    "href": "files/labs/lab4/lab4.html#step-2-your-turn---a-biased-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Step 2: Your Turn - A Biased Coin",
    "text": "Step 2: Your Turn - A Biased Coin\nNow let’s try a biased coin (not fair). This coin lands on Heads 70% of the time!\n\nTask 1: Biased Coin Probability\n\n⏱️ Estimated time: 3 minutes\n\nInstructions: Replace the ___ with the correct numbers.\n\n# A biased coin that lands Heads 70% of the time\nbiased_outcomes = [\"Tails\", \"Heads\"]\nbiased_probabilities = [___, ___]  # Hint: If Heads is 70%, what is Tails?\n\nprint(\"Biased coin outcomes:\", biased_outcomes)\nprint(\"Biased coin probabilities:\", biased_probabilities)\nprint(\"Total probability:\", sum(biased_probabilities))\n\n# Check your answer: this should print 1.0\n\nNow make a graph of your biased coin:\n\n# Copy the graphing code from above and modify the title\nplt.figure(figsize=(8, 5))\nplt.bar(___, ___, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\nplt.title('___')  # Change this title to describe your biased coin\nplt.ylabel('Probability')\nplt.ylim(0, 0.8)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#example-expected-value-of-a-fair-coin",
    "href": "files/labs/lab4/lab4.html#example-expected-value-of-a-fair-coin",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: Expected Value of a Fair Coin",
    "text": "Example: Expected Value of a Fair Coin\nLet’s say Heads = 1 point and Tails = 0 points.\n\n# Expected value calculation for fair coin\n# Formula: Expected Value = (Value1 × Probability1) + (Value2 × Probability2)\n\ncoin_values = [0, 1]  # Tails = 0, Heads = 1\ncoin_probs = [0.5, 0.5]\n\nexpected_value = 0 * 0.5 + 1 * 0.5\nprint(f\"Expected value of fair coin: {expected_value}\")\n\n# This means: if we flip many times, we expect about 0.5 points per flip on average\n\nExpected value of fair coin: 0.5\n\n\n\nTask 2: Expected Value of A Biased Coin\n\n⏱️ Estimated time: 4 minutes\n\nCalculate the expected value for your biased coin (Heads 70%, Tails 30%).\nStep 1: Fill in the calculation\n\n# Expected value for biased coin\n# Values: Tails = 0, Heads = 1\n# Probabilities: Tails = 30%, Heads = 70%\n\nexpected_biased = 0 * ___ + 1 * ___\nprint(f\"Expected value of biased coin: {expected_biased}\")\n\nStep 2: What does this mean?\n\nprint(f\"This means: if we flip the biased coin many times,\")\nprint(f\"we expect about {expected_biased} points per flip on average.\")\nprint(f\"Since this is closer to 1 than 0.5, the coin favors ___\")  \n# Fill in: Heads or Tails?"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#bernoulli-distribution-single-coin-flip",
    "href": "files/labs/lab4/lab4.html#bernoulli-distribution-single-coin-flip",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Bernoulli Distribution (Single Coin Flip)",
    "text": "Bernoulli Distribution (Single Coin Flip)\nA Bernoulli distribution models any single “yes/no” or “success/failure” experiment. It has just one parameter, \\(p\\), which is the probability of success.\n\nWe record a 1 if the outcome is a “success”\nWe record a 0 if the outcome is a “failure”\n\nThe probabilities are \\(P(X=1)=p,\\quad P(X=0)=1-p.\\)\n\n\n\n\n\n\nNote\n\n\n\nCoin‐flip example: If we let “Heads” be a success, then a fair coin flip is\n\\(X \\sim \\mathrm{Bernoulli}(p=0.5)\\), so \\(P(X=1)=0.5\\) (Heads) and \\(P(X=0)=0.5\\) (Tails).\n\n\nFlipping a coin once is therefore exactly a Bernoulli trial.\n\n# Create a Bernoulli distribution for a fair coin\nfair_coin = stats.bernoulli(0.5)  # 0.5 = 50% chance of success (Heads)\n\n# Ask for probabilities\nprob_tails = fair_coin.pmf(0)  # pmf = \"probability mass function\"\nprob_heads = fair_coin.pmf(1)\n\nprint(f\"Probability of Tails (0): {prob_tails}\")\nprint(f\"Probability of Heads (1): {prob_heads}\")\n\n# Get expected value automatically!\nprint(f\"Expected value: {fair_coin.mean()}\")\n\nProbability of Tails (0): 0.4999999999999999\nProbability of Heads (1): 0.5\nExpected value: 0.5\n\n\n\nTask 3: Biased Coin with Scipy\n\n⏱️ Estimated time: 4 minutes\n\nCreate a Bernoulli distribution for your biased coin (70% Heads).\n\n# Create Bernoulli distribution for biased coin\nbiased_coin = stats.bernoulli(___)  # Fill in: what's the probability of Heads?\n\n# Calculate probabilities\nprob_tails_biased = biased_coin.pmf(0)\nprob_heads_biased = biased_coin.pmf(1)\n\nprint(f\"Biased coin - Probability of Tails: {prob_tails_biased}\")\nprint(f\"Biased coin - Probability of Heads: {prob_heads_biased}\")\nprint(f\"Expected value: {biased_coin.mean()}\")\n\nQuestion: Does this match what you calculated by hand in Task 2? ___"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#visualizing-different-coins",
    "href": "files/labs/lab4/lab4.html#visualizing-different-coins",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Visualizing Different Coins",
    "text": "Visualizing Different Coins\nLet’s compare three different coins:\n\n# Compare three coins with different bias\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Three different coins\ncoin_types = [\n    {\"prob\": 0.2, \"name\": \"Mostly Tails\"},\n    {\"prob\": 0.5, \"name\": \"Fair Coin\"}, \n    {\"prob\": 0.8, \"name\": \"Mostly Heads\"}\n]\n\nfor i, coin in enumerate(coin_types):\n    # Create the distribution\n    distribution = stats.bernoulli(coin[\"prob\"])\n    \n    # Get probabilities\n    probs = [distribution.pmf(0), distribution.pmf(1)]\n    \n    # Make bar chart\n    axes[i].bar([0, 1], probs, color=['lightcoral', 'lightblue'], alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{coin[\"name\"]}\\n(p = {coin[\"prob\"]})')\n    axes[i].set_xlabel('Outcome')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_xticks([0, 1])\n    axes[i].set_xticklabels(['Tails', 'Heads'])\n    axes[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#example-5-fair-coin-flips",
    "href": "files/labs/lab4/lab4.html#example-5-fair-coin-flips",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: 5 Fair Coin Flips",
    "text": "Example: 5 Fair Coin Flips\n\n# Binomial distribution: 5 fair coin flips\nn_flips = 5        # number of flips\np_heads = 0.5      # probability of heads each time\n\nfive_flips = stats.binom(n_flips, p_heads)\n\nprint(\"Flipping 5 fair coins...\")\nprint(f\"Probability of 0 heads: {five_flips.pmf(0):.4f}\")\nprint(f\"Probability of 1 head:  {five_flips.pmf(1):.4f}\")\nprint(f\"Probability of 2 heads: {five_flips.pmf(2):.4f}\")\nprint(f\"Probability of 3 heads: {five_flips.pmf(3):.4f}\")\nprint(f\"Probability of 4 heads: {five_flips.pmf(4):.4f}\")\nprint(f\"Probability of 5 heads: {five_flips.pmf(5):.4f}\")\n\nprint(f\"\\nExpected number of heads: {five_flips.mean()}\")\n\nFlipping 5 fair coins...\nProbability of 0 heads: 0.0312\nProbability of 1 head:  0.1562\nProbability of 2 heads: 0.3125\nProbability of 3 heads: 0.3125\nProbability of 4 heads: 0.1562\nProbability of 5 heads: 0.0312\n\nExpected number of heads: 2.5\n\n\nLet’s make a graph:\n\n# Graph showing all possibilities\npossible_heads = [0, 1, 2, 3, 4, 5]\nprobabilities = [five_flips.pmf(k) for k in possible_heads]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_heads, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Heads')\nplt.ylabel('Probability')\nplt.title('5 Fair Coin Flips: How Many Heads?')\nplt.axvline(x=five_flips.mean(), color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {five_flips.mean()}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nTask 4: Basketball Free Throws\n\n⏱️ Estimated time: 8 minutes\n\nA basketball player makes 60% of their free throws. They shoot 10 free throws. How many will they make?\nStep 1: Set up the problem\n\n# Basketball free throws\nn_shots = ___      # How many shots? (Fill in)\np_make = ___       # Probability of making each shot? (Fill in as decimal)\n\nbasketball = stats.binom(___, ___)  # Create the distribution\n\nStep 2: Answer these questions\n\n# a) What's the probability of making exactly 6 shots?\nprob_exactly_6 = basketball.pmf(___)\nprint(f\"Probability of exactly 6 makes: {prob_exactly_6:.4f}\")\n\n# b) How many shots do we expect them to make?\nexpected_makes = basketball.mean()\nprint(f\"Expected number of makes: {expected_makes}\")\n\n# c) What's the probability of making 8 or more shots?\nprob_8_or_more = (basketball.pmf(8) + basketball.pmf(9) + basketball.pmf(10))\nprint(f\"Probability of 8+ makes: {prob_8_or_more:.4f}\")\n\nStep 3: Make a graph (fill in the blanks)\n\n# Create bar chart\npossible_makes = list(range(0, ___))  # 0 to 10 makes\nprobabilities = [basketball.pmf(k) for k in possible_makes]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_makes, probabilities, alpha=0.7, color='___', edgecolor='black')\nplt.xlabel('___')\nplt.ylabel('___')\nplt.title('___')  # Give it a descriptive title\nplt.axvline(x=expected_makes, color='red', linestyle='--', linewidth=2, \n            label=f'Expected = {expected_makes}')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#example-store-customers",
    "href": "files/labs/lab4/lab4.html#example-store-customers",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "Example: Store Customers",
    "text": "Example: Store Customers\n\n# On average, 4 customers enter the store per hour\naverage_customers = 4\n\nstore_customers = stats.poisson(average_customers)\n\nprint(\"Store customer arrivals per hour:\")\nprint(f\"Probability of 0 customers: {store_customers.pmf(0):.4f}\")\nprint(f\"Probability of 2 customers: {store_customers.pmf(2):.4f}\")\nprint(f\"Probability of 4 customers: {store_customers.pmf(4):.4f}\")\nprint(f\"Probability of 6 customers: {store_customers.pmf(6):.4f}\")\n\nprint(f\"\\nExpected customers per hour: {store_customers.mean()}\")\n\nStore customer arrivals per hour:\nProbability of 0 customers: 0.0183\nProbability of 2 customers: 0.1465\nProbability of 4 customers: 0.1954\nProbability of 6 customers: 0.1042\n\nExpected customers per hour: 4.0\n\n\nLet’s visualize this:\n\n# Graph customer arrivals\npossible_customers = list(range(0, 12))  # 0 to 11 customers\nprobabilities = [store_customers.pmf(k) for k in possible_customers]\n\nplt.figure(figsize=(10, 6))\nplt.bar(possible_customers, probabilities, alpha=0.7, color='purple', edgecolor='black')\nplt.xlabel('Number of Customers per Hour')\nplt.ylabel('Probability')\nplt.title('Store Customer Arrivals (Average = 4 per hour)')\nplt.axvline(x=average_customers, color='red', linestyle='--', linewidth=2, \n            label=f'Average = {average_customers}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nTask 5: Your Email Inbox\n\n⏱️ Estimated time: 6 minutes\n\nYour email inbox receives an average of 3 emails per hour.\nStep 1: Set up the Poisson distribution\n\n# Email arrivals\naverage_emails = ___  # Fill in the average\nemail_arrivals = stats.poisson(___)\n\nStep 2: Answer these questions\n\n# a) Probability of getting exactly 3 emails in an hour\nprob_exactly_3 = email_arrivals.pmf(___)\nprint(f\"Probability of exactly 3 emails: {prob_exactly_3:.4f}\")\n\n# b) Probability of getting no emails (quiet hour!)\nprob_no_emails = email_arrivals.pmf(___)\nprint(f\"Probability of no emails: {prob_no_emails:.4f}\")\n\n# c) Expected number of emails per hour\nexpected_emails = email_arrivals.___()  # Fill in the method\nprint(f\"Expected emails per hour: {expected_emails}\")\n\nStep 3: Make a graph (copy and modify the code from the store example)\n\n# Your graph code here (copy from above and modify)"
  },
  {
    "objectID": "files/labs/lab4/lab4.html#whats-next",
    "href": "files/labs/lab4/lab4.html#whats-next",
    "title": "Lab 4: Introduction to Probability with Python",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn future labs, we’ll learn about:\n\nContinuous probability distributions (like heights, weights)\nHypothesis testing (is a coin really fair?)\nConfidence intervals (estimating with uncertainty)\n\nGreat job completing your first probability lab! 🎯"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html",
    "href": "files/labs/lab4/lab4_v2.html",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "",
    "text": "Welcome to Lab 4! Today we’ll explore discrete random variables and probability distributions using Python. We’ll learn how to calculate probabilities, expected values, and visualize different distributions."
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#probability-mass-function-pmf",
    "href": "files/labs/lab4/lab4_v2.html#probability-mass-function-pmf",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Probability Mass Function (PMF)",
    "text": "Probability Mass Function (PMF)\nFor a discrete random variable X, the Probability Mass Function P(X = k) gives the probability that X takes the value k.\nKey properties of a PMF: - P(X = k) ≥ 0 for all k - Σ P(X = k) = 1 (sum over all possible values)\nLet’s start with a simple example:\n\n# Simple discrete random variable: rolling a fair die\n# X can take values 1, 2, 3, 4, 5, 6 each with probability 1/6\n\ndie_values = [1, 2, 3, 4, 5, 6]\ndie_probabilities = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]\n\nprint(\"Die Values:\", die_values)\nprint(\"Probabilities:\", die_probabilities)\nprint(\"Sum of probabilities:\", sum(die_probabilities))\n\nDie Values: [1, 2, 3, 4, 5, 6]\nProbabilities: [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666]\nSum of probabilities: 1.0\n\n\nLet’s visualize this distribution:\n\nplt.figure(figsize=(8, 5))\nplt.bar(die_values, die_probabilities, alpha=0.7, color='skyblue', edgecolor='black')\nplt.xlabel('Value')\nplt.ylabel('Probability')\nplt.title('Probability Mass Function - Fair Die')\nplt.ylim(0, 0.25)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-1",
    "href": "files/labs/lab4/lab4_v2.html#task-1",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 1",
    "text": "Task 1\n\n⏱️ Estimated time: 5 minutes\n\nConsider a biased coin where P(Heads) = 0.7 and P(Tails) = 0.3. Let X be a random variable where X = 1 for Heads and X = 0 for Tails.\n\nCreate lists for the values and probabilities of X\nVerify that the probabilities sum to 1\nCreate a bar plot showing the PMF"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#expected-value-mean",
    "href": "files/labs/lab4/lab4_v2.html#expected-value-mean",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Expected Value (Mean)",
    "text": "Expected Value (Mean)\nThe expected value of a discrete random variable X is:\n\n\\[E[X] = \\mu = \\sum_{k} k \\cdot P(X = k)\\]"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#variance",
    "href": "files/labs/lab4/lab4_v2.html#variance",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Variance",
    "text": "Variance\nThe variance of X is:\n\n\\[\\text{Var}(X) = \\sigma^2 = E[X^2] - (E[X])^2 = \\sum_{k} k^2 \\cdot P(X = k) - \\mu^2\\]\n\nLet’s calculate these for our fair die example:\n\n# Expected value of a fair die\nexpected_value = sum(k * p for k, p in zip(die_values, die_probabilities))\nprint(f\"Expected value of fair die: {expected_value}\")\n\n# Variance calculation\n# First calculate E[X^2]\nexpected_x_squared = sum(k**2 * p for k, p in zip(die_values, die_probabilities))\nvariance = expected_x_squared - expected_value**2\n\nprint(f\"E[X^2]: {expected_x_squared}\")\nprint(f\"Variance: {variance}\")\nprint(f\"Standard deviation: {np.sqrt(variance)}\")\n\nExpected value of fair die: 3.5\nE[X^2]: 15.166666666666666\nVariance: 2.916666666666666\nStandard deviation: 1.707825127659933"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-2",
    "href": "files/labs/lab4/lab4_v2.html#task-2",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 2",
    "text": "Task 2\n\n⏱️ Estimated time: 4 minutes\n\nCalculate the expected value and variance for the biased coin from Task 1 (where X = 1 for Heads with probability 0.7, and X = 0 for Tails with probability 0.3).\nShow your calculations step by step."
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#bernoulli-distribution",
    "href": "files/labs/lab4/lab4_v2.html#bernoulli-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Bernoulli Distribution",
    "text": "Bernoulli Distribution\n\n⏱️ Estimated time: 6 minutes\n\nA Bernoulli distribution models a single trial with two outcomes: success (1) or failure (0).\n\nParameter: p (probability of success)\nPMF: P(X = 1) = p, P(X = 0) = 1-p\nE[X] = p\nVar(X) = p(1-p)\n\n\n# Using scipy.stats for Bernoulli distribution\np = 0.3  # probability of success\n\n# Create Bernoulli distribution object\nbern = stats.bernoulli(p)\n\n# Calculate probabilities\nprint(f\"P(X = 0) = {bern.pmf(0)}\")\nprint(f\"P(X = 1) = {bern.pmf(1)}\")\n\n# Expected value and variance\nprint(f\"Expected value: {bern.mean()}\")\nprint(f\"Variance: {bern.var()}\")\n\nP(X = 0) = 0.6999999999999997\nP(X = 1) = 0.3\nExpected value: 0.3\nVariance: 0.21\n\n\nLet’s visualize several Bernoulli distributions:\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\np_values = [0.2, 0.5, 0.8]\n\nfor i, p in enumerate(p_values):\n    bern = stats.bernoulli(p)\n    x_vals = [0, 1]\n    y_vals = [bern.pmf(x) for x in x_vals]\n    \n    axes[i].bar(x_vals, y_vals, alpha=0.7, color='lightcoral', edgecolor='black')\n    axes[i].set_xlabel('Value')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Bernoulli(p={p})')\n    axes[i].set_ylim(0, 1)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#binomial-distribution",
    "href": "files/labs/lab4/lab4_v2.html#binomial-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n⏱️ Estimated time: 8 minutes\n\nA Binomial distribution models the number of successes in n independent Bernoulli trials.\n\nParameters: n (number of trials), p (probability of success)\nPMF: P(X = k) = C(n,k) × p^k × (1-p)^(n-k)\nE[X] = np\nVar(X) = np(1-p)\n\n\n# Binomial distribution example: 10 coin flips with p = 0.5\nn = 10\np = 0.5\n\nbinom = stats.binom(n, p)\n\n# Calculate probabilities for different numbers of successes\nk_values = range(0, n+1)\nprobabilities = [binom.pmf(k) for k in k_values]\n\n# Display some key probabilities\nprint(f\"P(X = 5) = {binom.pmf(5):.4f}\")\nprint(f\"P(X ≤ 3) = {binom.cdf(3):.4f}\")\nprint(f\"P(X ≥ 7) = {1 - binom.cdf(6):.4f}\")\n\nprint(f\"\\nExpected value: {binom.mean()}\")\nprint(f\"Variance: {binom.var()}\")\nprint(f\"Standard deviation: {binom.std()}\")\n\nP(X = 5) = 0.2461\nP(X ≤ 3) = 0.1719\nP(X ≥ 7) = 0.1719\n\nExpected value: 5.0\nVariance: 2.5\nStandard deviation: 1.5811388300841898\n\n\nLet’s visualize the binomial distribution:\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Number of Successes (k)')\nplt.ylabel('Probability')\nplt.title(f'Binomial Distribution (n={n}, p={p})')\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-3",
    "href": "files/labs/lab4/lab4_v2.html#task-3",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 3",
    "text": "Task 3\n\n⏱️ Estimated time: 6 minutes\n\nA basketball player makes 70% of their free throws. They take 15 free throws.\n\nWhat is the probability they make exactly 10 free throws?\nWhat is the probability they make at least 12 free throws?\nWhat is the expected number of free throws made?\nCreate a bar plot showing the PMF for this scenario"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#geometric-distribution",
    "href": "files/labs/lab4/lab4_v2.html#geometric-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\n⏱️ Estimated time: 6 minutes\n\nA Geometric distribution models the number of trials needed to get the first success.\n\nParameter: p (probability of success)\nPMF: P(X = k) = (1-p)^(k-1) × p\nE[X] = 1/p\nVar(X) = (1-p)/p²\n\n\n# Geometric distribution: rolling a die until we get a 6\np = 1/6  # probability of rolling a 6\n\ngeom = stats.geom(p)\n\n# Calculate probabilities for first few trials\nk_values = range(1, 21)  # trials 1 to 20\nprobabilities = [geom.pmf(k) for k in k_values]\n\nprint(f\"P(X = 1) = {geom.pmf(1):.4f}\")  # Success on first trial\nprint(f\"P(X = 6) = {geom.pmf(6):.4f}\")  # Success on sixth trial\nprint(f\"P(X ≤ 10) = {geom.cdf(10):.4f}\")  # Success within 10 trials\n\nprint(f\"\\nExpected value: {geom.mean():.2f}\")\nprint(f\"Variance: {geom.var():.2f}\")\n\nP(X = 1) = 0.1667\nP(X = 6) = 0.0670\nP(X ≤ 10) = 0.8385\n\nExpected value: 6.00\nVariance: 30.00\n\n\nVisualizing the geometric distribution:\n\nplt.figure(figsize=(10, 6))\nplt.bar(k_values, probabilities, alpha=0.7, color='orange', edgecolor='black')\nplt.xlabel('Trial Number (k)')\nplt.ylabel('Probability')\nplt.title(f'Geometric Distribution (p={p:.3f})')\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#poisson-distribution",
    "href": "files/labs/lab4/lab4_v2.html#poisson-distribution",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n⏱️ Estimated time: 8 minutes\n\nA Poisson distribution models the number of events occurring in a fixed interval when events occur independently at a constant average rate.\n\nParameter: λ (lambda, average rate)\nPMF: P(X = k) = (λ^k × e^(-λ)) / k!\nE[X] = λ\nVar(X) = λ\n\n\n# Poisson distribution: number of customers arriving per hour\nlam = 3.5  # average 3.5 customers per hour\n\npoisson = stats.poisson(lam)\n\n# Calculate probabilities\nk_values = range(0, 15)\nprobabilities = [poisson.pmf(k) for k in k_values]\n\nprint(f\"P(X = 0) = {poisson.pmf(0):.4f}\")  # No customers\nprint(f\"P(X = 3) = {poisson.pmf(3):.4f}\")  # Exactly 3 customers\nprint(f\"P(X ≤ 5) = {poisson.cdf(5):.4f}\")  # At most 5 customers\nprint(f\"P(X ≥ 6) = {1 - poisson.cdf(5):.4f}\")  # At least 6 customers\n\nprint(f\"\\nExpected value: {poisson.mean()}\")\nprint(f\"Variance: {poisson.var()}\")\n\nP(X = 0) = 0.0302\nP(X = 3) = 0.2158\nP(X ≤ 5) = 0.8576\nP(X ≥ 6) = 0.1424\n\nExpected value: 3.5\nVariance: 3.5\n\n\nVisualizing different Poisson distributions:\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nlambda_values = [1, 3.5, 8]\n\nfor i, lam in enumerate(lambda_values):\n    poisson = stats.poisson(lam)\n    k_vals = range(0, int(lam + 4*np.sqrt(lam)))\n    probs = [poisson.pmf(k) for k in k_vals]\n    \n    axes[i].bar(k_vals, probs, alpha=0.7, color='purple', edgecolor='black')\n    axes[i].set_xlabel('Number of Events (k)')\n    axes[i].set_ylabel('Probability')\n    axes[i].set_title(f'Poisson(λ={lam})')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-4",
    "href": "files/labs/lab4/lab4_v2.html#task-4",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 4",
    "text": "Task 4\n\n⏱️ Estimated time: 7 minutes\n\nA call center receives an average of 5 calls per minute.\n\nWhat is the probability of receiving exactly 7 calls in a minute?\nWhat is the probability of receiving no calls in a minute?\nWhat is the probability of receiving more than 8 calls in a minute?\nPlot the PMF for k = 0 to 15 calls"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-5",
    "href": "files/labs/lab4/lab4_v2.html#task-5",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 5",
    "text": "Task 5\n\n⏱️ Estimated time: 8 minutes\n\nDistribution Identification Practice\nFor each scenario below, identify the appropriate distribution and calculate the requested probability:\n\nScenario A: You flip a fair coin 20 times. What’s the probability of getting exactly 12 heads?\nScenario B: You keep rolling a die until you get a 6. What’s the probability it takes exactly 4 rolls?\nScenario C: A website gets an average of 2 visitors per minute. What’s the probability of getting exactly 3 visitors in a given minute?\nScenario D: A quality control inspector tests items where 5% are defective. What’s the probability the first defective item is found on the 8th test?"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#task-6",
    "href": "files/labs/lab4/lab4_v2.html#task-6",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Task 6",
    "text": "Task 6\n\n⏱️ Estimated time: 8 minutes\n\nSimulation Project:\nSimulate the basketball free throw scenario from Task 3 (15 shots, 70% success rate):\n\nSimulate this scenario 1000 times\nCalculate the proportion of simulations where the player made exactly 10 shots\nCompare this to the theoretical probability you calculated earlier\nCreate a histogram of the simulation results and overlay the theoretical PMF"
  },
  {
    "objectID": "files/labs/lab4/lab4_v2.html#final-challenge",
    "href": "files/labs/lab4/lab4_v2.html#final-challenge",
    "title": "Lab 4: Discrete Random Variables and Distributions",
    "section": "Final Challenge",
    "text": "Final Challenge\n\n⏱️ Estimated time: 10 minutes\n\nReal-World Application:\nA customer service center has the following characteristics: - 20% of calls result in a sale (Bernoulli process) - Calls arrive at an average rate of 4 per hour (Poisson process) - Agents keep working until they make their first sale of the day (Geometric process)\nCalculate: 1. In a day with 8 hours of operation, what’s the expected number of calls? 2. What’s the probability that exactly 2 of the next 10 calls result in sales? 3. What’s the expected number of calls an agent needs to handle to make their first sale? 4. Create a comprehensive visualization showing all three distributions"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#welcome-to-lecture-9",
    "href": "files/lecture_notes/lecture8/lecture8.html#welcome-to-lecture-9",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Welcome to Lecture 9",
    "text": "Welcome to Lecture 9\nContinuous Random Variables\nFrom discrete jumps to smooth curves: modeling the continuous world"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#review-discrete-vs-continuous",
    "href": "files/lecture_notes/lecture8/lecture8.html#review-discrete-vs-continuous",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Review: Discrete vs Continuous",
    "text": "Review: Discrete vs Continuous\n\n\n\n\nDiscrete Random Variables\n\n\n\nCountable values (can list them)\n\n\nGaps between possible values\n\n\nUses Probability Mass Function (PMF)\n\n\n\\(P(X = x)\\) makes sense\n\n\n\nExamples: Dice rolls, number of emails, quiz scores\n\n\n\n\n\n\n\n\n\nContinuous Random Variables\n\n\n\nUncountable values (infinite possibilities)\n\n\nNo gaps - any value in an interval\n\n\nUses Probability Density Function (PDF)\n\n\n\\(P(X = x) = 0\\) for any specific value!\n\n\n\nExamples: Height, weight, time, temperature"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#why-px-x-0-for-continuous-variables",
    "href": "files/lecture_notes/lecture8/lecture8.html#why-px-x-0-for-continuous-variables",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Why P(X = x) = 0 for Continuous Variables?",
    "text": "Why P(X = x) = 0 for Continuous Variables?\n\nFor continuous random variables, the probability of any exact value is zero!\nThink about it: What’s the probability someone is exactly 5.7324681… feet tall?\n\n\n\nInstead, we ask:\n\nP(5.7 ≤ X ≤ 5.8)?\nP(X ≤ 6.0)?\nP(X &gt; 5.5)?\n\nKey insight: We calculate probabilities for intervals, not exact points.\n\n\n\n\n\nClick to see why P(X = exact value) = 0"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#probability-density-function-pdf",
    "href": "files/lecture_notes/lecture8/lecture8.html#probability-density-function-pdf",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\n\n\n\n\n 🎯 Definition: The Probability Density Function (PDF) of a continuous random variable \\(X\\) is a function \\(f(x)\\) such that:\n\n\n\\[P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\\]\n\n\n\n\n\nProperties of PDF:\n\n\n\n\\(f(x) \\geq 0\\) for all \\(x\\)\n\n\n\\(\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\)\n\n\n\\(f(x)\\) is NOT a probability - it’s a density!\n\n\n\nKey Insight:\n\n\nThe area under the PDF curve between \\(a\\) and \\(b\\) gives the probability that \\(X\\) falls in that interval."
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#pdf-interpretation-height-is-density",
    "href": "files/lecture_notes/lecture8/lecture8.html#pdf-interpretation-height-is-density",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "PDF Interpretation: Height is Density",
    "text": "PDF Interpretation: Height is Density\n\nInteractive PDF Demo: Understanding Density\n\n\nNew Random Variable\n\n Uniform Distribution Normal Distribution Exponential Distribution  Interval: [ , ]\n\n\n\nProbability = Area under curve: Select an interval to see probability"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#cumulative-distribution-function-cdf",
    "href": "files/lecture_notes/lecture8/lecture8.html#cumulative-distribution-function-cdf",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\n\n\n\nFor continuous random variables, the CDF is:\n\\[F(x) = P(X \\leq x) = \\int_{-\\infty}^x f(t) \\, dt\\]\nKey relationship: \\[f(x) = \\frac{d}{dx}F(x)\\]\nThe PDF is the derivative of the CDF!\n\n\n\n\n\nClick anywhere to see F(x) = P(X ≤ x) for that point"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#expected-value-and-variance",
    "href": "files/lecture_notes/lecture8/lecture8.html#expected-value-and-variance",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Expected Value and Variance",
    "text": "Expected Value and Variance\n\n\nExpected Value: \\[E[X] = \\mu = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\\]\nVariance:\n\\[\\text{Var}(X) = \\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx = E[X^2] - (E[X])^2\\]\nWhere:\n\\[E[X^2] = \\int_{-\\infty}^{\\infty} x^2 \\cdot f(x) \\, dx\\]\n\n\n\n\n\n\n\nImportant\n\n\nNotice: Integrals replace sums when moving from discrete to continuous!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#common-continuous-distributions",
    "href": "files/lecture_notes/lecture8/lecture8.html#common-continuous-distributions",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\n\nUniform Distribution\nAll values equally likely in an interval\nParameters: \\(a\\) (min), \\(b\\) (max)\nPDF: \\(f(x) = \\frac{1}{b-a}\\) for \\(a \\leq x \\leq b\\)\nMean: \\(\\frac{a+b}{2}\\)\nVariance: \\(\\frac{(b-a)^2}{12}\\)\nUse: Random numbers, waiting times\n\n\nNormal Distribution\nBell-shaped, symmetric\nParameters: \\(\\mu\\) (mean), \\(\\sigma^2\\) (variance)\nPDF: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\nMean: \\(\\mu\\)\nVariance: \\(\\sigma^2\\)\nUse: Heights, test scores, errors\n\n\nExponential Distribution\nModels waiting times\nParameters: \\(\\lambda\\) (rate)\nPDF: \\(f(x) = \\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\)\nMean: \\(\\frac{1}{\\lambda}\\)\nVariance: \\(\\frac{1}{\\lambda^2}\\)\nUse: Time between events, lifetimes"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#interactive-distribution-explorer",
    "href": "files/lecture_notes/lecture8/lecture8.html#interactive-distribution-explorer",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Interactive Distribution Explorer",
    "text": "Interactive Distribution Explorer\n\nContinuous Distribution Visualizer\n\n Uniform Normal Exponential  Parameter 1:  Parameter 2:  a:  b:  Calculate P(a ≤ X ≤ b)\n\n\n\nDistribution Properties: Select parameters to see statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#the-normal-distribution-star-of-statistics",
    "href": "files/lecture_notes/lecture8/lecture8.html#the-normal-distribution-star-of-statistics",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "The Normal Distribution: Star of Statistics",
    "text": "The Normal Distribution: Star of Statistics\n\n\n\nWhy Normal is Special\n\nCentral Limit Theorem: Sample means approach normal\n68-95-99.7 Rule:\n\n68% within 1σ of μ\n95% within 2σ of μ\n\n99.7% within 3σ of μ\n\nStandard Normal: μ = 0, σ = 1\n\nZ-Score Transformation\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-1-uniform-distribution",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-1-uniform-distribution",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Practice Problem 1: Uniform Distribution",
    "text": "Practice Problem 1: Uniform Distribution\n\nA bus arrives uniformly between 10:00 AM and 10:20 AM. Let \\(X\\) = arrival time in minutes after 10:00 AM.\n(a) What is the PDF of \\(X\\)?\n(b) What’s the probability the bus arrives between 10:05 and 10:12?\n(c) What’s the expected arrival time?\n\nShow Solution\n\n\nSolution. (a) \\(X \\sim \\text{Uniform}(0, 20)\\) \\[f(x) = \\frac{1}{20-0} = \\frac{1}{20} \\text{ for } 0 \\leq x \\leq 20\\]\n(b) \\(P(5 \\leq X \\leq 12) = \\int_5^{12} \\frac{1}{20} dx = \\frac{1}{20} \\times (12-5) = \\frac{7}{20} = 0.35\\)\n(c) \\(E[X] = \\frac{a+b}{2} = \\frac{0+20}{2} = 10\\) minutes after 10:00 AM"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-2-normal-distribution",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-2-normal-distribution",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Practice Problem 2: Normal Distribution",
    "text": "Practice Problem 2: Normal Distribution\n\nHeights of adult women are normally distributed with μ = 64 inches and σ = 2.5 inches.\n(a) What’s the probability a woman is taller than 67 inches?\n(b) What height represents the 90th percentile?\n(c) What’s the probability a woman is between 62 and 66 inches tall?\n\nShow Solution\n\n\nSolution. (a) \\(P(X &gt; 67) = P\\left(Z &gt; \\frac{67-64}{2.5}\\right) = P(Z &gt; 1.2) = 1 - 0.8849 = 0.1151\\)\n(b) For 90th percentile: \\(P(X \\leq x) = 0.90\\)\n\\(z_{0.90} = 1.28\\), so \\(x = 64 + 1.28(2.5) = 67.2\\) inches\n(c) \\(P(62 \\leq X \\leq 66) = P\\left(\\frac{62-64}{2.5} \\leq Z \\leq \\frac{66-64}{2.5}\\right)\\)\n\\(= P(-0.8 \\leq Z \\leq 0.8) = 0.7881 - 0.2119 = 0.5762\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3-exponential-distribution",
    "href": "files/lecture_notes/lecture8/lecture8.html#practice-problem-3-exponential-distribution",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Practice Problem 3: Exponential Distribution",
    "text": "Practice Problem 3: Exponential Distribution\n\nThe time between customer arrivals at a store follows an exponential distribution with an average of 5 minutes between arrivals.\n(a) What is the PDF?\n(b) What’s the probability the next customer arrives within 3 minutes?\n(c) What’s the probability no customer arrives in the next 10 minutes?\n\nShow Solution\n\n\nSolution. (a) Average = 5 minutes = \\(\\frac{1}{\\lambda}\\), so \\(\\lambda = 0.2\\)\n\\[f(x) = 0.2e^{-0.2x} \\text{ for } x \\geq 0\\]\n(b) \\(P(X \\leq 3) = \\int_0^3 0.2e^{-0.2x} dx = 1 - e^{-0.2 \\times 3} = 1 - e^{-0.6} = 0.4512\\)\n(c) \\(P(X &gt; 10) = e^{-0.2 \\times 10} = e^{-2} = 0.1353\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#central-limit-theorem",
    "href": "files/lecture_notes/lecture8/lecture8.html#central-limit-theorem",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nInteractive CLT Demo: Sample Means Approach Normal\n\n Uniform Population Exponential Population Bimodal Population  Sample Size:  Run Simulation\n\n\n\n\n\n\nPopulation Distribution\n\n\n\n\n\n\nSample Means Distribution\n\n\n\n\nCLT in Action: Run simulation to see the magic!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#transformations-of-random-variables",
    "href": "files/lecture_notes/lecture8/lecture8.html#transformations-of-random-variables",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Transformations of Random Variables",
    "text": "Transformations of Random Variables\n\n\nLinear Transformations\nIf \\(Y = aX + b\\), then:\n\n\\(E[Y] = aE[X] + b\\)\n\\(\\text{Var}(Y) = a^2\\text{Var}(X)\\)\nIf \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Y \\sim N(a\\mu + b, a^2\\sigma^2)\\)\n\n\nStandardization\n\\[Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\]\n\nImportant: Normal distributions are closed under linear transformations!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#comparing-discrete-and-continuous",
    "href": "files/lecture_notes/lecture8/lecture8.html#comparing-discrete-and-continuous",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Comparing Discrete and Continuous",
    "text": "Comparing Discrete and Continuous\n\n\n\n\n\n\n\n\n\nProperty\nDiscrete\nContinuous\n\n\n\n\nProbability Function\nPMF: \\(P(X = x)\\)\nPDF: \\(f(x)\\)\n\n\nExact Value Probability\n\\(P(X = x) &gt; 0\\) possible\n\\(P(X = x) = 0\\) always\n\n\nInterval Probability\n\\(\\sum P(X = x_i)\\)\n\\(\\int_a^b f(x) dx\\)\n\n\nExpected Value\n\\(\\sum x \\cdot P(X = x)\\)\n\\(\\int x \\cdot f(x) dx\\)\n\n\nVariance\n\\(\\sum (x-\\mu)^2 P(X = x)\\)\n\\(\\int (x-\\mu)^2 f(x) dx\\)\n\n\nCDF\n\\(\\sum_{x_i \\leq x} P(X = x_i)\\)\n\\(\\int_{-\\infty}^x f(t) dt\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#properties-of-continuous-distributions",
    "href": "files/lecture_notes/lecture8/lecture8.html#properties-of-continuous-distributions",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Properties of Continuous Distributions",
    "text": "Properties of Continuous Distributions\n\n\n\nKey Properties\n\nMemoryless Property (Exponential only):\n\\(P(X &gt; s+t | X &gt; s) = P(X &gt; t)\\)\nSymmetry (Normal):\n\\(P(X \\leq \\mu - a) = P(X \\geq \\mu + a)\\)\nScaling Invariance (Normal):\nLinear combinations of normals are normal\n\n\nUseful Relationships\n\nCDF to PDF: \\(f(x) = F'(x)\\)\nPDF to CDF: \\(F(x) = \\int_{-\\infty}^x f(t) dt\\)\nComplementary CDF: \\(P(X &gt; x) = 1 - F(x)\\)\n\n\nRemember: Area under PDF = 1, but PDF values can exceed 1!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#key-takeaways",
    "href": "files/lecture_notes/lecture8/lecture8.html#key-takeaways",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nMain Concepts\n\nContinuous variables require PDFs, not PMFs\nProbabilities are areas under curves, not function values\nIntegration replaces summation for continuous distributions\nNormal distribution is central due to CLT\n\n\nDistribution Selection\nChoose distributions based on the data characteristics:\n\nUniform for equally likely intervals\nNormal for symmetric, bell-shaped data\n\nExponential for waiting times/lifetimes\nUse CLT when working with sample means\n\nKey Principle\n\nCentral Limit Theorem makes normal distributions ubiquitous in statistics"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#resources",
    "href": "files/lecture_notes/lecture8/lecture8.html#resources",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Resources",
    "text": "Resources\n\n\n\n Read OpenIntro Statistics Chapter 4 sections 4.1-4.3\n\n\n Khan Academy - Continuous Random Variables\n\n\n Seeing Theory - Probability Distributions\n\n\n Central Limit Theorem - Wikipedia\n\n\n Introduction to Probability - Continuous Random Variables"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln_draft.html",
    "href": "files/worksheets/worksheet4_sln_draft.html",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "",
    "text": "Section A: Probability - SOLUTIONS\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions - SOLUTION\nFor a valid probability distribution, two conditions must be met:\n\nAll probabilities must be non-negative (≥ 0)\nThe sum of all probabilities must equal 1\n\nAnalysis:\n(a) Invalid\n\nSum = 0.3 + 0.3 + 0.3 + 0.2 + 0.1 = 1.2 &gt; 1 The probabilities sum to more than 1, violating the second condition.\n\n(b) Valid\n\nSum = 0 + 0 + 1 + 0 + 0 = 1 All probabilities are non-negative and sum to 1. This represents a class where everyone receives a C.\n\n(c) Invalid\n\nSum = 0.3 + 0.3 + 0.3 + 0 + 0 = 0.9 &lt; 1 The probabilities sum to less than 1, violating the second condition.\n\n(d) Invalid\n\nContains F = -0.1 &lt; 0 Although the sum would equal 1.0, the probability for grade F is negative, violating the first condition.\n\n(e) Valid\n\nSum = 0.2 + 0.4 + 0.2 + 0.1 + 0.1 = 1.0 All probabilities are non-negative and sum to 1.\n\n(f) Invalid\n\nContains B = -0.1 &lt; 0 Although the sum equals 1.0, the probability for grade B is negative, violating the first condition.\n\n\n\n\nSection B: Permutations and Combinations - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations - SOLUTION\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\nSolution: Since letters must come before digits, we have a fixed structure: LLL DDD\n\nStep 1: Arrange 3 letters in the first 3 positions\n\nThis is a permutation: P(3,3) = 3! = 6 ways\n\nStep 2: Arrange 3 digits in the last 3 positions\n\nThis is a permutation: P(3,3) = 3! = 6 ways\n\nStep 3: Apply multiplication principle\n\nTotal passwords = 6 × 6 = 36 passwords\n\n\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\nSolution: Since order doesn’t matter, this is a combination problem.\n\\[C(12,4) = \\binom{12}{4} = \\frac{12!}{4!(12-4)!} = \\frac{12!}{4! \\cdot 8!}\\]\n\\[= \\frac{12 \\times 11 \\times 10 \\times 9}{4 \\times 3 \\times 2 \\times 1} = \\frac{11880}{24} = \\textbf{495 ways}\\]\n\n\n\nSection C: Conditional Probability - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Conditional Probability and Medical Testing - SOLUTION\nGiven Information:\n\nP(has variant) = 0.03\nP(test positive | has variant) = 0.95 (sensitivity)\nP(test negative | no variant) = 0.92 (specificity)\nTherefore: P(test positive | no variant) = 1 - 0.92 = 0.08\n\nPart (a): What is the probability that a randomly selected person tests positive?\nSolution:\nUsing the Law of Total Probability:\n\\[P(\\text{test positive}) = P(\\text{test positive | has variant}) \\times P(\\text{has variant}) + P(\\text{test positive | no variant}) \\times P(\\text{no variant})\\]\n\\[P(\\text{test positive}) = 0.95 \\times 0.03 + 0.08 \\times 0.97\\] \\[= 0.0285 + 0.0776 = \\textbf{0.1061}\\]\nPart (b): If someone tests positive, what is the probability they actually have the variant?\nSolution: Using Bayes’ Theorem:\n\\[P(\\text{has variant | test positive}) = \\frac{P(\\text{test positive | has variant}) \\times P(\\text{has variant})}{P(\\text{test positive})}\\]\n\\[= \\frac{0.95 \\times 0.03}{0.1061} = \\frac{0.0285}{0.1061} = \\textbf{0.2686}\\]\nPart (c): If someone tests negative, what is the probability they actually don’t have the variant?\nSolution: First, find P(test negative): \\[P(\\text{test negative}) = 1 - P(\\text{test positive}) = 1 - 0.1061 = 0.8939\\]\nUsing Bayes’ Theorem: \\[P(\\text{no variant | test negative}) = \\frac{P(\\text{test negative | no variant}) \\times P(\\text{no variant})}{P(\\text{test negative})}\\]\n\\[= \\frac{0.92 \\times 0.97}{0.8939} = \\frac{0.8924}{0.8939} = \\textbf{0.9983}\\]\nPart (d) [Challenge]: Two consecutive positive tests - what is the probability they actually have the variant?\nSolution: Assuming independence between tests:\n\\[P(\\text{two positive | has variant}) = 0.95^2 = 0.9025\\] \\[P(\\text{two positive | no variant}) = 0.08^2 = 0.0064\\]\n\\[P(\\text{two positive}) = 0.9025 \\times 0.03 + 0.0064 \\times 0.97 = 0.027075 + 0.006208 = 0.033283\\]\n\\[P(\\text{has variant | two positive}) = \\frac{0.027075}{0.033283} = \\textbf{0.8134}\\]\n\n\nProblem C1: Advanced Counting with Restrictions - SOLUTION\nPart (a): How many valid meal combinations are possible?\nSolution: We need to consider cases based on the restrictions.\nCase 1: Seafood appetizer is chosen\n\n1 appetizer option (seafood)\n7 main course options (cannot choose vegetarian)\n5 dessert options\nCombinations: 1 × 7 × 5 = 35\n\nCase 2: Non-seafood appetizer + chocolate dessert\n\n5 appetizer options (non-seafood)\n3 main course options (only beef or chicken allowed with chocolate)\n1 dessert option (chocolate)\nCombinations: 5 × 3 × 1 = 15\n\nCase 3: Non-seafood appetizer + non-chocolate dessert - 5 appetizer options (non-seafood)\n\n8 main course options (no restrictions)\n4 dessert options (non-chocolate)\nCombinations: 5 × 8 × 4 = 160\n\nTotal valid combinations: 35 + 15 + 160 = 210 combinations\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\nSolution: Combinations with chocolate dessert: 15 (from Case 2 above) Total valid combinations: 210\n\\[P(\\text{chocolate dessert}) = \\frac{15}{210} = \\frac{1}{14} = \\textbf{0.0714}\\]\n\n\n\nSection D: Review - SOLUTIONS\n⏱️ Estimated time: 12 minutes\n\nProblem B3: Daily Expenses - SOLUTION\nGiven:\n\nCoffee: Mean = $1.40, SD = $0.30\nMuffin: Mean = $2.50, SD = $0.15\nPrices are independent\n\nPart (a): What is the mean and standard deviation of the amount she spends on breakfast daily?\nSolution: For the sum of independent random variables:\nMean of daily expenses: \\[E[\\text{Daily}] = E[\\text{Coffee}] + E[\\text{Muffin}] = \\$1.40 + \\$2.50 = \\textbf{\\$3.90}\\]\nVariance of daily expenses: \\[\\text{Var}[\\text{Daily}] = \\text{Var}[\\text{Coffee}] + \\text{Var}[\\text{Muffin}] = (0.30)^2 + (0.15)^2 = 0.09 + 0.0225 = 0.1125\\]\nStandard deviation of daily expenses: \\[SD[\\text{Daily}] = \\sqrt{0.1125} = \\textbf{\\$0.3354}\\]\nPart (b): What is the mean and standard deviation of the amount she spends on breakfast weekly (7 days)?\nSolution: For the sum of 7 independent daily expenses:\nMean of weekly expenses: \\[E[\\text{Weekly}] = 7 \\times E[\\text{Daily}] = 7 \\times \\$3.90 = \\textbf{\\$27.30}\\]\nVariance of weekly expenses: \\[\\text{Var}[\\text{Weekly}] = 7 \\times \\text{Var}[\\text{Daily}] = 7 \\times 0.1125 = 0.7875\\]\nStandard deviation of weekly expenses: \\[SD[\\text{Weekly}] = \\sqrt{0.7875} = \\textbf{\\$0.8874}\\]"
  },
  {
    "objectID": "files/worksheets/worksheet4draft.html",
    "href": "files/worksheets/worksheet4draft.html",
    "title": "PSTAT 5A Practice Worksheet 3",
    "section": "",
    "text": "Instructions and Overview\n⏰ Time Allocation:\n\nSection A (Warm-up): 8 minutes\nSection B (Intermediate): 15 minutes\nSection C (Advanced): 15 minutes\nSection D (Review): 12 minutes\nTotal: 50 minutes\n\n\n📝 Important Instructions:\n\nUse the formulas provided for guidance\nRound final answers to 4 decimal places unless otherwise specified\nIdentify your approach before calculating\nUse calculator as needed\n\n\n\n📚 Key Formulas Reference:\nBasic Probability:\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\cdot P(B_i)\\)\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)\\)\n\nCounting:\n\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\n\n\n\n\nSection A: Probability\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions\nEach row in the table below is a proposed grade distribution for a class. Identify each as a valid or invalid probability distribution, and explain your reasoning.\n\n\n\nClass\nA\nB\nC\nD\nF\n\n\n\n\n(a)\n0.3\n0.3\n0.3\n0.2\n0.1\n\n\n(b)\n0\n0\n1\n0\n0\n\n\n(c)\n0.3\n0.3\n0.3\n0\n0\n\n\n(d)\n0.3\n0.5\n0.2\n0.1\n-0.1\n\n\n(e)\n0.2\n0.4\n0.2\n0.1\n0.1\n\n\n(f)\n0\n-0.1\n1.1\n0\n0\n\n\n\n\nWork Space:\n\n\n\nSection B: Permutations and Combination\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations\nA cybersecurity team needs to create a secure access protocol.\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\n\n\n\n\n\n\nTip\n\n\n\nSince letters must come before digits, think of this as two separate arrangement problems:\n\nFirst, arrange the 3 letters in the first 3 positions\nThen, arrange the 3 digits in the last 3 positions\nUse the multiplication principle to combine these results\n\n\n\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\n\n\n\n\n\n\nTip\n\n\n\nSince order doesn’t matter, this is a combination problem. Ask yourself:\n\nAre we arranging people in specific positions, or just selecting a group?\nWhich formula should you use: \\(P(n,r)\\) or \\(C(n,r)\\)?\n\n\n\n\nWork Space:\n\n\n\nSection C: Conditional Probability\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Conditional Probability and Medical Testing\nA new COVID variant test has the following characteristics:\n\nThe variant affects 3% of the tested population\nThe test correctly identifies 95% of people with the variant (sensitivity)\nThe test correctly identifies 92% of people without the variant (specificity)\n\nPart (a): What is the probability that a randomly selected person tests positive?\nPart (b): If someone tests positive, what is the probability they actually have the variant?\nPart (c): If someone tests negative, what is the probability they actually don’t have the variant?\nPart (d) [Challenge]: The health department wants to reduce false positives. They decide to require two consecutive positive tests for a positive diagnosis. Assuming test results are independent, what is the new probability that someone with two positive tests actually has the variant?\n\nWork Space:\n\n\n\nSection C: Conditional Probability\n⏱️ Estimated time: 15 minutes\n\nProblem C1: Advanced Counting with Restrictions\nA restaurant offers a prix fixe menu where customers must choose:\n\n1 appetizer from 6 options\n1 main course from 8 options\n1 dessert from 5 options\n\nHowever, there are restrictions:\n\nIf you choose the seafood appetizer, you cannot choose the vegetarian main course\nIf you choose the chocolate dessert, you must choose either the beef or chicken main course (3 of the 8 main courses)\n\nPart (a): How many valid meal combinations are possible?\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\n\nWork Space:\n\n\n\nSection D: Review\n⏱️ Estimated time: 12 minutes\n\nProblem B3: Daily Expenses\nSally gets a cup of coffee and a muffin every day for breakfast from one of the many coffee shops in her neighborhood. She picks a coffee shop each morning at random and independently of previous days. The average price of a cup of coffee is $1.40 with a standard deviation of 30¢ ($0.30), the average price of a muffin is $2.50 with a standard deviation of 15¢, and the two prices are independent of each other.\nPart (a): What is the mean and standard deviation of the amount she spends on breakfast daily?\nPart (b): What is the mean and standard deviation of the amount she spends on breakfast weekly (7 days)?\n\nWork Space:"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-a1-distribution-identification",
    "href": "files/worksheets/worksheet4_sln.html#problem-a1-distribution-identification",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "",
    "text": "Important\n\n\n\nInstructions: For each scenario below, identify the appropriate probability distribution and specify its parameters. Justify your choice by identifying the key characteristics.\n\n\n\n\n\n\nA fair coin is flipped until the first head appears. Let \\(X\\) = number of flips needed.\n\nSolution:\n\n\n\n\n\n\nGeometric Distribution with parameter \\(p = 0.5\\)\n\n\n\nKey Characteristics:\n\n✓ We count the number of trials until the first success\n✓ Each flip is independent with constant probability of success\n✓ Only two outcomes per trial (head or tail)\n✓ We stop as soon as we get a success\n\nNotation: \\(X \\sim \\text{Geometric}(p = 0.5)\\)\n\n\n\n\n\n\n\nA quality control inspector tests \\(20\\) randomly selected items from a production line where \\(5\\%\\) are defective. Let \\(X\\) = number of defective items found.\n\nSolution:\n\n\n\n\n\n\nBinomial Distribution with parameters \\(n = 20\\), \\(p = 0.05\\)\n\n\n\nKey Characteristics:\n\n✓ Fixed number of trials (\\(n = 20\\))\n✓ Each item has the same probability of being defective (\\(p = 0.05\\))\n✓ We count the number of successes (defective items)\n✓ Each test is independent\n\nNotation: \\(X \\sim \\text{Binomial}(n = 20, p = 0.05)\\)\n\n\n\n\n\n\n\nA website receives visitors at an average rate of \\(3\\) per minute. Let \\(X\\) = number of visitors in a 2-minute period.\n\nSolution:\n\n\n\n\n\n\nPoisson Distribution with parameter \\(\\lambda = 6\\)\n\n\n\nKey Characteristics:\n\n✓ Events occurring over time at a constant average rate\n✓ Events are independent and rare\n✓ Rate calculation: \\(3 \\text{ visitors/minute} \\times 2 \\text{ minutes} = 6\\) expected visitors\n\nNotation: \\(X \\sim \\text{Poisson}(\\lambda = 6)\\)\n\n\n\n\n\n\n\nA basketball player shoots one free throw with an \\(80\\%\\) success rate. Let \\(X = 1\\) if successful, \\(0\\) if unsuccessful.\n\nSolution:\n\n\n\n\n\n\nBernoulli Distribution with parameter \\(p = 0.8\\)\n\n\n\nKey Characteristics:\n\n✓ Single trial with exactly two outcomes\n✓ Success (make shot) vs. Failure (miss shot)\n✓ Binary outcome: \\(X \\in \\{0, 1\\}\\)\n\nNotation: \\(X \\sim \\text{Bernoulli}(p = 0.8)\\)\n\n\n\n\n\n\n\nA student keeps taking a driving test until they pass. The probability of passing on any attempt is \\(0.7\\). Let \\(X\\) = number of attempts needed to pass.\n\nSolution:\n\n\n\n\n\n\nGeometric Distribution with parameter \\(p = 0.7\\)\n\n\n\nKey Characteristics:\n\n✓ We count trials until first success (passing the test)\n✓ Each attempt is independent with constant probability\n✓ Student continues until success occurs\n\nNotation: \\(X \\sim \\text{Geometric}(p = 0.7)\\)"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-a2-probability-mass-function",
    "href": "files/worksheets/worksheet4_sln.html#problem-a2-probability-mass-function",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "",
    "text": "Given distribution:\n\n\n\nX\n1\n2\n3\n4\n5\n\n\n\n\nP(X=k)\n0.1\n0.3\n0.4\na\n0.1\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Probability Mass Function\n\n\n\n\n\n\n\nSolution. Since probabilities must sum to \\(1\\):\n\\(0.1 + 0.3 + 0.4 + a + 0.1 = 1\\)\n\\(0.9 + a = 1\\)\n\\(\\boxed{a = 0.1}\\)\n\n\n\n\n\nSolution. \\(P(X ≤ 3) = P(X = 1) + P(X = 2) + P(X = 3)\\)\n\\(P(X ≤ 3) = 0.1 + 0.3 + 0.4 = \\boxed{0.8}\\)\n\n\n\n\n\n                            \n                                            \n\n\nFigure 3: PMF showing P(X ≤ 3) = 0.8\n\n\n\n\n\n\n\n\nSolution. \\(P(X &gt; 2) = P(X = 3) + P(X = 4) + P(X = 5)\\)\n\\(P(X &gt; 2) = 0.4 + 0.1 + 0.1 = \\boxed{0.6}\\)\n(Check: \\(0.8 + 0.2 = 1\\) and the full PMF sums to 1, so the results are consistent.)\n\n\n\n\n\n                            \n                                            \n\n\nFigure 4: PMF showing P(X &gt; 2) = 0.6\n\n\n\n\nPutting everything together:\n\n\n\n\n\n\nTip\n\n\n\nKey Insights from Visualizations\nDistribution Shape: The PMF shows \\(X = 3\\) has the highest probability (\\(0.4\\)), making it the mode\nCumulative Probability: \\(P(X ≤ 3) = 0.8\\) means \\(80\\%\\) of outcomes are 3 or less\nComplement Relationship: \\(P(X &gt; 2) = 0.6\\) and \\(P(X ≤ 2) = 0.4\\) sum to \\(1\\)\nSymmetry: The distribution has some symmetry around the center, with equal probabilities at the extremes (\\(X = 1 \\quad \\text{and} \\quad X = 5\\) both have \\(P = 0.1\\))\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 5: PMF showing both P(X ≤ 3) and P(X &gt; 2) regions"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-b1-manual-calculations",
    "href": "files/worksheets/worksheet4_sln.html#problem-b1-manual-calculations",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem B1: Manual Calculations",
    "text": "Problem B1: Manual Calculations\n\nUsing the distribution from Problem A2:\n\n\n\nX\n1\n2\n3\n4\n5\n\n\n\n\nP(X=k)\n0.1\n0.3\n0.4\n0.1\n0.1\n\n\n\n\n\n(a) Compute the expected value (E[X])\n\nSolution. For a discrete random variable, the expected value is the probability-weighted average of all possible outcomes:\n\\[\nE[X] \\;=\\;\\sum_{k=1}^{5} k  \\times \\,P(X=k).\n\\]\n\nSet up the sum\n\n\\[\nE[X] \\;=\\; 1(0.1) \\;+\\; 2(0.3) \\;+\\; 3(0.4) \\;+\\; 4(0.1) \\;+\\; 5(0.1).\n\\]\n\nMultiply each outcome by its probability\n\n\\[\n= 0.1 \\;+\\; 0.6 \\;+\\; 1.2 \\;+\\; 0.4 \\;+\\; 0.5.\n\\]\n\nAdd the terms\n\n\\[\n\\boxed {E[X] = 2.8}\n\\]\n\n\n\n\n\n\n\n\n\nFigure 6: Probability Mass Function showing E[X] = 2.8\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nVisual Interpretation\nLooking at the PMF plot:\n\nThe highest probability (\\(0.4\\)) occurs at \\(X = 3\\)\nThe second highest (\\(0.3\\)) occurs at \\(X = 2\\)\n\nTogether, these two values account for \\(70\\%\\) of the probability mass\nThe expected value \\(E[X] = 2.8\\) (red dashed line) falls between these two most likely outcomes\nThis visual confirms our intuition that the “center of gravity” should be close to, but slightly less than, 3\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation & quick check\nInterpretation: If we were to observe this experiment many, many times, the long-run average value of \\(X\\) would settle down around 2.8. Although 2.8 itself isn’t an attainable outcome (only integers 1–5 are), it represents the center of gravity of the distribution.\nCheck: Notice most probability mass is on 2 and 3 (0.3 + 0.4 = 0.7). A mix that skews slightly toward the larger of those two values should indeed give an average a bit below 3, exactly what we see with 2.8.\n\n\n\n\n\n\n                            \n                                            \n\n\nFigure 7: Multiple Simulation Runs Showing Convergence\n\n\n\n\n\n\n(b) Compute the variance \\(\\operatorname{Var}(X)\\)\n\nSolution. The variance measures how far the values of (X) tend to deviate from the mean.\nWe use the shortcut formula\n\\[\n\\operatorname{Var}(X) \\;=\\; E[X^2] - \\bigl(E[X]\\bigr)^2,\n\\]\nwhere \\(E[X]=2.8\\) was found in part (a).\n\nFind \\(E[X^2]\\) (the mean of the squared outcomes)\n\n\\[\n\\begin{aligned}\nE[X^2]\n&= \\sum_{k=1}^{5} k^{2}\\,P(X=k) \\\\[4pt]\n&= 1^{2}(0.1) \\;+\\; 2^{2}(0.3) \\;+\\; 3^{2}(0.4) \\;+\\; 4^{2}(0.1) \\;+\\; 5^{2}(0.1) \\\\[4pt]\n&= 1(0.1) \\;+\\; 4(0.3) \\;+\\; 9(0.4) \\;+\\; 16(0.1) \\;+\\; 25(0.1) \\\\[4pt]\n&= 0.1 \\;+\\; 1.2 \\;+\\; 3.6 \\;+\\; 1.6 \\;+\\; 2.5 \\\\[4pt]\n&= 9.0\n\\end{aligned}\n\\]\n\nApply the variance formula\n\n\\[\n\\operatorname{Var}(X) \\;=\\; 9.0 - (2.8)^2 = 9.0 - 7.84 = \\boxed{1.16}\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation & quick check\nInterpretation: A variance of \\(1.16\\) tells us that typical values of \\(X\\) deviate from the mean (\\(2.8\\)) by a little over one unit (figure 8).\nCheck: Most probability mass is on 2 and 3; the only “far” value is \\(5\\) (probability \\(0.1\\)). So we expect a modest spread, larger than \\(0\\) but well below the maximum possible of \\((5-2.8)^2 = 4.84\\). The calculated \\(1.16\\) fits this intuition.\n\n\n\n\n(c) Compute the standard deviation \\(\\sigma\\)\n\nSolution. The standard deviation is the square root of the variance:\n\\[\n\\sigma \\;=\\; \\sqrt{\\operatorname{Var}(X)}\n           \\;=\\; \\sqrt{1.16}\n           \\;\\approx\\; \\boxed{1.08}.\n\\]\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation\nA standard deviation (\\(\\sigma\\)) of about \\(1.08\\) means typical observations of \\(X\\) lie roughly one unit above or below the mean value \\(2.8\\). This agrees with our earlier intuition that the distribution is fairly concentrated around \\(2 – 3\\), with only a small chance of the extreme value \\(5\\).\n\n\nLet’s visualise this!\n\n\n\n\n\n\n\n\nFigure 8: PMF with Variance Illustration"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-b2-bernoulli-and-binomial-applications",
    "href": "files/worksheets/worksheet4_sln.html#problem-b2-bernoulli-and-binomial-applications",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem B2: Bernoulli and Binomial Applications",
    "text": "Problem B2: Bernoulli and Binomial Applications\n\nManufacturing Scenario: A manufacturing process has a 15% defect rate.\n\n\n\n(a) Single Item Selection\n\nIf you select one item randomly, what is the expected value and variance of \\(X\\) = number of defective items?\n\n\nSolution. This is a Bernoulli distribution with parameter \\(p = 0.15\\)\n\\[X \\sim \\text{Bernoulli}(p = 0.15)\\]\nStep 1: Expected Value \\[E[X] = p = \\boxed{0.15}\\]\nStep 2: Variance \\[\\text{Var}(X) = p(1-p) = 0.15 \\times 0.85 = \\boxed{0.1275}\\]\nStep 3: Standard Deviation \\[\\sigma = \\sqrt{\\text{Var}(X)} = \\sqrt{0.1275} = \\boxed{0.357}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation:\n\nOn average, 15% of items selected will be defective\nSince this is a single trial, \\(X\\) can only be 0 (not defective) or 1 (defective)\nThe variance measures the uncertainty in this binary outcome\n\n\n\n\n\n\n(b) Multiple Items Selection\n\nIf you select 25 items randomly, what is the expected number of defective items and the standard deviation?\n\n\nSolution. This is a Binomial distribution with parameters \\(n = 25\\), \\(p = 0.15\\)\n\\[X \\sim \\text{Binomial}(n = 25, p = 0.15)\\]\nStep 1: Expected Value \\[E[X] = np = 25 \\times 0.15 = \\boxed{3.75}\\]\nStep 2: Variance \\[\\text{Var}(X) = np(1-p) = 25 \\times 0.15 \\times 0.85 = \\boxed{3.1875}\\]\nStep 3: Standard Deviation \\[\\sigma = \\sqrt{\\text{Var}(X)} = \\sqrt{3.1875} = \\boxed{1.785}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation:\n\nOn average, we expect about 3.75 defective items out of 25\nThe actual number will typically be within ±1.785 items of this average\nValues between 2 and 6 defective items would be quite common"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-c1-binomial-distribution",
    "href": "files/worksheets/worksheet4_sln.html#problem-c1-binomial-distribution",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem C1: Binomial Distribution",
    "text": "Problem C1: Binomial Distribution\nQuiz: 10 questions, 4 choices each, student guesses randomly. This is Binomial(n = 10, p = 0.25)\n(a) What is the probability the student gets exactly 3 questions correct?\nSolution: P(X = 3) = C(10,3) × (0.25)³ × (0.75)⁷ P(X = 3) = 120 × 0.015625 × 0.1335 P(X = 3) = 0.2503\n(b) What is the probability the student gets at least 2 questions correct?\nSolution: P(X ≥ 2) = 1 - P(X ≤ 1) = 1 - [P(X = 0) + P(X = 1)]\nP(X = 0) = C(10,0) × (0.25)⁰ × (0.75)¹⁰ = 1 × 1 × 0.0563 = 0.0563 P(X = 1) = C(10,1) × (0.25)¹ × (0.75)⁹ = 10 × 0.25 × 0.0751 = 0.1877\nP(X ≥ 2) = 1 - (0.0563 + 0.1877) = 1 - 0.2440 P(X ≥ 2) = 0.7560\n(c) What is the expected number of correct answers?\nSolution: E[X] = np = 10 × 0.25 = 2.5"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-c2-poisson-distribution",
    "href": "files/worksheets/worksheet4_sln.html#problem-c2-poisson-distribution",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem C2: Poisson Distribution",
    "text": "Problem C2: Poisson Distribution\nCall center: λ = 4 calls per minute This is Poisson(λ = 4)\n(a) What is the probability of receiving exactly 6 calls in a given minute?\nSolution: P(X = 6) = (λ⁶ × e^(-λ))/6! = (4⁶ × e^(-4))/6! P(X = 6) = (4096 × 0.0183)/720 = 0.1042 P(X = 6) = 0.1042\n(b) What is the probability of receiving no calls in a given minute?\nSolution: P(X = 0) = (λ⁰ × e^(-λ))/0! = (1 × e^(-4))/1 = e^(-4) P(X = 0) = 0.0183\n(c) What is the probability of receiving more than 2 calls in a given minute?\nSolution: P(X &gt; 2) = 1 - P(X ≤ 2) = 1 - [P(X = 0) + P(X = 1) + P(X = 2)]\nP(X = 0) = e^(-4) = 0.0183 P(X = 1) = (4¹ × e^(-4))/1! = 4 × 0.0183 = 0.0733 P(X = 2) = (4² × e^(-4))/2! = 16 × 0.0183/2 = 0.1465\nP(X &gt; 2) = 1 - (0.0183 + 0.0733 + 0.1465) = 1 - 0.2381 P(X &gt; 2) = 0.7619"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-d1-geometric-distribution",
    "href": "files/worksheets/worksheet4_sln.html#problem-d1-geometric-distribution",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem D1: Geometric Distribution",
    "text": "Problem D1: Geometric Distribution\nSoftware updates: p = 0.6 success rate This is Geometric(p = 0.6)\n(a) What is the probability that the first successful bug fix occurs on the 3rd update?\nSolution: P(X = 3) = (1-p)^(k-1) × p = (0.4)² × 0.6 P(X = 3) = 0.16 × 0.6 = 0.0960\n(b) What is the expected number of updates needed to get the first successful bug fix?\nSolution: E[X] = 1/p = 1/0.6 = 1.6667\n(c) What is the probability that it takes more than 4 updates to get the first successful bug fix?\nSolution: P(X &gt; 4) = (1-p)⁴ = (0.4)⁴ = 0.0256"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#problem-d2-mixed-applications",
    "href": "files/worksheets/worksheet4_sln.html#problem-d2-mixed-applications",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Problem D2: Mixed Applications",
    "text": "Problem D2: Mixed Applications\nPharmaceutical testing: p = 0.02 defect rate\n(a) What is the probability that the first defective pill is found on the 5th test?\nSolution: Geometric(p = 0.02) P(X = 5) = (1-p)^(k-1) × p = (0.98)⁴ × 0.02 P(X = 5) = 0.9224 × 0.02 = 0.0184\n(b) What is the expected number of pills they need to test to find the first defective one?\nSolution: Geometric(p = 0.02) E[X] = 1/p = 1/0.02 = 50\n(c) In a batch of 50 pills, what is the probability that exactly 2 pills are defective?\nSolution: Binomial(n = 50, p = 0.02) P(X = 2) = C(50,2) × (0.02)² × (0.98)⁴⁸ P(X = 2) = 1225 × 0.0004 × 0.3773 = 0.1849\n(d) In a batch of 50 pills, what is the expected number of defective pills?\nSolution: Binomial(n = 50, p = 0.02) E[X] = np = 50 × 0.02 = 1"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#distribution-parameters-used",
    "href": "files/worksheets/worksheet4_sln.html#distribution-parameters-used",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Distribution Parameters Used:",
    "text": "Distribution Parameters Used:\n\nA1: Geometric(0.5), Binomial(20,0.05), Poisson(6), Bernoulli(0.8), Geometric(0.7)\nA2: a = 0.1, P(X≤3) = 0.8, P(X&gt;2) = 0.6\nB1: E[X] = 2.8, Var(X) = 1.16, σ = 1.0770\nB2: Bernoulli: E[X]=0.15, Var(X)=0.1275; Binomial: E[X]=3.75, σ=1.7854\nC1: P(X=3)=0.2503, P(X≥2)=0.7560, E[X]=2.5\nC2: P(X=6)=0.1042, P(X=0)=0.0183, P(X&gt;2)=0.7619\nD1: P(X=3)=0.0960, E[X]=1.6667, P(X&gt;4)=0.0256\nD2: P(X=5)=0.0184, E[X]=50, P(X=2)=0.1849, E[X]=1"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "",
    "text": "Conditional Probabilities\nUnderstanding probability when information changes the game"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#welcome-to-lecture-6",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#welcome-to-lecture-6",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "",
    "text": "Conditional Probabilities\nUnderstanding probability when information changes the game"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#todays-learning-objectives",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#todays-learning-objectives",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nDefine and calculate conditional probabilities\nApply the multiplication rule for dependent events\nUse tree diagrams to solve multi-stage problems\nApply the law of total probability\nUse Bayes’ theorem to solve real-world problems\nDistinguish between independence and conditional independence\nRecognize and avoid common conditional probability fallacies"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#motivation-why-conditional-probability",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#motivation-why-conditional-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Motivation: Why Conditional Probability?",
    "text": "Motivation: Why Conditional Probability?\nIn real life, we rarely make decisions with no information\nExamples: - Medical diagnosis with test results - Weather forecast with current conditions\n- Investment decisions with market data - Sports betting with team statistics - Insurance premiums based on risk factors\n\nConditional probability helps us update our beliefs when we gain new information"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#what-is-conditional-probability",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#what-is-conditional-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "What is Conditional Probability?",
    "text": "What is Conditional Probability?\nConditional Probability is the probability of an event occurring, given that another event has already occurred\nNotation: \\(P(A|B)\\) read as “probability of A given B”\n\nKey insight: When we know B has occurred, our sample space effectively “shrinks” to only outcomes where B is true"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#intuitive-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#intuitive-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Intuitive Example",
    "text": "Intuitive Example\nYou roll a fair six-sided die, but before revealing the result, someone tells you “the number is even”\nWhat’s the probability it’s a 4?\n\nWithout information: \\(P(\\text{rolling 4}) = \\frac{1}{6}\\)\nWith information: \\(P(\\text{4 | even}) = ?\\)\nGiven it’s even, possible outcomes: \\(\\{2, 4, 6\\}\\) So \\(P(\\text{4 | even}) = \\frac{1}{3}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#formal-definition",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#formal-definition",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Formal Definition",
    "text": "Formal Definition\nFor events A and B where \\(P(B) &gt; 0\\):\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\nInterpretation:\n\nNumerator: Outcomes where both A and B occur\nDenominator: All outcomes where B occurs\nRatio: Fraction of B-outcomes where A also occurs"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#understanding-the-formula",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#understanding-the-formula",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Understanding the Formula",
    "text": "Understanding the Formula\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWhy this formula makes sense:\n\nWe restrict our attention to outcomes where B occurs\nAmong those outcomes, what fraction also have A?\nThis is exactly \\(\\frac{P(A \\cap B)}{P(B)}\\)\n\n\nRearranging: \\(P(A \\cap B) = P(A|B) \\times P(B)\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-1",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-1",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 1",
    "text": "Practice Problem 1\nA card is drawn from a standard 52-card deck. Find:\n\n\\(P(\\text{King | Face card})\\)\n\\(P(\\text{Heart | Red card})\\)\n\n\\(P(\\text{Ace | Black card})\\)\n\n\nSolutions:\n\n\\(P(\\text{King | Face}) = \\frac{4}{12} = \\frac{1}{3}\\) (4 kings among 12 face cards)\n\\(P(\\text{Heart | Red}) = \\frac{13}{26} = \\frac{1}{2}\\) (13 hearts among 26 red cards)\n\\(P(\\text{Ace | Black}) = \\frac{2}{26} = \\frac{1}{13}\\) (2 black aces among 26 black cards)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#two-way-tables",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#two-way-tables",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Two-Way Tables",
    "text": "Two-Way Tables\nTwo-way tables are excellent for conditional probability problems\nExample: Survey of 1000 people about coffee preference\n\n\n\n\nCoffee\nNo Coffee\nTotal\n\n\n\n\nMorning\n350\n150\n500\n\n\nEvening\n200\n300\n500\n\n\nTotal\n550\n450\n1000"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#using-two-way-tables",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#using-two-way-tables",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Using Two-Way Tables",
    "text": "Using Two-Way Tables\nFind: \\(P(\\text{Coffee | Morning person})\\)\nFrom the table:\n\nMorning people: 500\nMorning people who drink coffee: 350\n\n\n\\(P(\\text{Coffee | Morning}) = \\frac{350}{500} = 0.7\\)\nCompare to: \\(P(\\text{Coffee}) = \\frac{550}{1000} = 0.55\\)\nBeing a morning person increases coffee probability!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-2",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-2",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 2",
    "text": "Practice Problem 2\nUsing the coffee table, find:\n\n\\(P(\\text{Morning | Coffee drinker})\\)\n\\(P(\\text{No Coffee | Evening person})\\)\n\\(P(\\text{Evening | No Coffee})\\)\n\n\nSolutions:\n\n\\(P(\\text{Morning | Coffee}) = \\frac{350}{550} = \\frac{7}{11} \\approx 0.636\\)\n\\(P(\\text{No Coffee | Evening}) = \\frac{300}{500} = 0.6\\)\n\\(P(\\text{Evening | No Coffee}) = \\frac{300}{450} = \\frac{2}{3} \\approx 0.667\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#independence-revisited",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#independence-revisited",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Independence Revisited",
    "text": "Independence Revisited\nEvents A and B are independent if knowing that B occurred doesn’t change the probability of A\n\\[P(A|B) = P(A)\\]\nEquivalently: \\[P(A \\cap B) = P(A) \\times P(B)\\]\n\nExample: Two coin flips are independent because \\(P(\\text{H}_2 | \\text{H}_1) = P(\\text{H}_2) = 0.5\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#testing-for-independence",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#testing-for-independence",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Testing for Independence",
    "text": "Testing for Independence\nMethod 1: Check if \\(P(A|B) = P(A)\\)\nMethod 2: Check if \\(P(A \\cap B) = P(A) \\times P(B)\\)\nMethod 3: Check if \\(P(B|A) = P(B)\\)\n\nCoffee Example: Are coffee preference and time preference independent?\n\\(P(\\text{Coffee}) = 0.55\\)\n\\(P(\\text{Coffee | Morning}) = 0.7\\)\nSince \\(0.7 \\neq 0.55\\), they are not independent"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#the-multiplication-rule",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#the-multiplication-rule",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "The Multiplication Rule",
    "text": "The Multiplication Rule\nGeneral Multiplication Rule: \\[P(A \\cap B) = P(A) \\times P(B|A) = P(B) \\times P(A|B)\\]\nFor Independent Events: \\[P(A \\cap B) = P(A) \\times P(B)\\]\n\nExtension to Multiple Events: \\[P(A_1 \\cap A_2 \\cap A_3) = P(A_1) \\times P(A_2|A_1) \\times P(A_3|A_1 \\cap A_2)\\]"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#multiplication-rule-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#multiplication-rule-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Multiplication Rule Example",
    "text": "Multiplication Rule Example\nA jar contains 5 red balls and 3 blue balls. Two balls are drawn without replacement. What’s the probability both are red?\n\nLet \\(R_1\\) = first ball is red, \\(R_2\\) = second ball is red\n\\(P(R_1 \\cap R_2) = P(R_1) \\times P(R_2|R_1)\\)\n\\(= \\frac{5}{8} \\times \\frac{4}{7} = \\frac{20}{56} = \\frac{5}{14}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-3",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-3",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 3",
    "text": "Practice Problem 3\nA box contains 4 defective and 6 working items. Three items are selected without replacement. Find the probability that:\n\nAll three work\nThe first two work and the third is defective\nExactly two work\n\n\nSolutions: a) \\(P(\\text{WWW}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{120}{720} = \\frac{1}{6}\\)\n\n\\(P(\\text{WWD}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{120}{720} = \\frac{1}{6}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-3-continued",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-3-continued",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 3 (continued)",
    "text": "Practice Problem 3 (continued)\n\nExactly two work (three scenarios: WWD, WDW, DWW)\n\n\\(P(\\text{WWD}) = \\frac{6}{10} \\times \\frac{5}{9} \\times \\frac{4}{8} = \\frac{1}{6}\\)\n\\(P(\\text{WDW}) = \\frac{6}{10} \\times \\frac{4}{9} \\times \\frac{5}{8} = \\frac{1}{6}\\)\n\\(P(\\text{DWW}) = \\frac{4}{10} \\times \\frac{6}{9} \\times \\frac{5}{8} = \\frac{1}{6}\\)\n\nTotal: \\(\\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagrams",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagrams",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagrams",
    "text": "Tree Diagrams\nTree diagrams help visualize sequential events and conditional probabilities\n                    0.5   Red\n            0.6 ──┐\n                    0.5   Blue\nBall 1      \n                    0.4   Red  \n            0.4 ──┐\n                    0.6   Blue\nEach branch shows conditional probabilities"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagram-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagram-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagram Example",
    "text": "Tree Diagram Example\nMedical test scenario: - 2% of population has disease - Test is 95% accurate for sick people\n- Test is 90% accurate for healthy people\nWhat’s the probability of testing positive?"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagram-solution",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#tree-diagram-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Tree Diagram Solution",
    "text": "Tree Diagram Solution\n                    0.95   Test +\n            0.02 ──┐\n                    0.05   Test -\nDisease?    \n                    0.10   Test +\n            0.98 ──┐\n                    0.90   Test -\n\n\\(P(\\text{Test+}) = P(\\text{Test+|Disease}) \\times P(\\text{Disease}) + P(\\text{Test+|Healthy}) \\times P(\\text{Healthy})\\)\n\\(= 0.95 \\times 0.02 + 0.10 \\times 0.98 = 0.019 + 0.098 = 0.117\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#law-of-total-probability",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#law-of-total-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\nIf events \\(B_1, B_2, \\ldots, B_n\\) form a partition of the sample space (mutually exclusive and exhaustive), then:\n\\[P(A) = \\sum_{i=1}^{n} P(A|B_i) \\times P(B_i)\\]\n\nPartition means:\n\n\\(B_i \\cap B_j = \\emptyset\\) for \\(i \\neq j\\) (mutually exclusive)\n\\(\\bigcup_{i=1}^{n} B_i = S\\) (exhaustive)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#law-of-total-probability-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#law-of-total-probability-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Law of Total Probability Example",
    "text": "Law of Total Probability Example\nA factory has three machines: - Machine A: 50% of production, 1% defective - Machine B: 30% of production, 2% defective\n- Machine C: 20% of production, 3% defective\nWhat’s the overall defect rate?\n\n\\(P(\\text{Defective}) = P(D|A)P(A) + P(D|B)P(B) + P(D|C)P(C)\\)\n\\(= 0.01 \\times 0.5 + 0.02 \\times 0.3 + 0.03 \\times 0.2\\)\n\\(= 0.005 + 0.006 + 0.006 = 0.017 = 1.7\\%\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-4",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-4",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 4",
    "text": "Practice Problem 4\nA student studies for an exam with three possible outcomes based on study time: - Studies hard (40%): 90% chance of passing - Studies moderately (35%): 70% chance of passing\n- Doesn’t study (25%): 30% chance of passing\nWhat’s the overall probability of passing?\n\n\\(P(\\text{Pass}) = 0.9 \\times 0.4 + 0.7 \\times 0.35 + 0.3 \\times 0.25\\)\n\\(= 0.36 + 0.245 + 0.075 = 0.68\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem",
    "text": "Bayes’ Theorem\nThe Foundation: We often want to “reverse” conditional probabilities\nGiven: \\(P(B|A)\\), \\(P(A)\\), \\(P(B)\\) Want: \\(P(A|B)\\)\nBayes’ Theorem: \\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem-components",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem-components",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem Components",
    "text": "Bayes’ Theorem Components\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\]\n\n\\(P(A|B)\\): Posterior probability (what we want)\n\\(P(B|A)\\): Likelihood (what we observe)\n\n\\(P(A)\\): Prior probability (initial belief)\n\\(P(B)\\): Evidence (marginal probability)\n\n\n“In light of evidence B, how should we update our belief in A?”"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem-with-total-probability",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#bayes-theorem-with-total-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bayes’ Theorem with Total Probability",
    "text": "Bayes’ Theorem with Total Probability\nWhen we need to find \\(P(B)\\):\n\\[P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B|A) \\times P(A) + P(B|A^c) \\times P(A^c)}\\]\nThis is the most common form for applications"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#medical-diagnosis-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#medical-diagnosis-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Medical Diagnosis Example",
    "text": "Medical Diagnosis Example\nRevisiting our medical test: - 2% of population has disease (prior) - Test positive (evidence)\n- Test is 95% accurate for sick, 90% accurate for healthy\nGiven a positive test, what’s the probability of having the disease?"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#medical-diagnosis-solution",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#medical-diagnosis-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Medical Diagnosis Solution",
    "text": "Medical Diagnosis Solution\nLet D = disease, T+ = positive test\n\\[P(D|T+) = \\frac{P(T+|D) \\times P(D)}{P(T+|D) \\times P(D) + P(T+|D^c) \\times P(D^c)}\\]\n\\[= \\frac{0.95 \\times 0.02}{0.95 \\times 0.02 + 0.10 \\times 0.98}\\]\n\\[= \\frac{0.019}{0.019 + 0.098} = \\frac{0.019}{0.117} \\approx 0.162\\]\n\nSurprising: Only 16.2% chance of disease despite positive test!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#why-the-low-probability",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#why-the-low-probability",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Why the Low Probability?",
    "text": "Why the Low Probability?\nBase Rate Fallacy: When disease is rare (2%), most positive tests are false positives\nIntuition: Out of 10,000 people: - 200 have disease → 190 test positive\n- 9,800 healthy → 980 test positive - Total positive tests: 1,170 - True positives: 190\n\\(P(\\text{Disease | Positive}) = \\frac{190}{1,170} \\approx 0.162\\) ✓"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-5",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-5",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 5",
    "text": "Practice Problem 5\nEmail spam filter: - 60% of emails are spam - Filter catches 95% of spam - Filter incorrectly flags 8% of legitimate emails\nIf an email is flagged as spam, what’s the probability it’s actually spam?\n\n\\(P(\\text{Spam | Flagged}) = \\frac{0.95 \\times 0.6}{0.95 \\times 0.6 + 0.08 \\times 0.4}\\)\n\\(= \\frac{0.57}{0.57 + 0.032} = \\frac{0.57}{0.602} \\approx 0.947\\)\nThe filter is quite reliable!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#multiple-events-and-bayes",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#multiple-events-and-bayes",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Multiple Events and Bayes’",
    "text": "Multiple Events and Bayes’\nExtended Bayes’ Theorem: If \\(A_1, A_2, \\ldots, A_n\\) partition the sample space:\n\\[P(A_i|B) = \\frac{P(B|A_i) \\times P(A_i)}{\\sum_{j=1}^{n} P(B|A_j) \\times P(A_j)}\\]\nThis allows us to update probabilities for multiple hypotheses"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#three-machine-example-revisited",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#three-machine-example-revisited",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Three Machine Example Revisited",
    "text": "Three Machine Example Revisited\nA defective item is found. Which machine most likely produced it?\nFrom before: - Machine A: 50% production, 1% defective\n- Machine B: 30% production, 2% defective - Machine C: 20% production, 3% defective - Overall defect rate: 1.7%"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#three-machine-solution",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#three-machine-solution",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Three Machine Solution",
    "text": "Three Machine Solution\n\\[P(A|\\text{Defective}) = \\frac{0.01 \\times 0.5}{0.017} = \\frac{0.005}{0.017} \\approx 0.294\\]\n\\[P(B|\\text{Defective}) = \\frac{0.02 \\times 0.3}{0.017} = \\frac{0.006}{0.017} \\approx 0.353\\]\n\\[P(C|\\text{Defective}) = \\frac{0.03 \\times 0.2}{0.017} = \\frac{0.006}{0.017} \\approx 0.353\\]\n\nMachine B or C are most likely sources of the defective item"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-6",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-6",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 6",
    "text": "Practice Problem 6\nThree boxes contain colored balls: - Box 1: 3 red, 2 blue (chosen 40% of time) - Box 2: 2 red, 3 blue (chosen 35% of time)\n- Box 3: 1 red, 4 blue (chosen 25% of time)\nA red ball is drawn. Which box was it most likely from?\n\n\\(P(\\text{Red}) = \\frac{3}{5} \\times 0.4 + \\frac{2}{5} \\times 0.35 + \\frac{1}{5} \\times 0.25 = 0.24 + 0.14 + 0.05 = 0.43\\)\n\\(P(\\text{Box 1 | Red}) = \\frac{0.6 \\times 0.4}{0.43} \\approx 0.558\\) \\(P(\\text{Box 2 | Red}) = \\frac{0.4 \\times 0.35}{0.43} \\approx 0.326\\)\n\\(P(\\text{Box 3 | Red}) = \\frac{0.2 \\times 0.25}{0.43} \\approx 0.116\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#conditional-independence",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#conditional-independence",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Conditional Independence",
    "text": "Conditional Independence\nEvents A and B are conditionally independent given C if:\n\\[P(A \\cap B | C) = P(A|C) \\times P(B|C)\\]\nImportant: Conditional independence doesn’t imply independence!\n\nExample: Weather in two cities may be independent normally, but conditionally dependent given a major weather system"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#simpsons-paradox",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#simpsons-paradox",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nSimpson’s Paradox: A trend in subgroups can reverse when groups are combined\nClassic Example: University admissions by gender\n\n\n\n\nMen\nWomen\n\n\n\n\nDept A\n62% (825/1327)\n82% (108/131)\n\n\nDept B\n63% (560/893)\n68% (25/37)\n\n\nOverall\n44% (1385/2220)\n30% (133/168)\n\n\n\nWomen have higher rates in each department but lower overall!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#common-fallacies",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#common-fallacies",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Common Fallacies",
    "text": "Common Fallacies\n1. Confusion of the Inverse - Confusing \\(P(A|B)\\) with \\(P(B|A)\\) - “If it rains, the ground is wet” ≠ “If the ground is wet, it rained”\n2. Base Rate Neglect\n- Ignoring prior probabilities - Medical test example\n3. Prosecutor’s Fallacy - \\(P(\\text{Evidence | Innocent}) \\neq P(\\text{Innocent | Evidence})\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#prosecutors-fallacy-example",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#prosecutors-fallacy-example",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Prosecutor’s Fallacy Example",
    "text": "Prosecutor’s Fallacy Example\nDNA evidence matches defendant with probability 1 in a million for random person\nWrong reasoning: “Probability of innocence is 1 in a million”\nCorrect reasoning: Need to consider: - How many people could have committed the crime? - What’s the prior probability of guilt? - Possibility of lab error, planted evidence, etc."
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-7",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-7",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 7",
    "text": "Practice Problem 7\nQuality control: 5% of items are defective. A test detects 96% of defective items but has a 4% false positive rate.\n\nWhat’s the probability an item testing positive is actually defective?\nWhat’s the probability an item testing negative is actually good?\n\n\n\n\\(P(\\text{Defective | Positive}) = \\frac{0.96 \\times 0.05}{0.96 \\times 0.05 + 0.04 \\times 0.95} = \\frac{0.048}{0.086} \\approx 0.558\\)\n\\(P(\\text{Good | Negative}) = \\frac{0.96 \\times 0.95}{0.96 \\times 0.95 + 0.04 \\times 0.05} = \\frac{0.912}{0.914} \\approx 0.998\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#real-world-applications",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#real-world-applications",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Real-World Applications",
    "text": "Real-World Applications\nMedical Screening: - Mammograms, COVID tests - Balancing sensitivity vs specificity\nMachine Learning: - Naive Bayes classifiers - Spam detection, recommendation systems\nFinance: - Credit scoring - Fraud detection\nLegal System: - DNA evidence interpretation - Probability of guilt/innocence"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#technology-and-tools",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#technology-and-tools",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Technology and Tools",
    "text": "Technology and Tools\nCalculators: - Basic probability calculations - Watch for rounding errors\nSoftware: - R: conditional probability tables - Python: pandas for two-way tables - Excel: pivot tables for conditional analysis\nVisualization: - Tree diagrams\n- Contingency tables - Bayes networks"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#diagnostic-thinking",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#diagnostic-thinking",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Diagnostic Thinking",
    "text": "Diagnostic Thinking\nQuestions to ask: 1. What information am I conditioning on? 2. How does this information change the probability? 3. What’s the base rate or prior probability? 4. Am I confusing \\(P(A|B)\\) with \\(P(B|A)\\)? 5. Are the events independent?"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#problem-solving-strategy",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#problem-solving-strategy",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Problem-Solving Strategy",
    "text": "Problem-Solving Strategy\n\nIdentify the type: Direct conditional, Bayes’, or law of total probability?\nDefine events clearly: Use precise notation\nOrganize information: Two-way tables or tree diagrams\nCheck for independence: Does additional info matter?\nApply appropriate formula: Don’t forget denominators!\nVerify answer: Does it make intuitive sense?"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-8",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-8",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 8",
    "text": "Practice Problem 8\nA survey shows: - 70% of people like pizza - 60% of people like movies\n- 40% like both pizza and movies\n\nAre liking pizza and movies independent?\nWhat’s \\(P(\\text{Pizza | Movies})\\)?\nWhat’s \\(P(\\text{Movies | Pizza})\\)?"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-8-solutions",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#practice-problem-8-solutions",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Practice Problem 8 Solutions",
    "text": "Practice Problem 8 Solutions\n\nCheck independence: \\(P(\\text{Pizza}) \\times P(\\text{Movies}) = 0.7 \\times 0.6 = 0.42 \\neq 0.4\\) Not independent!\n\\(P(\\text{Pizza | Movies}) = \\frac{P(\\text{Pizza} \\cap \\text{Movies})}{P(\\text{Movies})} = \\frac{0.4}{0.6} = \\frac{2}{3}\\)\n\\(P(\\text{Movies | Pizza}) = \\frac{P(\\text{Pizza} \\cap \\text{Movies})}{P(\\text{Pizza})} = \\frac{0.4}{0.7} = \\frac{4}{7}\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#advanced-topics-preview",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#advanced-topics-preview",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Advanced Topics Preview",
    "text": "Advanced Topics Preview\nMarkov Chains: - Sequences where future depends only on present - \\(P(X_{n+1} | X_n, X_{n-1}, \\ldots, X_1) = P(X_{n+1} | X_n)\\)\nBayesian Statistics: - Using Bayes’ theorem for statistical inference - Updating beliefs with data\nInformation Theory: - Conditional entropy - Mutual information"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#historical-context",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#historical-context",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Historical Context",
    "text": "Historical Context\nThomas Bayes (1701-1761): - Presbyterian minister and mathematician - Bayes’ theorem published posthumously\nPierre-Simon Laplace (1749-1827): - Developed and popularized Bayesian methods - “Probability is nothing but common sense reduced to calculation”\nModern Applications: AI, machine learning, medical diagnosis, finance"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#common-student-questions",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#common-student-questions",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Common Student Questions",
    "text": "Common Student Questions\nQ: “How do I know when to use Bayes’ theorem?” A: When you want to “reverse” a conditional probability\nQ: “Why are medical test problems so counterintuitive?”\nA: Base rates matter more than we intuitively expect\nQ: “What’s the difference between independence and conditional independence?” A: Independence means no relationship; conditional independence means no relationship given specific information"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#key-formulas-summary",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#key-formulas-summary",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Key Formulas Summary",
    "text": "Key Formulas Summary\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\times P(B|A)\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\times P(B_i)\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}\\)\nIndependence: \\(P(A|B) = P(A)\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#looking-ahead",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#looking-ahead",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nNext lecture: Discrete Random Variables - Random variables as functions - Probability mass functions - Expected value and variance - Common discrete distributions (binomial, geometric, Poisson)\nConnection: Conditional probability is essential for understanding dependence in random variables"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#study-tips",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#study-tips",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Study Tips",
    "text": "Study Tips\n\nPractice with real scenarios: Medical tests, quality control\nDraw diagrams: Tree diagrams and two-way tables\nCheck your intuition: Do answers make sense?\nMaster the basics: Conditional probability formula\nWatch for fallacies: Don’t confuse \\(P(A|B)\\) and \\(P(B|A)\\)"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#final-thoughts",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#final-thoughts",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nConditional probability is everywhere: - Updates beliefs with new information - Foundation of Bayesian thinking - Critical for proper statistical reasoning - Essential for machine learning and AI\n\nKey insight: Information changes probability - embrace this uncertainty!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#questions",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#questions",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Questions?",
    "text": "Questions?\nOffice Hours: [Your office hours] Email: [Your email]\nNext Class: Discrete Random Variables\nRemember: Homework due [date]"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#bonus-monty-hall-problem",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#bonus-monty-hall-problem",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bonus: Monty Hall Problem",
    "text": "Bonus: Monty Hall Problem\nThree doors: one has a car, two have goats 1. You choose a door 2. Host opens a door with a goat 3. Do you switch?\n\nAnswer: Yes! Switch! - \\(P(\\text{Car behind your door}) = \\frac{1}{3}\\) - \\(P(\\text{Car behind other remaining door}) = \\frac{2}{3}\\)\nConditional probability in action!"
  },
  {
    "objectID": "files/lecture_notes/test_lectures/lecture8_condProb.html#bonus-birthday-paradox-connection",
    "href": "files/lecture_notes/test_lectures/lecture8_condProb.html#bonus-birthday-paradox-connection",
    "title": "PSTAT 5A: Conditional Probabilities",
    "section": "Bonus: Birthday Paradox Connection",
    "text": "Bonus: Birthday Paradox Connection\nIn a room of 23 people, probability of shared birthday ≈ 50%\nConditional approach: What’s \\(P(\\text{no match | first $k$ people have different birthdays})\\)?\nThis helps build intuition for why the probability grows so quickly!\nSurprising results often involve conditional probability!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#continuous-random-variables",
    "href": "files/lecture_notes/lecture8/lecture8.html#continuous-random-variables",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\nWhy P(X = x) = 0 for Continuous Variables?\n\nFor continuous random variables, the probability of any exact value is zero!\nThink about it: What’s the probability someone is exactly 5.7324681… feet tall?\n\n\n\nInstead, we ask:\n\nP(5.7 ≤ X ≤ 5.8)?\nP(X ≤ 6.0)?\nP(X &gt; 5.5)?\n\nKey insight: We calculate probabilities for intervals, not exact points.\n\n\n\n\n\nClick to see why P(X = exact value) = 0"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#sec-dis-cont-vars",
    "href": "files/lecture_notes/lecture8/lecture8.html#sec-dis-cont-vars",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Review: Discrete vs Continuous",
    "text": "Review: Discrete vs Continuous\n\n\n\n\nDiscrete Random Variables\n\n\n\nCountable values (can list them)\n\n\nGaps between possible values\n\n\nUses Probability Mass Function (PMF)\n\n\n\\(P(X = x)\\) makes sense\n\n\n\nExamples: Dice rolls, number of emails, quiz scores\n\n\n\n\n\n\n\n\n\nContinuous Random Variables\n\n\n\nUncountable values (infinite possibilities)\n\n\nNo gaps - any value in an interval\n\n\nUses Probability Density Function (PDF)\n\n\n\\(P(X = x) = 0\\) for any specific value!\n\n\n\nExamples: Height, weight, time, temperature"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#sec-pdf",
    "href": "files/lecture_notes/lecture8/lecture8.html#sec-pdf",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Why P(X = x) = 0 for Continuous Variables?",
    "text": "Why P(X = x) = 0 for Continuous Variables?\n\n\nFor continuous random variables, the probability of any exact value is zero!\nThink about it: What’s the probability someone is exactly 5.7324681… feet tall?\n\n\n\n\nInstead, we ask:\n\nP(5.7 ≤ X ≤ 5.8)?\nP(X ≤ 6.0)?\nP(X &gt; 5.5)?\n\nKey insight: We calculate probabilities for intervals, not exact points.\n\n\n\n\n\nClick to see why P(X = exact value) = 0"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#understanding-distribution-parameters",
    "href": "files/lecture_notes/lecture8/lecture8.html#understanding-distribution-parameters",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Understanding Distribution Parameters",
    "text": "Understanding Distribution Parameters\n\n\nUniform Distribution ( (a, b) ) - Parameter 1: ( a ) (minimum value) - Parameter 2: ( b ) (maximum value) - Example: ( (0, 10) ) means values between 0 and 10 are equally likely - Use: Random numbers, waiting times with fixed bounds\n\n\nNormal Distribution ( N(, ^2) ) - Parameter 1: ( ) (mean/center) - Parameter 2: ( ) (standard deviation/spread) - Example: ( N(64, 2.5^2) ) for heights with mean 64 inches, SD 2.5 inches - Use: Heights, test scores, measurement errors\n\n\nExponential Distribution ( () ) - Parameter 1: ( ) (rate parameter) - Parameter 2: (not used) - Example: ( (0.2) ) means average time between events is ( 1/0.2 = 5 ) minutes - Use: Time between events, lifetimes, waiting times\n\n:::\n\nRemember: The parameters control the shape, location, and spread of each distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#understanding-continuous-distribution-parameters",
    "href": "files/lecture_notes/lecture8/lecture8.html#understanding-continuous-distribution-parameters",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Understanding Continuous Distribution Parameters",
    "text": "Understanding Continuous Distribution Parameters\n\n\nUniform Distribution ( (a, b) ) - Parameter 1: ( a ) (minimum value) - Parameter 2: ( b ) (maximum value) - Example: ( (0, 10) ) means values between 0 and 10 are equally likely - Use: Random numbers, waiting times with fixed bounds\n\n\nNormal Distribution ( N(, ^2) ) - Parameter 1: ( ) (mean/center) - Parameter 2: ( ) (standard deviation/spread) - Example: ( N(64, 2.5^2) ) for heights with mean 64 inches, SD 2.5 inches - Use: Heights, test scores, measurement errors\n\n\nExponential Distribution ( () ) - Parameter 1: ( ) (rate parameter) - Parameter 2: (not used) - Example: ( (0.2) ) means average time between events is ( 1/0.2 = 5 ) minutes - Use: Time between events, lifetimes, waiting times\n\n\nRemember: The parameters control the shape, location, and spread of each distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#the-normal-distribution",
    "href": "files/lecture_notes/lecture8/lecture8.html#the-normal-distribution",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\n\n\n\nWhy Normal is Special\n\nCentral Limit Theorem: Sample means approach normal\n68-95-99.7 Rule:\n\n68% within \\(1 \\sigma\\) of \\(\\mu\\)\n95% within \\(2 \\sigma\\) of \\(\\mu\\)\n\n99.7% within \\(3 \\sigma\\) of \\(\\mu\\)\n\nStandard Normal: \\(\\mu = 0\\) , \\(\\sigma = 1\\)\n\nZ-Score Transformation\n\\[Z = \\frac{X - \\mu}{\\sigma}\\]"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#summary-table",
    "href": "files/worksheets/worksheet4_sln.html#summary-table",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "",
    "text": "Table 1: Distribution Identification Summary\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\nDecision Framework Visualization\n\n\n\n\n                            \n                                            \n\n\nFigure 1: Decision Framework for Distribution Identification\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuick Reference Guide Ask these key questions to identify distributions:\nHow many trials?\n\nOne trial → Bernoulli\nFixed number → Binomial (if counting successes)\nUntil first success → Geometric\n\nWhat are we counting?\n\nSuccesses in fixed trials → Binomial\nTrials until success → Geometric\nEvents over time/space → Poisson\n\nTime component?\n\nEvents at constant rate over time → Poisson\nNo time component → Binomial/Bernoulli/Geometric\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGeometric vs. Binomial: Geometric counts trials until success;\n\nBinomial counts successes in fixed trials\nPoisson parameter: Remember to multiply rate by time period (e.g., 3/minute × 2 minutes = \\(\\lambda\\) = 6)\n\nIndependence assumption: All these distributions require independent trials/events"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#visualizations",
    "href": "files/worksheets/worksheet4_sln.html#visualizations",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Visualizations",
    "text": "Visualizations\nLet’s visualize this to build more intuition\n\nBernoulli Distribution (Single Item)\n\n\n\n\n                            \n                                            \n\n\nFigure 9: Bernoulli Distribution: P(X=k) for Single Item\n\n\n\n\n\nBinomial Distribution (25 Items)\n\n\n\n\n                            \n                                            \n\n\nFigure 10: Binomial Distribution: Number of Defective Items in 25 Trials\n\n\n\n\n\n\nComparison: Bernoulli vs Binomial Relationship\n\n\n\n\n                            \n                                            \n\n\nFigure 11: Relationship Between Bernoulli and Binomial Distributions\n\n\n\n\n\n\n\n\n\nTable 2: Summary of Bernoulli vs Binomial Distributions\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(\\textbf{Bernoulli} \\rightarrow \\text{Binomial Connection:}\\)\n\nA Binomial distribution is the sum of \\(n\\) independent Bernoulli trials\nIf \\(X_1, X_2, \\dots, X_{25}\\) are independent \\(\\text{Bernoulli}(0.15)\\), then \\(X_1 + X_2 + \\cdots + X_{25} \\sim \\text{Binomial}(25, 0.15)\\)\n\n\\(\\textbf{Scaling Formulas:}\\)\n\n\\(\\textbf{Expected Value:}\\) \\(E[\\text{Binomial}] = n \\times E[\\text{Bernoulli}]\\) = \\(25 \\times 0.15 = 3.75\\)\n\\(\\textbf{Variance:}\\) \\(\\text{Var}(\\text{Binomial}) = n \\times \\text{Var}(\\text{Bernoulli}) = 25 \\times 0.1275 = 3.1875\\)\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nSingle inspection: \\(15\\%\\) chance of finding a defect\nBatch inspection (\\(25\\) items): Expect \\(3-4\\) defective items typically Acceptable range: \\(2-6\\) defective items would be within \\(1\\) standard deviation\nRed flag: Finding 7+ defective items might indicate process issues (beyond 2 \\(\\sigma\\))"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#a-binomial-vs.-geometric-distributions",
    "href": "files/worksheets/worksheet4_sln.html#a-binomial-vs.-geometric-distributions",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "(a) Binomial vs. Geometric Distributions",
    "text": "(a) Binomial vs. Geometric Distributions\n\nExplain the key difference between a Binomial distribution and a Geometric distribution in terms of what they count.\n\nSolution:\n\n\n\n\n\n\nKey Difference: What We Count\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nWhat We Count\nFixed Parameter\nVariable\n\n\n\n\nBinomial\nNumber of successes\nNumber of trials (n)\nNumber of successes\n\n\nGeometric\nNumber of trials\nUntil first success\nNumber of trials\n\n\n\n\n\n\nBinomial Distribution: Counts the number of successes in a fixed number of trials\n\nExample: “How many heads in 10 coin flips?”\nWe know we’ll flip exactly 10 times, but don’t know how many heads\n\nGeometric Distribution: Counts the number of trials needed to get the first success\n\nExample: “How many coin flips until the first head?”\nWe know we’ll get exactly 1 head, but don’t know how many flips it takes\n\n\n\nVisual Comparison : Binomial vs Geometric: Fundamental Difference\n\n\n\n\n                            \n                                            \n\n\nFigure 12: Binomial vs Geometric: What They Count\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBinomial: “How many successes in a fixed box of trials?”\nFixed trials, variable successes\nGeometric: “How many attempts until first success?”\nFixed successes (1), variable trials\n\n\n\nPoisson vs. Binomial: When to Use Each\n\n\nWhen would you use a Poisson distribution instead of a Binomial distribution?\n\n\nSolution. Use Poisson when:\nEvents occur over time or space at a constant rate The number of possible events is very large but the probability of each is very small We don’t have a fixed number of trials Examples: arrivals, defects per unit area, accidents per day\n\n\n\n\n\n\n\nNote\n\n\n\nDecision Framework: Poisson vs. Binomial CriterionUse BinomialUse PoissonTrialsFixed number (n)No fixed limitTime/SpaceNot the focusEvents over time/spaceProbabilityModerate pVery small pRateNot applicableConstant rate (λ)ExamplesCoin flips, surveysPhone calls, defects\n\n\nComparative Examples\n\n\n\n\n                            \n                                            \n\n\nFigure 13: Poisson vs Binomial: When to Use Each\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake Don’t use Poisson just because events are “rare.” The key criteria are:\nNo fixed number of trials Events over time/space Constant rate\nA rare event in a fixed number of trials is still Binomial!\n\n\n\nVariance Maximization in Binomial Distribution :::{.problem} If X∼Binomial(n,p)X (n, p) X∼Binomial(n,p), under what conditions would the variance be maximized? :::\n\n\nSolution. For a Binomial distribution: Var(X)=np(1−p)(X) = np(1-p) Var(X)=np(1−p) For fixed nn n, variance is maximized when p(1−p)p(1-p) p(1−p) is maximized.\nMathematical Approach:Taking the derivative with respect to pp p:\nddp[p(1−p)]=ddp[p−p2]=1−2p[p(1-p)] = [p - p^2] = 1 - 2pdpd​[p(1−p)]=dpd​[p−p2]=1−2p Setting equal to zero:\n1−2p=0  ⟹  p=0.51 - 2p = 0 1−2p=0⟹p=0.5​ Verification: Second derivative = −2&lt;0-2 &lt; 0 −2&lt;0, confirming this is a maximum.\nConclusion: The variance is maximized when p=0.5p = 0.5 p=0.5 (fair coin scenario).\n\nVisualization of Variance vs. Probability\n\n\n\n\n                            \n                                            \n\n\nFigure 14: Binomial Variance Maximization: Effect of p\n\n\n\n\nMathematical Proof and Intuition\n\n\n\n\n                            \n                                            \n\n\nFigure 15: Why p = 0.5 Maximizes Variance: Mathematical Intuition\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Insights\nMathematical Maximum: p=0.5p = 0.5 p=0.5 maximizes p(1−p)p(1-p) p(1−p) for any fixed nn n Intuitive Explanation: Maximum uncertainty occurs when success and failure are equally likely Practical Meaning: A fair coin (50-50) has the highest variability in outcomes Extremes: When pp p approaches 0 or 1, outcomes become predictable (low variance)"
  },
  {
    "objectID": "files/worksheets/worksheet4_sln.html#comparative-examples-poisson-vs-binomial---choosing-the-right-distribution",
    "href": "files/worksheets/worksheet4_sln.html#comparative-examples-poisson-vs-binomial---choosing-the-right-distribution",
    "title": "PSTAT 5A Practice Worksheet 4 - SOLUTIONS",
    "section": "Comparative Examples: Poisson vs Binomial - Choosing the Right Distribution",
    "text": "Comparative Examples: Poisson vs Binomial - Choosing the Right Distribution\n\n\n\n\n                            \n                                            \n\n\nFigure 13: Poisson vs Binomial: When to Use Each\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nCommon Mistake\n\nDon’t use Poisson just because events are “rare.” The key criteria are:\nNo fixed number of trials\nEvents over time/space\nConstant rate (\\(\\lambda\\))\n\nA rare event in a fixed number of trials is still Binomial!\n\n\n\n(c) Variance Maximization in Binomial Distribution\n\nIf \\(X \\sim \\text{Binomial}(n, p)\\), under what conditions would the variance be maximized?\n\n\nSolution. For a Binomial distribution: \\(\\text{Var}(X) = np(1-p)\\)\nFor fixed \\(n\\), variance is maximized when \\(p(1−p)\\) is maximized.\nApproach:\nTaking the derivative with respect to \\(p\\):\n\\(\\frac{d}{dp} \\bigl[ p(1-p) \\bigr] =\n\\frac{d}{dp} \\bigl[ p - p^2 \\bigr] =\n1 - 2p\\)\nSetting equal to zero:\n\\(1−2p=0 \\quad \\implies \\boxed{p = 0.5}\\)\nSecond derivative \\(= −2&lt;0\\), confirming this is a maximum.\nThe variance is maximized when \\(p=0.5\\) (fair coin scenario).\n\n\n\nVisualization of Variance vs. Probability\n\n\n\n\n                            \n                                            \n\n\nFigure 14: Binomial Variance Maximization: Effect of p\n\n\n\n\n\n\nIntuitive Understanding of Variance Maximization\n\n\n\n\n                            \n                                            \n\n\nFigure 15: Why p = 0.5 Maximizes Variance: Mathematical Intuition\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKey Insights\nMaximum: \\(p=0.5\\) maximizes \\(p(1−p)\\) for any fixed \\(n\\)\nIntuitive Explanation: Maximum uncertainty occurs when success and failure are equally likely\nPractical Meaning: A fair coin (50-50) has the highest variability in outcomes\nExtremes: When \\(p\\) approaches \\(0\\) or \\(1\\), outcomes become predictable (low variance)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sec-sampling-dist",
    "href": "files/lecture_notes/lecture9/lecture9.html#sec-sampling-dist",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Big Picture: Statistical Inference",
    "text": "The Big Picture: Statistical Inference\n\n\n\n\nPopulation vs Sample\n\n\n\nPopulation: All individuals of interest\n\n\nSample: Subset we actually observe\n\n\nParameter: Population characteristic (\\(\\mu\\), \\(p\\))\n\n\nStatistic: Sample characteristic (\\(\\bar{x}\\), \\(\\hat{p}\\))\n\n\n\nGoal: Use sample statistics to estimate population parameters\n\n\n\n\n\n\n\n\n\nWhy Confidence Intervals?\n\n\n\nPoint estimates are rarely exactly correct\n\n\nInterval estimates capture uncertainty\n\n\nConfidence level quantifies our certainty\n\n\nMargin of error shows precision\n\n\n\nKey Insight: We trade precision for confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sampling-distributions",
    "href": "files/lecture_notes/lecture9/lecture9.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nA sampling distribution is the distribution of a statistic (like \\(\\bar{x}\\)) across all possible samples of size \\(n\\).\n\n\n\nKey Properties:\nCenter:\n\\(E[\\bar{X}] = \\mu\\) (unbiased)\nSpread:\n\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nShape:\nApproaches normal as \\(n\\) increases (Central Limit Theorem)\nStandard Error vs Standard Deviation:\n\n\\(\\sigma\\): spread of individual observations\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\): spread of sample means\n\n\n\n\n\n\nDrag the slider to see how sample size affects the sampling distribution"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sec-clt-sampling",
    "href": "files/lecture_notes/lecture9/lecture9.html#sec-clt-sampling",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Central Limit Theorem in Action",
    "text": "Central Limit Theorem in Action\n\n\n\n\nNew Population\n\n Uniform Population Exponential Population Bimodal Population Right-Skewed Population  Sample Size:  Collect 1000 Sample Means\n\n\n\nPopulation μ: - | Sample Means μ: - | Standard Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sec-ci-interpretation",
    "href": "files/lecture_notes/lecture9/lecture9.html#sec-ci-interpretation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Concept",
    "text": "Confidence Intervals: The Concept\n\n\nWhat is a Confidence Interval? A confidence interval provides a range of plausible values for a population parameter. 95% Confidence Interval: If we repeated our sampling process many times, about \\(95\\%\\) of the intervals we construct would contain the true population parameter.\n\n\n\n\n\nClick to generate new 95% confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sec-ci-means",
    "href": "files/lecture_notes/lecture9/lecture9.html#sec-ci-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Means",
    "text": "Confidence Intervals for Population Means\n\n\n\n\n 🎯 When σ is Known:\n\n\n\\[\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\n\n\nWhen \\(\\sigma\\) is Unknown (more common):\n\n\n\\[\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\bar{x}\\): sample mean\n\n\n\\(t^*\\): critical value (df = n-1)\n\n\n\\(\\frac{s}{\\sqrt{n}}\\): standard error\n\n\n\n\n\n\nCommon Confidence Levels:\n\n\n\n90%: z* = 1.645, more precise\n\n\n95%: z* = 1.96, most common\n\n\n99%: z* = 2.576, more confident\n\n\n\nConditions Required:\n\n\n\nRandom sampling\nNearly normal population OR n ≥ 30\nIndependent observations"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-confidence-levels",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-confidence-levels",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Confidence Levels:",
    "text": "Common Confidence Levels:"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sec-ci-proportions",
    "href": "files/lecture_notes/lecture9/lecture9.html#sec-ci-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Proportions",
    "text": "Confidence Intervals for Population Proportions\n\n\n\n 🎯 Formula:\n\n\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\hat{p} = \\frac{x}{n}\\): sample proportion\n\n\n\\(z^*\\): critical value\n\n\n\\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\): standard error\n\n\n\n\nConditions Required:\n\n\n\nRandom sampling\n\n\n\\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\)\n\n\nIndependent observations\n\n\nPopulation at least 10× sample size\n\n\n\nConservative Approach:\n\n\nUse \\(\\hat{p} = 0.5\\) for planning when true proportion unknown (maximizes margin of error)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-1-ci-for-mean",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-1-ci-for-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 1: CI for Mean",
    "text": "Practice Problem 1: CI for Mean\n\nA random sample of 25 college students shows a mean daily screen time of 6.2 hours with a standard deviation of 1.8 hours. (a) Construct a 95% confidence interval for the mean daily screen time. (b) Interpret the confidence interval in context. (c) What would happen to the interval width if we used 99% confidence instead? Show Solution\n\nSolution. (a)\nGiven: \\(n = 25\\), \\(\\bar{x} = 6.2\\), \\(s = 1.8\\), 95% confidence\nFor \\(df = 24\\), \\(t^* = 2.064\\)\n\\(SE = \\frac{s}{\\sqrt{n}} = \\frac{1.8}{\\sqrt{25}} = 0.36\\)\n\\(CI = 6.2 \\pm 2.064 \\times 0.36 = 6.2 \\pm 0.743 = (5.46, 6.94)\\) hours\n(b)\nWe are 95% confident that the true mean daily screen time for all college students is between \\(5.46\\) and \\(6.94\\) hours.\n(c)\nFor 99% confidence, we use \\(t^* = 2.797\\), giving a wider interval: \\((5.19, 7.21)\\) hours."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-2-ci-for-proportion",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-2-ci-for-proportion",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 2: CI for Proportion",
    "text": "Practice Problem 2: CI for Proportion\n\nIn a survey of 400 voters, 240 support a particular candidate. (a) Construct a 90% confidence interval for the true proportion of supporters. (b) Check if the conditions for inference are met. (c) How large a sample would be needed for a margin of error of 0.03 with 95% confidence? Show Solution\n\nSolution. (a)\n\\(\\hat{p} = \\frac{240}{400} = 0.6\\), \\(n = 400\\), 90% confidence, \\(z^* = 1.645\\)\n\\(SE = \\sqrt{\\frac{0.6 \\times 0.4}{400}} = \\sqrt{\\frac{0.24}{400}} = 0.0245\\)\n\\(CI = 0.6 \\pm 1.645 \\times 0.0245 = 0.6 \\pm 0.0403 = (0.560, 0.640)\\)\n(b)\nCheck conditions: \\(n\\hat{p} = 400 \\times 0.6 = 240 \\geq 10\\) ✓\n\\(n(1-\\hat{p}) = 400 \\times 0.4 = 160 \\geq 10\\) ✓\n(c)\nSample size calculation:\n\\(n = \\frac{(z^*)^2 \\hat{p}(1-\\hat{p})}{ME^2} =\n\\frac{(1.96)^2 \\times 0.6 \\times 0.4}{(0.03)^2} =\n\\frac{0.9216}{0.0009} = 1024\\) people"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#practice-problem-3-sample-size-planning",
    "href": "files/lecture_notes/lecture9/lecture9.html#practice-problem-3-sample-size-planning",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 3: Sample Size Planning",
    "text": "Practice Problem 3: Sample Size Planning\n\nA market researcher wants to estimate the average amount spent on coffee per week by college students. (a) How large a sample is needed for a 95% CI with margin of error $2 if \\(\\sigma\\) = $8? (b) If the budget only allows for 100 students, what confidence level gives a $2 margin of error? (c) What’s the trade-off between sample size, confidence level, and precision?\n\nShow Solution\n\n\nSolution. (a)\nFor means:\n\\(n = \\frac{(z^*)^2 \\sigma^2}{ME^2} =\n\\frac{(1.96)^2 \\times 8^2}{2^2} =\n\\frac{245.86}{4} = 62\\) students\n(b)\nWith \\(n = 100\\):\n\\(ME = z^* \\frac{\\sigma}{\\sqrt{n}} =\nz^* \\frac{8}{\\sqrt{100}} =\n0.8 z^*\\)\nFor \\(ME = 2\\):\n\\(z^* = \\frac{2}{0.8} = 2.5\\),\nwhich corresponds to about 98.8% confidence\n(c) Trade-offs:\n\nHigher confidence \\(\\rightarrow\\) wider intervals (less precision)\nLarger sample \\(\\rightarrow\\) narrower intervals (more precision)\nLower margin of error \\(\\rightarrow\\) need larger sample or lower confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-mistakes-and-misconceptions",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-mistakes-and-misconceptions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes and Misconceptions",
    "text": "Common Mistakes and Misconceptions\n\n\nInterpretation Errors\n❌ Wrong: “\\(95\\%\\) of the data falls in this interval”\n✅ Right: “We’re \\(95\\%\\) confident the parameter is in this interval”\n❌ Wrong: “There’s a \\(95\\%\\) chance \\(\\mu\\) is in this interval”\n✅ Right: “\\(95\\%\\) of such intervals contain \\(\\mu\\)”\n\nTechnical Errors\n\nUsing \\(z*\\) when σ is unknown and \\(n &lt; 30\\)\nForgetting to check conditions\nConfusing standard error with standard deviation\nUsing wrong degrees of freedom for t-distribution\n\n\nRemember: The confidence level refers to the long-run proportion of intervals that capture the parameter!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sample-size-and-margin-of-error-relationships",
    "href": "files/lecture_notes/lecture9/lecture9.html#sample-size-and-margin-of-error-relationships",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size and Margin of Error Relationships",
    "text": "Sample Size and Margin of Error Relationships\n\n\nPopulation σ:  Confidence Level:  90% 95% 99%   Desired Margin of Error: \n\n\n\n\n\n\nSample Size vs Margin of Error\n\n\n\n\nRequired Sample Size: - | Resulting ME: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture9/lecture9.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Types of Sampling Methods",
    "text": "Types of Sampling Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nSimple Random\nEvery individual has equal chance\nUnbiased, simple\nMay not represent subgroups\n\n\nStratified\nSample from each subgroup\nEnsures representation\nMore complex\n\n\nCluster\nSample entire groups\nCost-effective for spread populations\nHigher variability\n\n\nSystematic\nEvery k-th individual\nSimple to implement\nCan miss patterns\n\n\nConvenience\nEasily accessible individuals\nQuick and cheap\nHighly biased\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nSampling Method Matters: Only probability sampling methods allow for valid statistical inference!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#confidence-intervals-in-practice",
    "href": "files/lecture_notes/lecture9/lecture9.html#confidence-intervals-in-practice",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals in Practice",
    "text": "Confidence Intervals in Practice\n\n\n\nWhen to Use Each Type\nMeans: Continuous data (height, income, test scores)\nProportions: Categorical data (yes/no, success/failure)\nChoosing Confidence Level\n\n90%: Quick estimates, less critical decisions\n95%: Standard in most research\n99%: High-stakes decisions, medical trials\n\n\nReal-World Applications\n\nPolitical polls: Proportion confidence intervals\nQuality control: Mean confidence intervals\nMedical research: Both types with high confidence\nBusiness analytics: Varies by decision importance\n\nCommunication Tips\n\nAlways include the confidence level\nState what the interval estimates\nAcknowledge the uncertainty\nConsider practical significance"
  },
  {
    "objectID": "files/lecture_notes/lecture8/lecture8.html#back-to-main-page",
    "href": "files/lecture_notes/lecture8/lecture8.html#back-to-main-page",
    "title": "PSTAT 5A: Continuous Random Variables",
    "section": "Back to Main Page",
    "text": "Back to Main Page\n🏠 Back to Main Page"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-in-action",
    "href": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-in-action",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Central Limit Theorem in Action",
    "text": "Central Limit Theorem in Action\n\nInteractive CLT Demo: Sample Means\n\n\nNew Population\n\n Uniform Population Exponential Population Bimodal Population Right-Skewed Population  Sample Size:  Collect 1000 Sample Means\n\n\n\nPopulation μ: - | Sample Means μ: - | Standard Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#key-takeaways",
    "href": "files/lecture_notes/lecture9/lecture9.html#key-takeaways",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nRandom variables translate random outcomes into numbers\nDiscrete variables have countable values; continuous variables have uncountable values\nDistributions describe the probability patterns of random variables\nChoosing the right distribution depends on understanding your data’s nature\nReal applications exist in every field - think about your research!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#resources",
    "href": "files/lecture_notes/lecture9/lecture9.html#resources",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Resources",
    "text": "Resources\n\n  \n    \n      \n      Read OpenIntro Statistics Chapter 5 sections 5.1-5.3\n    \n    \n      \n      Khan Academy - Confidence Intervals\n    \n    \n      \n      Seeing Theory - Frequentist Inference\n    \n    \n      \n      Confidence Intervals - Wikipedia\n    \n    \n      \n      Understanding Different Types of Intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#interactive-ci-demo-confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture9/lecture9.html#interactive-ci-demo-confidence-intervals-for-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Means",
    "text": "Interactive CI Demo: Confidence Intervals for Means\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population μ:  Population σ:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures μ? - | Margin of Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture9/lecture9.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Proportions",
    "text": "Interactive CI Demo: Confidence Intervals for Proportions\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population p:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures p? - | Sample Proportion: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#what-well-learn-today",
    "href": "files/lecture_notes/lecture9/lec9.html#what-well-learn-today",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "What We’ll Learn Today 🎯",
    "text": "What We’ll Learn Today 🎯\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#the-big-picture-from-sample-to-population",
    "href": "files/lecture_notes/lecture9/lec9.html#the-big-picture-from-sample-to-population",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " The Big Picture: From Sample to Population  ",
    "text": "The Big Picture: From Sample to Population  \n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#why-point-estimates-arent-enough",
    "href": "files/lecture_notes/lecture9/lec9.html#why-point-estimates-arent-enough",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Why Point Estimates Aren’t Enough  ",
    "text": "Why Point Estimates Aren’t Enough  \n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\nThe Solution: Use confidence intervals to show the range of reasonable values."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#understanding-sampling-distributions",
    "href": "files/lecture_notes/lecture9/lec9.html#understanding-sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Understanding Sampling Distributions",
    "text": "Understanding Sampling Distributions\n\n\nSimple Analogy: Think of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#the-central-limit-theorem-clt",
    "href": "files/lecture_notes/lecture9/lec9.html#the-central-limit-theorem-clt",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "  The Central Limit Theorem (CLT) 🎯  ",
    "text": "The Central Limit Theorem (CLT) 🎯  \n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#standard-error-the-key-to-everything",
    "href": "files/lecture_notes/lecture9/lec9.html#standard-error-the-key-to-everything",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Standard Error: The Key to Everything  ",
    "text": "Standard Error: The Key to Everything  \n\n\n\nStandard Deviation (σ): How spread out individual people are\nStandard Error (SE): How spread out sample averages are\n\nThe Formula: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nWhere:\n\n\\(\\sigma\\) = population standard deviation (how varied people are)\n\\(n\\) = sample size (how many people we measured)\n\\(\\sqrt{n}\\) = square root of sample size\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#confidence-intervals-the-intuitive-idea",
    "href": "files/lecture_notes/lecture9/lec9.html#confidence-intervals-the-intuitive-idea",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Confidence Intervals: The Intuitive Idea  ",
    "text": "Confidence Intervals: The Intuitive Idea  \n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re 95% confident the true average height is between 64.3 and 70.1 inches”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#building-confidence-intervals-step-by-step",
    "href": "files/lecture_notes/lecture9/lec9.html#building-confidence-intervals-step-by-step",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Building Confidence Intervals Step-by-Step  ",
    "text": "Building Confidence Intervals Step-by-Step  \n\n\nFor Population Means (Most Common Case)\nWhen we DON’T know the population standard deviation (σ):\n\n\n                            \n                                            \n\n\n\nThe Formula: \\(\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\)\nBreaking it down: - \\(\\bar{x}\\) = our sample average (the center of our guess) - \\(t^*\\) = critical value (how many standard errors to go out) - \\(\\frac{s}{\\sqrt{n}}\\) = standard error (our uncertainty measure)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#the-t-distribution-when-σ-is-unknown",
    "href": "files/lecture_notes/lecture9/lec9.html#the-t-distribution-when-σ-is-unknown",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The t-Distribution: When σ is Unknown",
    "text": "The t-Distribution: When σ is Unknown\n\n\nWhy not use the normal distribution? Because when we estimate σ with s, we add extra uncertainty!\n\n\n                            \n                                            \n\n\n\nKey Points: - Small samples (n &lt; 30): Use t-distribution - Large samples (n ≥ 30): t ≈ normal - Degrees of freedom = n - 1"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture9/lec9.html#confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\n\nFor Yes/No questions like: “What percentage of students prefer online classes?”\n\n\n                            \n                                            \n\n\n\nThe Formula: \\(\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nResults: Sample: 60% prefer online (120/200)\n95% CI: (53.2%, 66.8%) - We’re 95% confident the true percentage is in this range."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#what-does-95-confident-really-mean",
    "href": "files/lecture_notes/lecture9/lec9.html#what-does-95-confident-really-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "What Does “95% Confident” Really Mean? 🤔",
    "text": "What Does “95% Confident” Really Mean? 🤔\n\n\nThe Biggest Misconception: “There’s a 95% chance the true mean is in our interval”\nActually: “If we repeated this study 100 times, about 95 of our intervals would contain the true mean”\n\n\n                            \n                                            \n\n\n\nRemember: The interval either contains the true value or it doesn’t - there’s no probability involved for a single interval!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#sample-size-planning-getting-the-precision-you-want",
    "href": "files/lecture_notes/lecture9/lec9.html#sample-size-planning-getting-the-precision-you-want",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size Planning: Getting the Precision You Want",
    "text": "Sample Size Planning: Getting the Precision You Want\n\n\nThe Question: “How many people do we need to survey?”\n\n\n                            \n                                            \n\n\n\nKey Formula for Means: \\(n = \\left(\\frac{z^* \\sigma}{ME}\\right)^2\\)\nTrade-offs: - Want smaller margin of error? Need bigger sample - Want higher confidence? Need bigger sample\n- Want to save money? Accept wider intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#real-example-student-sleep-study",
    "href": "files/lecture_notes/lecture9/lec9.html#real-example-student-sleep-study",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Real Example: Student Sleep Study 😴",
    "text": "Real Example: Student Sleep Study 😴\n\n\nResearch Question: How many hours do UCSB students sleep per night?\n\n\n                            \n                                            \n\n\n\nBottom Line: We’re 95% confident that UCSB students sleep between 6.73 and 7.47 hours per night on average."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture9/lec9.html#common-mistakes-to-avoid",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes to Avoid ⚠️",
    "text": "Common Mistakes to Avoid ⚠️\n\n\n❌ Wrong Interpretations\n“95% of students sleep in this range” - NO! This is about the population mean, not individual students\n“There’s a 95% chance μ is in our interval” - NO! μ is fixed; our interval varies\n“We can be 95% certain” - NO! Use “confident” not “certain”\n\n✅ Correct Approach\n“We are 95% confident the population mean is in this interval”\nKey Reminders: - Check conditions before using formulas - Use t-distribution when σ is unknown - Larger samples give narrower intervals - Higher confidence gives wider intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#practice-problems",
    "href": "files/lecture_notes/lecture9/lec9.html#practice-problems",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problems 📝",
    "text": "Practice Problems 📝\n\n\nProblem 1: Coffee Shop Revenue\nA coffee shop owner samples 36 days and finds average daily revenue of $850 with standard deviation $120.\nYour turn: Calculate a 90% confidence interval for the true average daily revenue.\n\n\n\n\n\n\nSolution\n\n\nGiven: n = 36, x̄ = $850, s = $120, 90% confidence\nStep 1: Check conditions ✓ (large sample)\nStep 2: Find critical value\ndf = 35, t* = 1.690 (90% confidence)\nStep 3: Calculate standard error\nSE = 120/√36 = 120/6 = $20\nStep 4: Build interval\nCI = 850 ± 1.690 × 20 = 850 ± 33.8 = ($816.20, $883.80)\nInterpretation: We are 90% confident the true average daily revenue is between $816.20 and $883.80.\n\n\n\n\nProblem 2: Student Survey\nIn a survey of 400 students, 280 say they would recommend their major to a friend.\nYour turn: 1. Calculate the sample proportion 2. Build a 95% confidence interval\n3. Check if conditions are met\n\n\n\n\n\n\nSolution\n\n\nStep 1: Sample proportion\np̂ = 280/400 = 0.70 (70%)\nStep 2: Check conditions\nnp̂ = 400(0.70) = 280 ≥ 10 ✓\nn(1-p̂) = 400(0.30) = 120 ≥ 10 ✓\nStep 3: Build CI\nSE = √[0.70(0.30)/400] = √[0.000525] = 0.0229\nz* = 1.96 (95% confidence)\nCI = 0.70 ± 1.96(0.0229) = 0.70 ± 0.045 = (0.655, 0.745)\nInterpretation: We are 95% confident that between 65.5% and 74.5% of all students would recommend their major."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#looking-ahead-hypothesis-testing",
    "href": "files/lecture_notes/lecture9/lec9.html#looking-ahead-hypothesis-testing",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead: Hypothesis Testing 🔮",
    "text": "Looking Ahead: Hypothesis Testing 🔮\n\n\nNext week we’ll learn: - How to test specific claims about populations - When to reject or fail to reject hypotheses\n- The connection between confidence intervals and hypothesis tests"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#key-takeaways",
    "href": "files/lecture_notes/lecture9/lec9.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Key Takeaways 🎯",
    "text": "Key Takeaways 🎯\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#questions",
    "href": "files/lecture_notes/lecture9/lec9.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions? 🤔",
    "text": "Questions? 🤔\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#sampling-distributions",
    "href": "files/lecture_notes/lecture9/lec9.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Sampling Distributions  ",
    "text": "Sampling Distributions  \n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#from-observation-to-experimentation-the-foundation-of-statistical-inference",
    "href": "files/lecture_notes/lecture9/lec9.html#from-observation-to-experimentation-the-foundation-of-statistical-inference",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "From Observation to Experimentation: The Foundation of Statistical Inference”",
    "text": "From Observation to Experimentation: The Foundation of Statistical Inference”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#from-observation-to-experimentation",
    "href": "files/lecture_notes/lecture9/lec9.html#from-observation-to-experimentation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "  From Observation to Experimentation ",
    "text": "From Observation to Experimentation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lec9.html#standard-error-the-key-to-everything-1",
    "href": "files/lecture_notes/lecture9/lec9.html#standard-error-the-key-to-everything-1",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": " Standard Error: The Key to Everything  ",
    "text": "Standard Error: The Key to Everything  \n\n\nThink of it like this:\n\nStandard Deviation (σ): How spread out individual people are\nStandard Error (SE): How spread out sample averages are\n\nThe Formula: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nWhere:\n\n\\(\\sigma\\) = population standard deviation (how varied people are)\n\\(n\\) = sample size (how many people we measured)\n\\(\\sqrt{n}\\) = square root of sample size (the magic that makes averages more precise)\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#todays-learning-objectives",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this session, you will be able to:\n\nDefine what a random variable is\nDistinguish between different types of random variables\nIdentify examples of random variables in your field of study\nConnect probability concepts to real-world applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#what-is-a-random-variable",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#what-is-a-random-variable",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\n\n\nA random variable (r.v.) is a function that assigns numerical values to the outcomes of a random experiment\nNotation: Usually denoted by capital letters (X, Y, Z)\nIt’s a bridge between the sample space and real numbers\nThink of it as a “rule” that translates outcomes into numbers\n\n\n\nKey Point: It’s not actually “random”, it’s a deterministic function applied to random outcomes!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#real-world-connection",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#real-world-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Real-World Connection",
    "text": "Real-World Connection"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#activity-your-research-field",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#activity-your-research-field",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Activity: Your Research Field",
    "text": "Activity: Your Research Field\n\n\n\n\n\n\n\n\nThink About Your Major/Research Area\n\n\nTake 2 minutes to brainstorm:\n\nWhat random phenomena occur in your field?\nHow might you assign numbers to these outcomes?\nWhat questions could you answer with this data?\n\n\n\n\n\n\n\n\n\n\n\n\nExamples by Field\n\n\n\nPsychology: Reaction times, survey responses\nBiology: Species counts, gene expression levels\nEconomics: Stock prices, unemployment rates\nEngineering: System failures, signal strength"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#types-of-random-variables",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#types-of-random-variables",
    "title": "Random Variables: Review of Concepts and Applications",
    "section": "Types of Random Variables",
    "text": "Types of Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#discrete-random-variables",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#discrete-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\nDefinition: Takes on countable values (finite or countably infinite)\nExamples:\n\nNumber of emails received per day\nNumber of defective products in a batch\nStudent enrollment in courses\nNumber of research papers published per year\n\n\n\n\nNote: If X is discrete, then X can take values \\(x_1, x_2, x_3, \\cdot\\) where we can list all possible values."
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#continuous-random-variables",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#continuous-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\nDefinition: Takes on uncountably infinite values (any value in an interval)\nExamples:\n\nHeight of students\nTime until equipment failure\nTemperature measurements\nGPA (technically discrete, but often treated as continuous)\n\n\n\n\nNote: If \\(X\\) is continuous, then \\(X\\) can take any value in an interval \\([a,b]\\) or \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#sampling-and-random-variables",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#sampling-and-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Sampling and Random Variables",
    "text": "Sampling and Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#the-central-limit-theorem-connection",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#the-central-limit-theorem-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "The Central Limit Theorem Connection",
    "text": "The Central Limit Theorem Connection\n\n\n\n\nWhen we repeatedly sample from a population, the sample mean becomes a random variable\nFormula: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)\nEach time we sample, we get a different \\(\\bar{X}\\)\nThe distribution of \\(\\bar{X}\\) has special properties!\n\n\n\n\n\nCenter (Unbiased): \\(E[\\bar{X}] = \\mu\\).\nSpread Shrinks with \\(n\\): \\(\\mathrm{Var}(\\bar{X}) = \\sigma^2/n\\); \\(\\mathrm{SE}(\\bar{X}) = \\sigma/\\sqrt{n}\\) (estimate with \\(s/\\sqrt{n}\\)).\nShape:\n\nIf the population is Normal, then \\(\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma^2/n)\\) exactly.\n\nOtherwise, CLT: for large \\(n\\), \\(\\bar{X}\\) is approximately Normal even when the data aren’t.\n\nConsistency / Law of Large Numbers: \\(\\bar{X} \\xrightarrow{P} \\mu\\) as \\(n \\to \\infty\\) (estimates get closer to the truth with more data).\n(If sampling w/out replacement, pop size \\(N\\)): Apply finite population correction (FPC):\n\\(\\mathrm{SE}(\\bar{X}) = \\dfrac{\\sigma}{\\sqrt{n}}\\sqrt{\\dfrac{N-n}{N-1}}\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#common-discrete-distributions",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#common-discrete-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBinomial Distribution - Characteristics\n\n\n\n\nFixed number of trials (n)\nEach trial has two outcomes\nConstant probability of success\nTrials are independent\n\nExample: Number of successful research grants out of 10 applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#binomial-distribution",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#binomial-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\nCharacteristics: - Fixed number of trials (n) - Each trial has two outcomes - Constant probability of success - Trials are independent\nExample: Number of successful research grants out of 10 applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#poisson-distribution",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#poisson-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\n\nCharacteristics:\n\nModels rare events\nEvents occur independently\nConstant average rate\nUseful for counts over time/space\n\nExample: Number of emails received per hour, number of mutations in DNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#common-continuous-distributions",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#common-continuous-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\nNormal Distribution - Characteristics\n\n\n\nBell-shaped curve\nSymmetric around mean\nParameters: \\(\\mu\\) (mean), \\(\\sigma\\) (standard deviation)\nMany natural phenomena follow this pattern\n\nExample: Heights, test scores, measurement errors"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#normal-distribution",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#normal-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\nCharacteristics: - Bell-shaped curve - Symmetric around mean - Parameters: μ (mean), σ (standard deviation) - Many natural phenomena follow this pattern\nExample: Heights, test scores, measurement errors"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#exponential-distribution",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#exponential-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\n\nCharacteristics:\n\nModels waiting times\nMemoryless property\nParameter: \\(\\lambda\\) (rate)\nRight-skewed\n\nExample: Time between arrivals, equipment lifespan, time to next earthquake"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#interactive-activity-choose-your-distribution",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#interactive-activity-choose-your-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Interactive Activity: Choose Your Distribution",
    "text": "Interactive Activity: Choose Your Distribution\n\n\n\n\n\n\nGroup Discussion (5 minutes)\n\n\nFor each scenario, identify: 1. Is the random variable discrete or continuous? 2. What distribution might it follow? 3. What are the parameters?\nScenarios: - Number of students attending office hours per week - Time spent studying for an exam - Number of typos in a research paper - Body temperature of patients in a hospital"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#application-research-design",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#application-research-design",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Application: Research Design",
    "text": "Application: Research Design\n\nConsider your research question:\n\nIdentify your random variable(s)\n\nWhat are you measuring?\nWhat values can it take?\n\nChoose appropriate distribution\n\nBased on the nature of your data\nConsider the underlying process\n\nPlan your analysis\n\nHow will you collect data?\nWhat statistical tests are appropriate?"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#probability-mass-vs.-density-functions",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#probability-mass-vs.-density-functions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Probability Mass vs. Density Functions",
    "text": "Probability Mass vs. Density Functions\n\n\nDiscrete: Probability Mass Function (PMF)\n\n\\(P(X = x)\\) for specific values\nSums to 1 over all possible values\nCan find exact probabilities\n\nExample: \\(P(X = 3) = 0.2\\)\n\nContinuous: Probability Density Function (PDF)\n\n\\(f(x)\\) represents density\nArea under curve = 1\n\\(P(X = x) = 0\\) for any specific value\nFind probabilities over intervals\n\nExample: \\(P(a &lt; X &lt; b) =  \\int_{a}^{b} f(x)dx\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#comparing-distributions-side-by-side",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#comparing-distributions-side-by-side",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Comparing Distributions Side-by-Side",
    "text": "Comparing Distributions Side-by-Side"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#key-takeaways",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#key-takeaways",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nRandom variables translate random outcomes into numbers\nDiscrete variables have countable values; continuous variables have uncountable values\nDistributions describe the probability patterns of random variables\nChoosing the right distribution depends on understanding your data’s nature\nReal applications exist in every field - think about your research!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#next-steps",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#next-steps",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nFor Your Research/Interests\n\n\n\nIdentify random variables in your field\nThink about appropriate distributions\nConsider data collection methods\nPlan statistical analyses\nConnect theory to practice"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#questions-and-discussion",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#questions-and-discussion",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Questions and Discussion",
    "text": "Questions and Discussion\n\nShare with the class:\n\nWhat random variables are important in your field of study/major?\nWhich distributions might be most relevant?\nWhat challenges do you anticipate in data collection?\n\n\n\nThank you for your participation!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#appendix-python-code-examples",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#appendix-python-code-examples",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Appendix: Python Code Examples",
    "text": "Appendix: Python Code Examples\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n\n# Generate random samples from different distributions\n\n# Binomial\nbinom_data = np.random.binomial(n=10, p=0.3, size=100)\n\n# Poisson  \npoisson_data = np.random.poisson(lam=3, size=100)\n\n# Normal\nnormal_data = np.random.normal(loc=0, scale=1, size=100)\n\n# Exponential\nexp_data = np.random.exponential(scale=1/1.5, size=100)\n\n# Create histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0,0].hist(binom_data, bins=11, alpha=0.7, color='steelblue')\naxes[0,0].set_title('Binomial Sample')\n\naxes[0,1].hist(poisson_data, bins=15, alpha=0.7, color='coral')\naxes[0,1].set_title('Poisson Sample')\n\naxes[1,0].hist(normal_data, bins=20, alpha=0.7, color='lightblue')\naxes[1,0].set_title('Normal Sample')\n\naxes[1,1].hist(exp_data, bins=20, alpha=0.7, color='lightgreen')\naxes[1,1].set_title('Exponential Sample')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#additional-resources",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#additional-resources",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n# Useful Python libraries for statistics and probability\nimport numpy as np           # Numerical computing\nimport scipy.stats as stats  # Statistical functions\nimport matplotlib.pyplot as plt  # Plotting\nimport seaborn as sns        # Statistical visualization\nimport pandas as pd          # Data manipulation\n\n# Quick reference for common distributions:\n# stats.binom.pmf(k, n, p)     # Binomial PMF\n# stats.poisson.pmf(k, lam)    # Poisson PMF  \n# stats.norm.pdf(x, mu, sigma) # Normal PDF\n# stats.expon.pdf(x, scale)    # Exponential PDF\n\n# Generate random samples:\n# np.random.binomial(n, p, size)\n# np.random.poisson(lam, size)\n# np.random.normal(mu, sigma, size)\n# np.random.exponential(scale, size)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#clt-connection",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#clt-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "CLT Connection",
    "text": "CLT Connection\n\n\n\n\nWhen we repeatedly sample from a population, the sample mean becomes a random variable\nFormula: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)\nEach time we sample, we get a different \\(\\bar{X}\\)\nThe distribution of \\(\\bar{X}\\) has special properties!\n\n\n\n\n\nCenter (Unbiased): \\(E[\\bar{X}] = \\mu\\).\nSpread Shrinks with \\(n\\): \\(\\mathrm{Var}(\\bar{X}) = \\sigma^2/n\\); \\(\\mathrm{SE}(\\bar{X}) = \\sigma/\\sqrt{n}\\) (estimate with \\(s/\\sqrt{n}\\)).\nShape:\n\nIf the population is Normal, then \\(\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma^2/n)\\) exactly.\n\nOtherwise, CLT: for large \\(n\\), \\(\\bar{X}\\) is approximately Normal even when the data aren’t.\n\nConsistency / Law of Large Numbers: \\(\\bar{X} \\xrightarrow{P} \\mu\\) as \\(n \\to \\infty\\) (estimates get closer to the truth with more data).\n(If sampling w/out replacement, pop size \\(N\\)): Apply finite population correction (FPC):\n\\(\\mathrm{SE}(\\bar{X}) = \\dfrac{\\sigma}{\\sqrt{n}}\\sqrt{\\dfrac{N-n}{N-1}}\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#central-limit-theorem-connection",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#central-limit-theorem-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\nWhen we repeatedly sample from a population, the sample mean becomes a random variable\nFormula: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)\nEach time we sample, we get a different \\(\\bar{X}\\)\nThe distribution of \\(\\bar{X}\\) has special properties!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#central-limit-theorem-connection-1",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#central-limit-theorem-connection-1",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\n\nThese properties are :\n\nCenter (Unbiased): \\(E[\\bar{X}] = \\mu\\).\nSpread Shrinks with \\(n\\): \\(\\mathrm{Var}(\\bar{X}) = \\sigma^2/n\\); \\(\\mathrm{SE}(\\bar{X}) = \\sigma/\\sqrt{n}\\) (estimate with \\(s/\\sqrt{n}\\)).\nShape:\n\nIf the population is Normal, then \\(\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma^2/n)\\) exactly.\n\nOtherwise, CLT: for large \\(n\\), \\(\\bar{X}\\) is approximately Normal even when the data aren’t.\n\n\n\n\n\n\nConsistency / Law of Large Numbers: \\(\\bar{X} \\xrightarrow{P} \\mu\\) as \\(n \\to \\infty\\) (estimates get closer to the truth with more data).\n(If sampling w/out replacement, pop size \\(N\\)): Apply finite population correction (FPC):\n\\(\\mathrm{SE}(\\bar{X}) = \\dfrac{\\sigma}{\\sqrt{n}}\\sqrt{\\dfrac{N-n}{N-1}}\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#poisson-distribution---characteristics",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#poisson-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Poisson Distribution - Characteristics",
    "text": "Poisson Distribution - Characteristics\n\n\n\nModels rare events\nEvents occur independently\nConstant average rate\nUseful for counts over time/space\n\nExample: Number of emails received per hour, number of mutations in DNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#exponential-distribution---characteristics",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#exponential-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Exponential Distribution - Characteristics",
    "text": "Exponential Distribution - Characteristics\n\n\n\nModels waiting times\nMemoryless property\nParameter: \\(\\lambda\\) (rate)\nRight-skewed\n\nExample: Time between arrivals, equipment lifespan, time to next earthquake"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#confidence-intervals-for-means",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Intervals for Means",
    "text": "Confidence Intervals for Means\n\n\nProblem: We have one sample mean, but want to estimate the population mean\nSolution: Use the sampling distribution to create a confidence interval\nKey Insight: If we know how \\(\\bar{X}\\) varies, we can make probabilistic statements about μ\n\n\n\n95% Confidence Interval Formula: \\(\\bar{x} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nInterpretation: “We are 95% confident that the true population mean lies within this interval”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#visualizing-confidence-intervals",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#visualizing-confidence-intervals",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#confidence-interval-interpretation",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#confidence-interval-interpretation",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Interval Interpretation",
    "text": "Confidence Interval Interpretation\n\n\n\n\n\n\nCommon Misconceptions\n\n\n❌ WRONG: “There’s a 95% probability that μ is in this specific interval”\n✅ CORRECT: “If we repeated this process many times, 95% of the intervals we construct would contain the true μ”\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe interval is random, not the population parameter\nBefore collecting data: 95% chance our method will work\nAfter collecting data: The interval either contains μ or it doesn’t\nConfidence level = Long-run success rate of the method"
  },
  {
    "objectID": "files/lecture_notes/lecture9/inclass_activity.html#factors-affecting-confidence-interval-width",
    "href": "files/lecture_notes/lecture9/inclass_activity.html#factors-affecting-confidence-interval-width",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Factors Affecting Confidence Interval Width",
    "text": "Factors Affecting Confidence Interval Width"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html",
    "href": "files/lecture_notes/lecture10/lecture10.html",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "From Samples to Populations: Understanding Uncertainty\n“In statistics, we make educated guesses about the whole by carefully studying a part”\n\n\n\n\n\n\n\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: 7 AM – 12 AM\n- ⏳ Duration: 1 hour once started\nWhere: 💻 Online via Canvas\nCovers: Material from Weeks 3-4\n\n\n\n\n\nDiscrete & continuous distributions\nProbability calculations\nExpected value & variance\nNormal distribution applications\nNote: Upload photos of written work for calculation problems\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy sampling? The power and necessity of statistical inference\nSample behavior - How sample means form predictable patterns\nUncertainty quantification - From point estimates to intervals\nThe CLT magic - Why normal distributions appear everywhere\nConfidence intervals - Our bridge from samples to populations\n\n\n\n\n\n\nDesign effective sampling strategies\nCalculate and interpret standard errors\nApply the Central Limit Theorem\nConstruct and interpret confidence intervals\nChoose appropriate sample sizes for desired precision\nRecognize and avoid sampling bias\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation vs. Sample Realities:\n\nTime: Surveying 40,000 UCSB students takes months\nCost: Each measurement costs money and resources\nLogistics: Some populations are impossible to reach entirely\nFeasibility: Testing every light bulb would destroy the product\n\n\n\n\nUse a representative sample to make valid inferences about the entire population\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n🔍 Observational Studies: - Observe what naturally occurs - Good for identifying associations - ⚠️ Cannot establish causation due to confounding\n\n🧪 Randomized Experiments: - Actively assign treatments randomly - Controls for confounding variables - ✅ Can establish causal relationships\n\n\n\n\n\n\n\n\n\n\n1. Simple Random Sampling (SRS)\nEvery individual has equal chance of selection\nGold standard for inference\n2. Stratified Sampling\nDivide population into groups (strata)\nSample randomly within each group\nEnsures representation of subgroups\n\n\n3. Cluster Sampling\nDivide into clusters, randomly select clusters\nSample all/some individuals within chosen clusters\nCost-effective for large populations\n4. Systematic Sampling\nSelect every \\(kth\\) individual from ordered list\nSimple but can introduce bias if pattern exists\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\n\nSelection Bias - Systematic exclusion of certain groups - Example: Online surveys miss non-internet users\nResponse Bias\n- Who chooses to respond affects results - Example: Satisfaction surveys - unhappy customers more likely to respond\nNonresponse Bias - Missing data isn’t random - Example: Wealthy people less likely to disclose income\nConvenience Sampling - Sampling whoever is easiest to reach - Example: Surveying only students in your dorm\n\n\n\n\n                            \n                                            \n\n\n💡 Key Insight: Bias can’t be fixed by increasing sample size!\n\n\n\n\n\n\n\n\n🎲 The Setup:\n\nTake many samples from the same population\nCalculate the mean of each sample\nPlot all these sample means\nObserve the magic!\n\n🎯 What We Discover:\n\nSample means cluster around the true population mean\nThey form a predictable pattern (normal distribution!)\nLarger samples give more consistent results\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\nFor a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), when sample size \\(n\\) is large enough:\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nOr equivalently: \\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\n\n\n\n\nRule of Thumb: \\(n \\geq 30\\) usually works\nShape doesn’t matter: Works for ANY population distribution\n\nLarger \\(n\\) = Better approximation\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\nStandard Error (SE) measures how much sample means vary from sample to sample.\n\\[SE = \\frac{\\sigma}{\\sqrt{n}} \\text{ (when } \\sigma \\text{ is known)}\\]\n\\[SE = \\frac{s}{\\sqrt{n}} \\text{ (usual case, } \\sigma \\text{ unknown)}\\]\n\n\n\n\nSmaller SE = More precise estimates\nSE decreases as sample size increases\nRate of decrease: \\(SE \\propto 1/\\sqrt{n}\\)\n4× larger sample = ½ the uncertainty!\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\nA confidence interval gives us a range of plausible values for the population parameter.\nFor a population mean: \\[\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\n\n\n90% CI: \\(z^* = 1.645\\)\n95% CI: \\(z^* = 1.96\\)\n99% CI: \\(z^* = 2.576\\)\n\n\n\n\n“We are 95% confident that the true population mean lies between [lower bound] and [upper bound]”\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\nTo achieve margin of error \\(E\\) with confidence level \\((1-\\alpha)\\):\n\\[n = \\left(\\frac{z^*\\sigma}{E}\\right)^2\\]\n\n\n\nMargin of Error Trade-offs: - Smaller \\(E\\) requires larger \\(n\\) - Higher confidence requires larger \\(n\\)\n- More variable population requires larger \\(n\\)\nPractical Constraints: - Budget limitations - Time constraints\n- Availability of participants\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n\n\n“What is the average height of UCSB students?”\nOur Approach:\n\nPopulation: All 26,000 UCSB students\nSample: Random sample of 100 students\nMeasurement: Height in inches\nGoal: 95% confidence interval for population mean\n\nResults:\n\nSample mean: \\(\\bar{x} = 68.2\\) inches\nSample std dev: \\(s = 4.1\\) inches\nSample size: \\(n = 100\\)\n\n\n\n\n\n                            \n                                            \n\n\n🎯 Interpretation: We are 95% confident that the true average height of UCSB students is between 67.40 and 69.00 inches.\n\n\n\n\n\n\n\n\n\n\n1. Sampling Wisdom\n\nRepresentative samples beat large biased samples\nRandomization is your best friend\nBias can’t be fixed with larger samples\n\n2. The CLT Magic\n\nSample means are approximately normal (\\(n ≥ 30\\))\nWorks for ANY population distribution\nEnables powerful statistical inference\n\n3. Standard Error\n\nMeasures precision of our estimates\nDecreases with \\(\\sqrt{n}\\), not \\(n\\)\nKey ingredient in confidence intervals\n\n\n\n\n\n4. Confidence Intervals\n\nQuantify uncertainty in our estimates\nCorrect interpretation is crucial\nWidth depends on confidence level and sample size\n\n5. Sample Size Planning\n\nBalance precision needs with resources\nConsider margin of error requirements\nAccount for practical constraints\n\n6. Quality Control\n\nAlways check for potential bias\nVerify assumptions (normality, independence)\nConsider the broader context\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\n\n1. Sample Size Question: If we want to halve our margin of error, by what factor should we increase our sample size?\n2. CLT Application:\nA population has a right-skewed distribution. What can we say about the distribution of sample means when n = 50?\n3. CI Interpretation: We calculated a 95% CI as (45, 55). What does this mean?\n4. Bias Detection: An online survey about internet usage gets 10,000 responses. What type of bias might be present?\n\n\n\n\n1. Increase by factor of 4 (since \\(SE \\propto \\frac{1}{\\sqrt{n}}\\))\n2. Sample means will be approximately normal regardless of population shape\n3. We’re 95% confident the true population parameter is between 45 and 55\n4. Selection bias - excludes people without internet access\n\n\n\n\n\n\n\n\n\n\n\n\nOpenIntro Statistics\n\nSection 1.3: Sampling principles and strategies\nSection 3.3: Confidence intervals for a mean\nSection 4.1: Central Limit Theorem\n\n\n\n\n\n\nKhan Academy: Central Limit Theorem\nStatQuest: Confidence Intervals Explained\n3Blue1Brown: Central Limit Theorem Visualization\n\n\n\n\n\n\nSeeing Theory: Probability Visualizations\nRossman & Chance Applets: Sampling Distributions\nCentral Limit Theorem Simulator\n\n\n\n\n\nOffice Hours: Thursday 11 AM-12 PM (Zoom link on Canvas)\nEmail: nmathlouthi@ucsb.edu\n\n\n\n\nNext Lecture: Hypothesis Testing and p-values\n\n\n\n\n\n\n\n\n\n\n\n“The goal is not to eliminate uncertainty, but to understand and quantify it intelligently”\nKey Questions for Reflection: - How do we balance precision with practicality?\n\nWhen might a larger sample actually be worse?\nWhat makes a “good” confidence interval?\nHow do we communicate uncertainty to non-statisticians?\n\n\n\n\n\nComing Up: Hypothesis Testing\n\nWhat are null and alternative hypotheses?\nHow do we make decisions with data?\nWhat does a p-value really mean?\nType I and Type II errors\n\nRecommended Prep:\n\nReview today’s confidence interval concepts\nThink about yes/no questions you’d test with data\nConsider what “statistical significance” means to you"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture10/lecture10.html#todays-learning-objectives",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this session, you will be able to:\n\nDefine what a random variable is\nDistinguish between different types of random variables\nIdentify examples of random variables in your field of study\nConnect probability concepts to real-world applications"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#what-is-a-random-variable",
    "href": "files/lecture_notes/lecture10/lecture10.html#what-is-a-random-variable",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "What is a Random Variable?",
    "text": "What is a Random Variable?\n\n\nA random variable (r.v.) is a function that assigns numerical values to the outcomes of a random experiment\nNotation: Usually denoted by capital letters (X, Y, Z)\nIt’s a bridge between the sample space and real numbers\nThink of it as a “rule” that translates outcomes into numbers\n\n\n\nKey Point: It’s not actually “random”, it’s a deterministic function applied to random outcomes!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#real-world-connection",
    "href": "files/lecture_notes/lecture10/lecture10.html#real-world-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Real-World Connection",
    "text": "Real-World Connection"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#activity-your-research-field",
    "href": "files/lecture_notes/lecture10/lecture10.html#activity-your-research-field",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Activity: Your Research Field",
    "text": "Activity: Your Research Field\n\n\n\n\n\n\n\n\nThink About Your Major/Research Area\n\n\nTake 2 minutes to brainstorm:\n\nWhat random phenomena occur in your field?\nHow might you assign numbers to these outcomes?\nWhat questions could you answer with this data?\n\n\n\n\n\n\n\n\n\n\n\n\nExamples by Field\n\n\n\nPsychology: Reaction times, survey responses\nBiology: Species counts, gene expression levels\nEconomics: Stock prices, unemployment rates\nEngineering: System failures, signal strength"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#discrete-random-variables",
    "href": "files/lecture_notes/lecture10/lecture10.html#discrete-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\nDefinition: Takes on countable values (finite or countably infinite)\nExamples:\n\nNumber of emails received per day\nNumber of defective products in a batch\nStudent enrollment in courses\nNumber of research papers published per year\n\n\n\n\nNote: If X is discrete, then X can take values \\(x_1, x_2, x_3, \\cdot\\) where we can list all possible values."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#continuous-random-variables",
    "href": "files/lecture_notes/lecture10/lecture10.html#continuous-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\nDefinition: Takes on uncountably infinite values (any value in an interval)\nExamples:\n\nHeight of students\nTime until equipment failure\nTemperature measurements\nGPA (technically discrete, but often treated as continuous)\n\n\n\n\nNote: If \\(X\\) is continuous, then \\(X\\) can take any value in an interval \\([a,b]\\) or \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#sampling-and-random-variables",
    "href": "files/lecture_notes/lecture10/lecture10.html#sampling-and-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Sampling and Random Variables",
    "text": "Sampling and Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#central-limit-theorem-connection",
    "href": "files/lecture_notes/lecture10/lecture10.html#central-limit-theorem-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\nWhen we repeatedly sample from a population, the sample mean becomes a random variable\nFormula: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)\nEach time we sample, we get a different \\(\\bar{X}\\)\nThe distribution of \\(\\bar{X}\\) has special properties!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#central-limit-theorem-connection-1",
    "href": "files/lecture_notes/lecture10/lecture10.html#central-limit-theorem-connection-1",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\n\nThese properties are :\n\nCenter (Unbiased): \\(E[\\bar{X}] = \\mu\\).\nSpread Shrinks with \\(n\\): \\(\\mathrm{Var}(\\bar{X}) = \\sigma^2/n\\); \\(\\mathrm{SE}(\\bar{X}) = \\sigma/\\sqrt{n}\\) (estimate with \\(s/\\sqrt{n}\\)).\nShape:\n\nIf the population is Normal, then \\(\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma^2/n)\\) exactly.\n\nOtherwise, CLT: for large \\(n\\), \\(\\bar{X}\\) is approximately Normal even when the data aren’t.\n\n\n\n\n\n\nConsistency / Law of Large Numbers: \\(\\bar{X} \\xrightarrow{P} \\mu\\) as \\(n \\to \\infty\\) (estimates get closer to the truth with more data).\n(If sampling w/out replacement, pop size \\(N\\)): Apply finite population correction (FPC):\n\\(\\mathrm{SE}(\\bar{X}) = \\dfrac{\\sigma}{\\sqrt{n}}\\sqrt{\\dfrac{N-n}{N-1}}\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#common-discrete-distributions",
    "href": "files/lecture_notes/lecture10/lecture10.html#common-discrete-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBinomial Distribution - Characteristics\n\n\n\n\nFixed number of trials (n)\nEach trial has two outcomes\nConstant probability of success\nTrials are independent\n\nExample: Number of successful research grants out of 10 applications"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#poisson-distribution---characteristics",
    "href": "files/lecture_notes/lecture10/lecture10.html#poisson-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Poisson Distribution - Characteristics",
    "text": "Poisson Distribution - Characteristics\n\n\n\nModels rare events\nEvents occur independently\nConstant average rate\nUseful for counts over time/space\n\nExample: Number of emails received per hour, number of mutations in DNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#common-continuous-distributions",
    "href": "files/lecture_notes/lecture10/lecture10.html#common-continuous-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\nNormal Distribution - Characteristics\n\n\n\nBell-shaped curve\nSymmetric around mean\nParameters: \\(\\mu\\) (mean), \\(\\sigma\\) (standard deviation)\nMany natural phenomena follow this pattern\n\nExample: Heights, test scores, measurement errors"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#exponential-distribution---characteristics",
    "href": "files/lecture_notes/lecture10/lecture10.html#exponential-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Exponential Distribution - Characteristics",
    "text": "Exponential Distribution - Characteristics\n\n\n\nModels waiting times\nMemoryless property\nParameter: \\(\\lambda\\) (rate)\nRight-skewed\n\nExample: Time between arrivals, equipment lifespan, time to next earthquake"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#interactive-activity-choose-your-distribution",
    "href": "files/lecture_notes/lecture10/lecture10.html#interactive-activity-choose-your-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Interactive Activity: Choose Your Distribution",
    "text": "Interactive Activity: Choose Your Distribution\n\n\n\n\n\n\nGroup Discussion (5 minutes)\n\n\nFor each scenario, identify: 1. Is the random variable discrete or continuous? 2. What distribution might it follow? 3. What are the parameters?\nScenarios: - Number of students attending office hours per week - Time spent studying for an exam - Number of typos in a research paper - Body temperature of patients in a hospital"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#application-research-design",
    "href": "files/lecture_notes/lecture10/lecture10.html#application-research-design",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Application: Research Design",
    "text": "Application: Research Design\n\nConsider your research question:\n\nIdentify your random variable(s)\n\nWhat are you measuring?\nWhat values can it take?\n\nChoose appropriate distribution\n\nBased on the nature of your data\nConsider the underlying process\n\nPlan your analysis\n\nHow will you collect data?\nWhat statistical tests are appropriate?"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#probability-mass-vs.-density-functions",
    "href": "files/lecture_notes/lecture10/lecture10.html#probability-mass-vs.-density-functions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Probability Mass vs. Density Functions",
    "text": "Probability Mass vs. Density Functions\n\n\nDiscrete: Probability Mass Function (PMF)\n\n\\(P(X = x)\\) for specific values\nSums to 1 over all possible values\nCan find exact probabilities\n\nExample: \\(P(X = 3) = 0.2\\)\n\nContinuous: Probability Density Function (PDF)\n\n\\(f(x)\\) represents density\nArea under curve = 1\n\\(P(X = x) = 0\\) for any specific value\nFind probabilities over intervals\n\nExample: \\(P(a &lt; X &lt; b) =  \\int_{a}^{b} f(x)dx\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#comparing-distributions-side-by-side",
    "href": "files/lecture_notes/lecture10/lecture10.html#comparing-distributions-side-by-side",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Comparing Distributions Side-by-Side",
    "text": "Comparing Distributions Side-by-Side"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture10/lecture10.html#confidence-intervals-for-means",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Intervals for Means",
    "text": "Confidence Intervals for Means\n\n\nProblem: We have one sample mean, but want to estimate the population mean\nSolution: Use the sampling distribution to create a confidence interval\nKey Insight: If we know how \\(\\bar{X}\\) varies, we can make probabilistic statements about μ\n\n\n\n95% Confidence Interval Formula: \\(\\bar{x} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nInterpretation: “We are 95% confident that the true population mean lies within this interval”"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#visualizing-confidence-intervals",
    "href": "files/lecture_notes/lecture10/lecture10.html#visualizing-confidence-intervals",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#confidence-interval-interpretation",
    "href": "files/lecture_notes/lecture10/lecture10.html#confidence-interval-interpretation",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Interval Interpretation",
    "text": "Confidence Interval Interpretation\n\n\n\n\n\n\nCommon Misconceptions\n\n\n❌ WRONG: “There’s a 95% probability that μ is in this specific interval”\n✅ CORRECT: “If we repeated this process many times, 95% of the intervals we construct would contain the true μ”\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe interval is random, not the population parameter\nBefore collecting data: 95% chance our method will work\nAfter collecting data: The interval either contains μ or it doesn’t\nConfidence level = Long-run success rate of the method"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#factors-affecting-confidence-interval-width",
    "href": "files/lecture_notes/lecture10/lecture10.html#factors-affecting-confidence-interval-width",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Factors Affecting Confidence Interval Width",
    "text": "Factors Affecting Confidence Interval Width"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#key-takeaways",
    "href": "files/lecture_notes/lecture10/lecture10.html#key-takeaways",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\nRandom variables translate random outcomes into numbers\nDiscrete variables have countable values; continuous variables have uncountable values\nDistributions describe the probability patterns of random variables\nChoosing the right distribution depends on understanding your data’s nature\nReal applications exist in every field - think about your research!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#next-steps",
    "href": "files/lecture_notes/lecture10/lecture10.html#next-steps",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nFor Your Research/Interests\n\n\n\nIdentify random variables in your field\nThink about appropriate distributions\nConsider data collection methods\nPlan statistical analyses\nConnect theory to practice"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#questions-and-discussion",
    "href": "files/lecture_notes/lecture10/lecture10.html#questions-and-discussion",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Questions and Discussion",
    "text": "Questions and Discussion\n\nShare with the class:\n\nWhat random variables are important in your field of study/major?\nWhich distributions might be most relevant?\nWhat challenges do you anticipate in data collection?\n\n\n\nThank you for your participation!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#appendix-python-code-examples",
    "href": "files/lecture_notes/lecture10/lecture10.html#appendix-python-code-examples",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Appendix: Python Code Examples",
    "text": "Appendix: Python Code Examples\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n\n# Generate random samples from different distributions\n\n# Binomial\nbinom_data = np.random.binomial(n=10, p=0.3, size=100)\n\n# Poisson  \npoisson_data = np.random.poisson(lam=3, size=100)\n\n# Normal\nnormal_data = np.random.normal(loc=0, scale=1, size=100)\n\n# Exponential\nexp_data = np.random.exponential(scale=1/1.5, size=100)\n\n# Create histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0,0].hist(binom_data, bins=11, alpha=0.7, color='steelblue')\naxes[0,0].set_title('Binomial Sample')\n\naxes[0,1].hist(poisson_data, bins=15, alpha=0.7, color='coral')\naxes[0,1].set_title('Poisson Sample')\n\naxes[1,0].hist(normal_data, bins=20, alpha=0.7, color='lightblue')\naxes[1,0].set_title('Normal Sample')\n\naxes[1,1].hist(exp_data, bins=20, alpha=0.7, color='lightgreen')\naxes[1,1].set_title('Exponential Sample')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#additional-resources",
    "href": "files/lecture_notes/lecture10/lecture10.html#additional-resources",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n# Useful Python libraries for statistics and probability\nimport numpy as np           # Numerical computing\nimport scipy.stats as stats  # Statistical functions\nimport matplotlib.pyplot as plt  # Plotting\nimport seaborn as sns        # Statistical visualization\nimport pandas as pd          # Data manipulation\n\n# Quick reference for common distributions:\n# stats.binom.pmf(k, n, p)     # Binomial PMF\n# stats.poisson.pmf(k, lam)    # Poisson PMF  \n# stats.norm.pdf(x, mu, sigma) # Normal PDF\n# stats.expon.pdf(x, scale)    # Exponential PDF\n\n# Generate random samples:\n# np.random.binomial(n, p, size)\n# np.random.poisson(lam, size)\n# np.random.normal(mu, sigma, size)\n# np.random.exponential(scale, size)"
  },
  {
    "objectID": "resources.html#week-4-continuous-random-variables-intro-to-confidence-intervals",
    "href": "resources.html#week-4-continuous-random-variables-intro-to-confidence-intervals",
    "title": "Week 4: Continuous Random Variables & Confidence Intervals",
    "section": "Week 4: Continuous Random Variables & Intro to Confidence Intervals",
    "text": "Week 4: Continuous Random Variables & Intro to Confidence Intervals"
  },
  {
    "objectID": "resources.html#from-discrete-to-continuous-understanding-density-and-intervals",
    "href": "resources.html#from-discrete-to-continuous-understanding-density-and-intervals",
    "title": "Course Resources",
    "section": "From Discrete to Continuous: Understanding Density and Intervals",
    "text": "From Discrete to Continuous: Understanding Density and Intervals\nThis week transitions from discrete to continuous random variables, introducing probability density functions, common continuous distributions, and the foundations of statistical inference through confidence intervals. You’ll learn how the Central Limit Theorem enables us to make probabilistic statements about population parameters."
  },
  {
    "objectID": "resources.html#week-5-statistical-methods-testing",
    "href": "resources.html#week-5-statistical-methods-testing",
    "title": "Course Resources",
    "section": "Week 5: Statistical Methods & Testing",
    "text": "Week 5: Statistical Methods & Testing\n\n\n🔬\n\n\nHypothesis Testing Fundamentals\nWeek 5 will cover Confidence Intervals, hypothesis testing, and two sample t-Tests. Materials will be posted by week 5."
  },
  {
    "objectID": "resources.html#week-6-statistical-methods-testing",
    "href": "resources.html#week-6-statistical-methods-testing",
    "title": "Course Resources",
    "section": "Week 6: Statistical Methods & Testing",
    "text": "Week 6: Statistical Methods & Testing\n\n\n🔬\n\n\nHypothesis Testing Fundamentals\nWeek 6 will cover hypothesis testing, and two sample t-Tests. Materials will be posted by week 6."
  },
  {
    "objectID": "files/labs/lab5/lab5.html",
    "href": "files/labs/lab5/lab5.html",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "",
    "text": "Welcome to Lab 5! Today we’re moving from discrete (countable) to continuous (measurable) random variables. We’ll explore the normal distribution, learn about sampling, and get our first taste of confidence intervals!"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#understanding-the-difference",
    "href": "files/labs/lab5/lab5.html#understanding-the-difference",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Understanding the Difference",
    "text": "Understanding the Difference\nLet’s start by understanding what makes a variable continuous:\n\n# Examples of different variable types\nprint(\"DISCRETE VARIABLES (countable):\")\nprint(\"• Number of students in class: 0, 1, 2, 3, ...\")\nprint(\"• Number of emails received: 0, 1, 2, 3, ...\")\nprint(\"• Number of coin flips showing heads: 0, 1, 2, 3, ...\")\nprint()\nprint(\"CONTINUOUS VARIABLES (measurable):\")\nprint(\"• Height: 5.5 ft, 5.73 ft, 6.02541 ft, ...\")\nprint(\"• Temperature: 72.1°F, 72.15°F, 72.152°F, ...\")\nprint(\"• Time: 2.5 seconds, 2.51 seconds, ...\")\n\nDISCRETE VARIABLES (countable):\n• Number of students in class: 0, 1, 2, 3, ...\n• Number of emails received: 0, 1, 2, 3, ...\n• Number of coin flips showing heads: 0, 1, 2, 3, ...\n\nCONTINUOUS VARIABLES (measurable):\n• Height: 5.5 ft, 5.73 ft, 6.02541 ft, ...\n• Temperature: 72.1°F, 72.15°F, 72.152°F, ...\n• Time: 2.5 seconds, 2.51 seconds, ..."
  },
  {
    "objectID": "files/labs/lab5/lab5.html#key-difference-pmf-vs-pdf",
    "href": "files/labs/lab5/lab5.html#key-difference-pmf-vs-pdf",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Key Difference: PMF vs PDF",
    "text": "Key Difference: PMF vs PDF\n\n\n\n\n\n\nImportant\n\n\n\nImportant Distinction:\n\nDiscrete: Use PMF (Probability Mass Function) - gives exact probabilities\n\nExample: P(exactly 3 heads) = 0.125\n\nContinuous: Use PDF (Probability Density Function) - gives probability densities\n\nExample: P(exactly 5.000000… feet) = 0 (impossible!)\nInstead: P(between 5.0 and 5.1 feet) = some value\n\n\nKey insight: For continuous variables, we calculate probabilities over ranges, not exact values!\n\n\nLet’s visualize this difference:\n\n\n\n\n\n\nWhat we’re doing\n\n\n\nWe call plt.subplots(nrows=1, ncols=2, figsize=(12, 5)) to create one row and two columns of plots, sized so the overall figure is 12 inches wide by 5 inches tall.\nWe first define the key parameters for a binomial distribution with n (number of trials) and p (success probability).\nThen, we create a probability mass function (PMF) by first generating all possible outcomes from 0 to 11 using np.arange():\n# Create outcomes from 0 to n (inclusive)\nx_discrete = np.arange(0, n+1)\n\n\n\n\n\n\nImportant\n\n\n\nFor a binomial distribution with \\(10\\) trials, the possible number of successes are: \n\\(0 \\quad \\text{successes}, 1 \\quad \\text{success}, 2 \\quad \\text{successes}, ..., 10 \\quad \\text{successes}\\)  That’s \\(11\\) different outcomes total (0 through 10 inclusive).  When we use np.arange(0, 11), we get: \\([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\)  If we mistakenly used np.arange(0, 10), we’d get: \\([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\) and miss the case where all \\(10\\) trials are successes!\n\n\nOnce we have our values, we compute the PMF using\npython    y_discrete = stats.binom(n, p).pmf(x_discrete)\nfrom SciPy’s stats.binom.\nLastly, we plot these probabilities as a bar chart on the left subplot.\nNext to illustrate a continuous random variable (i.e., Standard Normal), we start by creating the range of values needed to create a probability density function (PDF). We create a smooth grid of values with\nx_continuous = np.linspace(-4, 4, 1000)\nvia np.linspace() (recall we used this function in lab3).\nThen, compute the PDF using\ny_continuous = stats.norm(0, 1).pdf(x_continuous)\nfrom SciPy’s stats.norm.\nLastly, we draw the curve and shade underneath on the right subplot.\nOptional: plt.tight_layout() to automatically adjust spacing so that titles, labels, and ticks don’t overlap.\n\n\n\n# Create a figure with 1 row and 2 columns of axes, total size 12x5 inches\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n\n# ── Discrete example: Binomial distribution ────────────────────────────────────\n\nn, p = 10, 0.5                             # number of trials and success probability\nx_discrete = np.arange(0, 11)              # possible counts: 0 through 10 heads\ny_discrete = stats.binom(n, p).pmf(x_discrete)  # compute PMF at each count\n\nax1.bar(\n    x_discrete, y_discrete,                # x-values and their probabilities\n    alpha=0.7,                             # make bars semi-transparent\n    color='lightblue',                     # fill color for the bars\n    edgecolor='black'                      # outline color for clarity\n)\nax1.set_title('DISCRETE: Number of Heads in 10 Flips')  # subplot title\nax1.set_xlabel('Number of Heads')         # x-axis label\nax1.set_ylabel('Probability (PMF)')       # y-axis label\nax1.grid(True, alpha=0.3)                 # light grid lines for readability\n\n# ── Continuous example: Normal distribution ───────────────────────────────────\n\nx_continuous = np.linspace(-4, 4, 1000)    # 1000 points between -4 and +4\ny_continuous = stats.norm(0, 1).pdf(x_continuous)  # standard normal PDF values\n\nax2.plot(\n    x_continuous, y_continuous,            # x-values and density values\n    'b-',                                  # blue solid line\n    linewidth=2                            # thicker line width for emphasis\n)\nax2.fill_between(\n    x_continuous, y_continuous,            # shade area under the curve\n    alpha=0.3,                             # semi-transparent fill\n    color='lightgreen'                     # fill color\n)\nax2.set_title('CONTINUOUS: Standard Normal Distribution')  # subplot title\nax2.set_xlabel('Value')                   # x-axis label\nax2.set_ylabel('Density (PDF)')           # y-axis label\nax2.grid(True, alpha=0.3)                 # light grid lines for readability\n\nplt.tight_layout()                         # adjust spacing so titles/labels don’t overlap"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#standard-normal-distribution",
    "href": "files/labs/lab5/lab5.html#standard-normal-distribution",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\nLet’s start with the standard normal: mean = 0, standard deviation = 1.\n\n\n\n\n\n\nRecall\n\n\n\nThe standard normal distribution \\(N(0,1)\\) is obtained by the linear transformation\n\\[\nZ = \\frac{X - \\mu}{\\sigma},\n\\]\nwhich removes units by centering at zero and scaling to unit variance. Its PDF is\n\\[\n\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}.\n\\]\n\n\nIn SciPy, you can create or shift any normal distribution using the loc (mean) and scale (standard deviation) parameters of stats.norm. In particular, evaluating\nstats.norm.pdf(x, loc, scale)\nis equivalent to\ny = stats.norm(loc=loc, scale=scale).pdf(x)\nand under the hood it computes\n\\[\ny = \\frac{1}{\\sigma},\\phi (z) =\\Bigl(\\frac{x - \\mu}{\\sigma}\\Bigr)\n\\]\nwhere \\(\\phi(z)\\) is the standard normal PDF.\n\n# Create a standard normal distribution\nstandard_normal = stats.norm(loc=0, scale=1)  # loc=mean, scale=standard deviation\n\nprint(\"Standard Normal Distribution:\")\nprint(f\"Mean: {standard_normal.mean()}\")\nprint(f\"Standard deviation: {standard_normal.std()}\")\n\n# Generate a smooth range of x-values from -4 to +4\nx = np.linspace(-4, 4, 1000)       # 1000 points for a smooth curve \ny = standard_normal.pdf(x) # compute the pdf\n\n# Plot the PDF\nplt.figure(figsize=(10, 6))        # figure size in inches\nplt.plot(\n    x, y,                           # x-values and PDF values\n    'b-',                          # blue solid line\n    linewidth=2,\n    label=r'Standard Normal: $\\mu=0,\\ \\sigma=1$'  # legend with LaTeX\n)\nplt.fill_between(\n    x, y,                          # shade under the PDF curve\n    alpha=0.3,\n    color='lightblue'\n)\nplt.title(r'Standard Normal Distribution $(\\mu=0,\\ \\sigma=1)$')  # title with LaTeX\nplt.xlabel('Value')               # x-axis label\nplt.ylabel('Density')             # y-axis label\nplt.axvline(\n    0,                             # vertical line at x=0\n    color='red',\n    linestyle='--',\n    linewidth=2,\n    label=r'Mean $\\mu=0$'          # legend entry for the mean line\n)\nplt.legend()                      # display legend\nplt.grid(True, alpha=0.3)         # add light grid lines\n\nStandard Normal Distribution:\nMean: 0.0\nStandard deviation: 1.0"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#calculating-probabilities-with-areas",
    "href": "files/labs/lab5/lab5.html#calculating-probabilities-with-areas",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Calculating Probabilities with Areas",
    "text": "Calculating Probabilities with Areas\n\n\n\n\n\n\nImportant\n\n\n\nFor continuous distributions, probability = area under the curve!\n\n\nThat is, the probability that \\((X)\\) falls between \\((a)\\) and \\((b)\\) is the area under the PDF from \\((x=a)\\) to \\((x=b)\\). SciPy’s stats.norm.cdf computes the cumulative distribution function (CDF)\n\\[\nF(x) = P(X \\le x) = \\int_{-\\infty}^x \\phi(t),dt\n\\]\nTherefore,\n\\[\nP(a &lt; X &lt; b) = F(b) - F(a)\n\\]\n\n# Probability that a value is between -1 and 1 - P(-1 &lt; X &lt; 1) for X ~ N(0,1\nprob_between = standard_normal.cdf(1) - standard_normal.cdf(-1)\nprint(f\"P(-1 &lt; X &lt; 1) = {prob_between:.4f}\")\n\n# Visualize this probability\nx = np.linspace(-4, 4, 1000)\ny = standard_normal.pdf(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-', linewidth=2, label='Standard Normal')\n\n# Shade the area between -1 and 1\nx_fill = x[(x &gt;= -1) & (x &lt;= 1)]\ny_fill = standard_normal.pdf(x_fill)\nplt.fill_between(x_fill, y_fill, alpha=0.5, color='red', \n                label=f'P(-1 &lt; X &lt; 1) = {prob_between:.4f}')\n\nplt.title('Probability as Area Under the Curve')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.axvline(-1, color='red', linestyle='--', alpha=0.7)\nplt.axvline(1, color='red', linestyle='--', alpha=0.7)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nP(-1 &lt; X &lt; 1) = 0.6827\n\n\n\n\n\n\n\n\n\n\nTask 1: Your First Normal Distribution\n\n⏱️ Estimated time: 6 minutes\n\nLet’s say human heights follow a normal distribution with mean = 68 inches and standard deviation = 4 inches.\nCopy the code and try it on your own!\nStep 1: Create this distribution\n\n# Heights distribution\nmean_height = ___  # Fill in: 68\nstd_height = ___   # Fill in: 4\n\nheights = stats.norm(loc=___, scale=___)\n\nprint(f\"Mean height: {heights.mean()} inches\")\nprint(f\"Standard deviation: {heights.std()} inches\")\n\nStep 2: Calculate some probabilities\n\n# a) What's the probability someone is taller than 72 inches (6 feet)?\nprob_tall = 1 - heights.cdf(___)  # Fill in: 72\nprint(f\"P(height &gt; 72 inches) = {prob_tall:.4f}\")\n\n# b) What's the probability someone is between 64 and 72 inches?\nprob_between = heights.cdf(___) - heights.cdf(___)  # Fill in both values\nprint(f\"P(64 &lt; height &lt; 72) = {prob_between:.4f}\")\n\n# c) What height is at the 90th percentile? (90% of people are shorter)\nheight_90th = heights.ppf(___)  # ppf = \"percent point function\" (inverse of cdf)\nprint(f\"90th percentile height: {height_90th:.2f} inches\")\n\nStep 3: Make a visualization\n\n# Plot the height distribution\nx = np.linspace(50, 86, 1000)\ny = heights.pdf(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-', linewidth=2)\nplt.fill_between(x, y, alpha=0.3, color='lightgreen')\nplt.title('Human Heights Distribution')\nplt.xlabel('Height (inches)')\nplt.ylabel('Density')\nplt.axvline(mean_height, color='red', linestyle='--', linewidth=2, \n           label=f'Mean = {mean_height} inches')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#the-68-95-99.7-rule",
    "href": "files/labs/lab5/lab5.html#the-68-95-99.7-rule",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "The 68-95-99.7 Rule",
    "text": "The 68-95-99.7 Rule\nThis rule (also called the empirical rule) describes how data are distributed in a normal distribution:\n\nAbout 68% of observations fall within one standard deviation of the mean (\\(\\mu \\pm 1\\sigma\\)).\nAbout 95% lie within two standard deviations (\\(\\mu \\pm 2\\sigma\\)).\nNearly 99.7% lie within three standard deviations (\\(\\mu \\pm 3\\sigma\\)).\n\nIn practice, this gives a quick way to gauge how “typical” a value is: if a point lies beyond \\(\\pm 2\\sigma\\), it’s already in the outer 5% and might be considered unusual or an outlier.\n\n# The 68-95-99.7 rule for standard normal\nmean, std = 0, 1\n\nprob_68 = stats.norm.cdf(1) - stats.norm.cdf(-1)  # Within 1 std dev\nprob_95 = stats.norm.cdf(2) - stats.norm.cdf(-2)  # Within 2 std devs\nprob_997 = stats.norm.cdf(3) - stats.norm.cdf(-3) # Within 3 std devs\n\nprint(\"The 68-95-99.7 Rule:\")\nprint(f\"• About {prob_68:.1%} of data is within 1 standard deviation\")\nprint(f\"• About {prob_95:.1%} of data is within 2 standard deviations\") \nprint(f\"• About {prob_997:.1%} of data is within 3 standard deviations\")\n\n# Visualize the rule\nx = np.linspace(-4, 4, 1000)\ny = stats.norm.pdf(x)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# 68% (1 std dev)\naxes[0].plot(x, y, 'b-', linewidth=2)\nx1 = x[(x &gt;= -1) & (x &lt;= 1)]\naxes[0].fill_between(x1, stats.norm.pdf(x1), alpha=0.5, color='green')\naxes[0].set_title('68% within 1 σ')\naxes[0].axvline(-1, color='red', linestyle='--')\naxes[0].axvline(1, color='red', linestyle='--')\n\n# 95% (2 std devs)\naxes[1].plot(x, y, 'b-', linewidth=2)\nx2 = x[(x &gt;= -2) & (x &lt;= 2)]\naxes[1].fill_between(x2, stats.norm.pdf(x2), alpha=0.5, color='orange')\naxes[1].set_title('95% within 2 σ')\naxes[1].axvline(-2, color='red', linestyle='--')\naxes[1].axvline(2, color='red', linestyle='--')\n\n# 99.7% (3 std devs)\naxes[2].plot(x, y, 'b-', linewidth=2)\nx3 = x[(x &gt;= -3) & (x &lt;= 3)]\naxes[2].fill_between(x3, stats.norm.pdf(x3), alpha=0.5, color='purple')\naxes[2].set_title('99.7% within 3 σ')\naxes[2].axvline(-3, color='red', linestyle='--')\naxes[2].axvline(3, color='red', linestyle='--')\n\nfor ax in axes:\n    ax.set_ylim(0, 0.45)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nThe 68-95-99.7 Rule:\n• About 68.3% of data is within 1 standard deviation\n• About 95.4% of data is within 2 standard deviations\n• About 99.7% of data is within 3 standard deviations"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#uniform-distribution",
    "href": "files/labs/lab5/lab5.html#uniform-distribution",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe uniform distribution on an interval \\([a,b]\\) assigns equal probability density to every point between \\(a\\) and \\(b\\). Its PDF is\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{b - a}, & a \\le x \\le b,\\\\\n0, & \\text{otherwise},\n\\end{cases}\n\\]\nso the probability of any subinterval is simply its length divided by \\((b-a)\\). In SciPy, you specify this with loc=a and scale=(b - a) when calling stats.uniform.\nLet’s visualize this:\n\n# Uniform distribution between 0 and 10\nuniform_dist = stats.uniform(loc=0, scale=10)  # loc=start, scale=width\n\nprint(f\"Uniform distribution from 0 to 10:\")\nprint(f\"Mean: {uniform_dist.mean()}\")\nprint(f\"Standard deviation: {uniform_dist.std():.2f}\")\n\n# Plot it\nx = np.linspace(-1, 11, 1000)\ny = uniform_dist.pdf(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-', linewidth=3, label='Uniform(0, 10)')\nplt.fill_between(x, y, alpha=0.3, color='yellow')\nplt.title('Uniform Distribution: All Values Equally Likely')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Calculate a probability\nprob_middle = uniform_dist.cdf(7) - uniform_dist.cdf(3)\nprint(f\"P(3 &lt; X &lt; 7) = {prob_middle:.2f}\")\n\nUniform distribution from 0 to 10:\nMean: 5.0\nStandard deviation: 2.89\n\n\n\n\n\n\n\n\n\nP(3 &lt; X &lt; 7) = 0.40"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#exponential-distribution",
    "href": "files/labs/lab5/lab5.html#exponential-distribution",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nThe exponential distribution gives the probability of waiting time until the next event (for example, the time between customer arrivals) and is controlled by a single rate parameter \\(\\lambda\\). Its probability density function (PDF) is\n\\[\nf(x) \\;=\\; \\lambda\\,e^{-\\lambda x},\n\\quad x \\ge 0,\n\\]\nso the chance of a short wait (\\(x\\) small) is high and it decays exponentially for longer waits. The average waiting time is \\(1/\\lambda\\).\nIn SciPy, you can create this via:\nfrom scipy import stats\nrate = 0.5              # for example, 0.5 events per unit time\nexp_dist = stats.expon(scale=1/rate)  # scale = 1/λ\nHere is a quick example visualization of the exponential distribution :\n\n# Exponential distribution - models time between events\n# Parameter λ (lambda) = rate parameter\nrate = 0.5  # events per unit time\nexponential_dist = stats.expon(scale=1/rate)  # scale = 1/rate\n\nprint(f\"Exponential distribution (rate = {rate}):\")\nprint(f\"Mean waiting time: {exponential_dist.mean():.2f}\")\n\n# Plot it\nx = np.linspace(0, 10, 1000)\ny = exponential_dist.pdf(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'b-', linewidth=2, label='Exponential (λ=0.5)')\nplt.fill_between(x, y, alpha=0.3, color='pink')\nplt.title('Exponential Distribution: Waiting Times')\nplt.xlabel('Time')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Probability of waiting less than 2 time units\nprob_short_wait = exponential_dist.cdf(2)\nprint(f\"P(wait time &lt; 2) = {prob_short_wait:.4f}\")\n\nExponential distribution (rate = 0.5):\nMean waiting time: 2.00\n\n\n\n\n\n\n\n\n\nP(wait time &lt; 2) = 0.6321\n\n\n\nTask 2: Bus Waiting Times\n\n⏱️ Estimated time: 4 minutes\n\nThe time between buses follows an exponential distribution with an average of 15 minutes between buses. Copy the code and try it on your own!\n\n# Bus waiting times\naverage_wait = ___  # Fill in: 15 minutes\nrate = 1 / average_wait\nbus_times = stats.expon(scale=___)  # Fill in: average_wait\n\n# Questions:\n# a) What's the probability you wait less than 10 minutes?\nprob_short = bus_times.cdf(___)\nprint(f\"P(wait &lt; 10 min) = {prob_short:.4f}\")\n\n# b) What's the probability you wait more than 30 minutes?\nprob_long = 1 - bus_times.cdf(___)\nprint(f\"P(wait &gt; 30 min) = {prob_long:.4f}\")\n\n# c) What's the median waiting time? (50th percentile)\nmedian_wait = bus_times.ppf(___)  # Fill in: 0.5\nprint(f\"Median wait time: {median_wait:.2f} minutes\")"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#the-magic-of-sample-means",
    "href": "files/labs/lab5/lab5.html#the-magic-of-sample-means",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "The Magic of Sample Means",
    "text": "The Magic of Sample Means\n\n\n\n\n\n\nWhat we’re doing\n\n\n\n\nWe define a skewed population (Exponential) with mean = 2 to illustrate a non-normal distribution.\nWe plot the population PDF to see its right skew.\nWe draw many samples of size sample_size, compute each sample’s mean, and collect those means.\nWe plot the distribution of those sample means to show how they approximate a normal distribution.\nWe repeat this process for smaller and larger sample sizes to see how sample size affects the shape (normality) and spread (standard error) of the sample means.\n\n\n\n\n# 1) Define a skewed population: Exponential with scale=2 (mean=2)\npopulation = stats.expon(scale=2)  # Mean = 2, very right-skewed\n\n# Display the population's true mean and std dev\nprint(\"Original Population (Exponential):\")\nprint(f\"Population mean: {population.mean()}\")\nprint(f\"Population std: {population.std():.3f}\")\n\n# Prepare x-values to plot the population PDF\nx = np.linspace(0, 15, 1000)\n# Compute population PDF values\ny = population.pdf(x)\n\nplt.figure(figsize=(12, 8))\n\n# Plot 1: Population PDF\nplt.subplot(2, 2, 1)\nplt.plot(x, y, 'r-', linewidth=2)\nplt.fill_between(x, y, alpha=0.3, color='red')\nplt.title('Population: Exponential (Skewed!)')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\n# 2) Simulate sampling: draw n_samples of size sample_size\nsample_size = 30  # Size of each sample\nn_samples = 1000  # Number of samples to take\n\nsample_means = []\nfor i in range(n_samples):\n    #   a) Take one sample of size sample_size\n    sample = population.rvs(sample_size)  # rvs = random variates (samples)\n    #   b) Compute the sample mean\n    sample_mean = np.mean(sample)\n    sample_means.append(sample_mean)\n\nprint(f\"\\nSample Means (n={sample_size}, {n_samples} samples):\")\nprint(f\"Mean of sample means: {np.mean(sample_means):.3f}\")\nprint(f\"Std of sample means: {np.std(sample_means):.3f}\")\n\n# Plot 2: Distribution of sample means for n=sample_size\nplt.subplot(2, 2, 2)\nplt.hist(sample_means, bins=50, density=True, alpha=0.7, color='green', edgecolor='black')\nplt.title('Distribution of Sample Means\\n(Notice: It\\'s Normal!)')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\n\n# 3) Repeat sampling with smaller sample size to illustrate increased variability\nsmall_sample_size = 5\nsmall_sample_means = []\nfor i in range(n_samples):\n    sample = population.rvs(small_sample_size)\n    small_sample_means.append(np.mean(sample))\n\nplt.subplot(2, 2, 3)\nplt.hist(small_sample_means, bins=50, density=True, alpha=0.7, color='orange', edgecolor='black')\nplt.title(f'Sample Means (n={small_sample_size})\\n(Less normal, more spread)')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\n\n# 4) Repeat sampling with larger sample size to illustrate reduced variability\nlarge_sample_size = 100\nlarge_sample_means = []\nfor i in range(n_samples):\n    sample = population.rvs(large_sample_size)\n    large_sample_means.append(np.mean(sample))\n\nplt.subplot(2, 2, 4)\nplt.hist(large_sample_means, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black')\nplt.title(f'Sample Means (n={large_sample_size})\\n(Very normal, less spread)')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\n\n# Finalize layout and display all four plots\nplt.tight_layout()\nplt.show()\n\nOriginal Population (Exponential):\nPopulation mean: 2.0\nPopulation std: 2.000\n\nSample Means (n=30, 1000 samples):\nMean of sample means: 1.995\nStd of sample means: 0.352\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem (CLT)\n\n\n\nAmazing fact: No matter what shape your population has, if you take many samples and calculate their means, those sample means will be approximately normally distributed!\nThe bigger your sample size, the more normal it gets!\nIf \\(\\bar{X}\\) is the sample mean from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then:\n\\[\\bar{X} \\sim \\text{Normal}\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right)\\]\nWhere \\(n\\) is the sample size.\n\n\n\nTask 3: Explore the CLT\n\n⏱️ Estimated time: 6 minutes\n\nLet’s verify the Central Limit Theorem with a different population! Copy the code and try it on your own!\n\n# Population: Uniform distribution from 0 to 100\npopulation = stats.uniform(loc=0, scale=100)\n\nprint(\"Population (Uniform 0 to 100):\")\nprint(f\"Population mean: {population.mean()}\")\nprint(f\"Population std: {population.std():.2f}\")\n\n# Take 500 samples of size 25 each\nsample_size = ___  # Fill in: 25\nn_samples = ___    # Fill in: 500\n\nsample_means = []\nfor i in range(n_samples):\n    sample = population.rvs(___)  # Fill in: sample_size\n    sample_means.append(np.mean(sample))\n\n# Check the CLT prediction\npredicted_mean = population.mean()\npredicted_std = population.std() / np.sqrt(sample_size)\n\nprint(f\"\\nCLT Predictions:\")\nprint(f\"Sample means should have mean ≈ {predicted_mean:.2f}\")\nprint(f\"Sample means should have std ≈ {predicted_std:.2f}\")\n\nprint(f\"\\nActual Results:\")\nprint(f\"Sample means actually have mean = {np.mean(sample_means):.2f}\")\nprint(f\"Sample means actually have std = {np.std(sample_means):.2f}\")\n\n# Make a histogram\nplt.figure(figsize=(10, 6))\nplt.hist(sample_means, bins=30, density=True, alpha=0.7, color='purple', edgecolor='black')\nplt.title('Distribution of Sample Means from Uniform Population')\nplt.xlabel('Sample Mean')\nplt.ylabel('Density')\nplt.axvline(np.mean(sample_means), color='red', linestyle='--', linewidth=2, \n           label=f'Actual mean = {np.mean(sample_means):.2f}')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#the-problem",
    "href": "files/labs/lab5/lab5.html#the-problem",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "The Problem",
    "text": "The Problem\nImagine you want to know the average height of all students at UCSB (the population mean), but you can only measure a sample of 50 students. Your sample mean is 67.2 inches.\nQuestion: What can you say about the true population mean?"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#the-solution-confidence-intervals",
    "href": "files/labs/lab5/lab5.html#the-solution-confidence-intervals",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "The Solution: Confidence Intervals",
    "text": "The Solution: Confidence Intervals\n\n# Simulate the scenario\nnp.random.seed(123)\n\n# Unknown population (we pretend we don't know this)\ntrue_population_mean = 68.0\ntrue_population_std = 4.0\ntrue_population = stats.norm(true_population_mean, true_population_std)\n\n# We take ONE sample (this is what we'd really do)\nsample_size = 50\nour_sample = true_population.rvs(sample_size)\nsample_mean = np.mean(our_sample)\nsample_std = np.std(our_sample, ddof=1)  # ddof=1 for sample std dev\n\nprint(\"What we observe from our sample:\")\nprint(f\"Sample size: {sample_size}\")\nprint(f\"Sample mean: {sample_mean:.2f} inches\")\nprint(f\"Sample std dev: {sample_std:.2f} inches\")\nprint()\nprint(\"What we DON'T know (but want to estimate):\")\nprint(f\"True population mean: {true_population_mean} inches\")\n\nWhat we observe from our sample:\nSample size: 50\nSample mean: 68.05 inches\nSample std dev: 4.81 inches\n\nWhat we DON'T know (but want to estimate):\nTrue population mean: 68.0 inches"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#building-a-95-confidence-interval",
    "href": "files/labs/lab5/lab5.html#building-a-95-confidence-interval",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Building a 95% Confidence Interval",
    "text": "Building a 95% Confidence Interval\n\n# The Central Limit Theorem tells us that sample means are normally distributed\n# with mean = population mean and std = population_std / sqrt(n)\n\n# For a 95% confidence interval, we need the 97.5th percentile of standard normal\n# (because we want 2.5% in each tail, leaving 95% in the middle)\nconfidence_level = 0.95\nalpha = 1 - confidence_level\nz_critical = stats.norm.ppf(1 - alpha/2)  # 1.96 for 95% confidence\n\nprint(f\"For {confidence_level*100}% confidence:\")\nprint(f\"Critical value (z*): {z_critical:.3f}\")\n\n# Standard error of the mean\nstandard_error = sample_std / np.sqrt(sample_size)\nprint(f\"Standard error: {standard_error:.3f}\")\n\n# Margin of error\nmargin_of_error = z_critical * standard_error\nprint(f\"Margin of error: {margin_of_error:.3f}\")\n\n# Confidence interval\nci_lower = sample_mean - margin_of_error\nci_upper = sample_mean + margin_of_error\n\nprint(f\"\\n95% Confidence Interval for population mean:\")\nprint(f\"[{ci_lower:.2f}, {ci_upper:.2f}] inches\")\nprint()\nprint(f\"Interpretation: We are 95% confident that the true population\")\nprint(f\"mean height is between {ci_lower:.2f} and {ci_upper:.2f} inches.\")\n\n# Check if it captured the true mean\nif ci_lower &lt;= true_population_mean &lt;= ci_upper:\n    print(f\"✅ SUCCESS! Our interval captured the true mean ({true_population_mean})!\")\nelse:\n    print(f\"❌ Oops! Our interval missed the true mean ({true_population_mean}).\")\n\nFor 95.0% confidence:\nCritical value (z*): 1.960\nStandard error: 0.680\nMargin of error: 1.332\n\n95% Confidence Interval for population mean:\n[66.72, 69.39] inches\n\nInterpretation: We are 95% confident that the true population\nmean height is between 66.72 and 69.39 inches.\n✅ SUCCESS! Our interval captured the true mean (68.0)!"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#visualizing-confidence-intervals",
    "href": "files/labs/lab5/lab5.html#visualizing-confidence-intervals",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals\n\n# Let's see what \"95% confidence\" really means\n# We'll create 20 different confidence intervals and see how many capture the truth\n\nn_intervals = 20\nsample_size = 30\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n\n# Generate multiple confidence intervals\nintervals = []\ncaptures = []\n\nfor i in range(n_intervals):\n    # Take a new sample each time\n    sample = true_population.rvs(sample_size)\n    sample_mean = np.mean(sample)\n    sample_std = np.std(sample, ddof=1)\n    \n    # Calculate 95% CI\n    se = sample_std / np.sqrt(sample_size)\n    me = 1.96 * se\n    ci_low = sample_mean - me\n    ci_high = sample_mean + me\n    \n    intervals.append((ci_low, ci_high, sample_mean))\n    captures.append(ci_low &lt;= true_population_mean &lt;= ci_high)\n\n# Plot the intervals\nax1.axvline(true_population_mean, color='red', linewidth=3, \n           label=f'True Population Mean = {true_population_mean}')\n\nfor i, (low, high, mean) in enumerate(intervals):\n    color = 'green' if captures[i] else 'red'\n    alpha = 0.8 if captures[i] else 1.0\n    \n    # Plot the confidence interval\n    ax1.plot([low, high], [i, i], color=color, linewidth=3, alpha=alpha)\n    # Plot the sample mean\n    ax1.plot(mean, i, 'o', color=color, markersize=6, alpha=alpha)\n\nax1.set_xlabel('Height (inches)')\nax1.set_ylabel('Sample Number')\nax1.set_title(f'20 Different 95% Confidence Intervals\\n{sum(captures)} out of {n_intervals} captured the true mean')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Show our specific sample\nax2.hist(our_sample, bins=10, alpha=0.7, color='lightblue', edgecolor='black', density=True)\nax2.axvline(sample_mean, color='blue', linewidth=3, label=f'Sample Mean = {sample_mean:.2f}')\nax2.axvline(ci_lower, color='red', linestyle='--', linewidth=2, label=f'95% CI: [{ci_lower:.2f}, {ci_upper:.2f}]')\nax2.axvline(ci_upper, color='red', linestyle='--', linewidth=2)\nax2.axvline(true_population_mean, color='orange', linewidth=3, label=f'True Mean = {true_population_mean}')\nax2.set_title('Our Sample and Confidence Interval')\nax2.set_xlabel('Height (inches)')\nax2.set_ylabel('Density')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Capture rate: {sum(captures)}/{n_intervals} = {sum(captures)/n_intervals*100:.1f}%\")\nprint(\"This should be close to 95%!\")\n\n\n\n\n\n\n\n\nCapture rate: 19/20 = 95.0%\nThis should be close to 95%!\n\n\n\nTask 4: Your Own Confidence Interval\n\n⏱️ Estimated time: 8 minutes\n\nYou’re studying the average time students spend on homework per week. You survey 40 students and get the following results:\nCopy the below code and try it on your own!\n\n# Homework time data (in hours per week)\nnp.random.seed(456)\nhomework_data = np.random.normal(15, 5, 40)  # 40 students, roughly normal\n\nprint(\"Homework Survey Results:\")\nprint(f\"Sample size: {len(homework_data)}\")\nprint(f\"Sample mean: {np.mean(homework_data):.2f} hours/week\")\nprint(f\"Sample std dev: {np.std(homework_data, ddof=1):.2f} hours/week\")\n\n# Your task: Create a 90% confidence interval\n# Step 1: Calculate the needed values\nsample_mean = np.mean(homework_data)\nsample_std = np.std(homework_data, ddof=1)\nn = len(homework_data)\n\n# Step 2: Find the critical value for 90% confidence\nconfidence = 0.90\nalpha = 1 - confidence\nz_star = stats.norm.ppf(1 - alpha/2)\nprint(f\"Critical value for 90% confidence: {z_star:.3f}\")\n\n# Step 3: Calculate standard error and margin of error\nstandard_error = sample_std / np.sqrt(n)\nmargin_of_error = z_star * standard_error\n\nprint(f\"Standard error: {standard_error:.3f}\")\nprint(f\"Margin of error: {margin_of_error:.3f}\")\n\n# Step 4: Build the confidence interval\nci_lower = sample_mean - margin_of_error\nci_upper = sample_mean + margin_of_error\n\nprint(f\"\\n90% Confidence Interval for average homework time:\")\nprint(f\"[{ci_lower:.2f}, {ci_upper:.2f}] hours per week\")\n\n# Step 5: Interpret your result\nprint(f\"\\nInterpretation:\")\nprint(f\"We are 90% confident that the true average homework time\")\nprint(f\"for all students is between {ci_lower:.2f} and {ci_upper:.2f} hours per week.\")\n\nQuestions to think about:\n\nHow would a 95% confidence interval compare to your 90% interval?\nWhat if you had surveyed 100 students instead of 40?"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#different-confidence-levels",
    "href": "files/labs/lab5/lab5.html#different-confidence-levels",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "Different Confidence Levels",
    "text": "Different Confidence Levels\n\n# Compare different confidence levels using our height data\nconfidence_levels = [0.80, 0.90, 0.95, 0.99]\n\nprint(\"Comparison of Confidence Intervals:\")\nprint(\"=\" * 50)\n\nfor conf in confidence_levels:\n    alpha = 1 - conf\n    z_crit = stats.norm.ppf(1 - alpha/2)\n    margin = z_crit * standard_error\n    \n    lower = sample_mean - margin\n    upper = sample_mean + margin\n    width = upper - lower\n    \n    print(f\"{conf*100:2.0f}% CI: [{lower:.2f}, {upper:.2f}], width = {width:.2f}\")\n\nprint()\nprint(\"Notice: Higher confidence = Wider interval!\")\nprint(\"Trade-off: Confidence vs. Precision\")\n\nComparison of Confidence Intervals:\n==================================================\n80% CI: [66.49, 68.23], width = 1.74\n90% CI: [66.25, 68.48], width = 2.24\n95% CI: [66.03, 68.70], width = 2.66\n99% CI: [65.61, 69.11], width = 3.50\n\nNotice: Higher confidence = Wider interval!\nTrade-off: Confidence vs. Precision"
  },
  {
    "objectID": "files/labs/lab5/lab5.html#whats-next",
    "href": "files/labs/lab5/lab5.html#whats-next",
    "title": "Lab 5: Continuous Random Variables & Confidence Intervals",
    "section": "What’s Next?",
    "text": "What’s Next?\nIn lab 6, our final lab; we’ll learn about:\n\nHypothesis testing (is a claim supported by data?)\nComparing two groups (t-tests)\nRelationships between variables (correlation, regression)\n\nGreat job tackling lab 5 and learning about continuous random variables and confidence intervals! 📊🎯\n\n\n\n\n\n\nQuick Reference: Python Commands\n\n\n\n# Normal distribution\nnorm_dist = stats.norm(mean, std)\n\n# Uniform distribution  \nuniform_dist = stats.uniform(start, width)\n\n# Exponential distribution\nexp_dist = stats.expon(scale=mean)\n\n# Calculate probabilities\nprob = dist.cdf(x)              # P(X ≤ x)\nprob = dist.cdf(b) - dist.cdf(a) # P(a &lt; X &lt; b)\n\n# Get percentiles\nvalue = dist.ppf(0.95)          # 95th percentile\n\n# Generate random samples\nsample = dist.rvs(size=100)     # 100 random values\n\n# Confidence interval (95%)\nz_star = stats.norm.ppf(0.975)  # Critical value\nmargin = z_star * (std / np.sqrt(n))\nci = [mean - margin, mean + margin]\n\n\n\n\nBonus Challenge:\nTry changing the parameters in any of the examples above and see how the distributions change. What happens to confidence intervals when you change the confidence level or sample size? Experiment and explore! 🔬"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "From Samples to Populations: Understanding Uncertainty\n“In statistics, we make educated guesses about the whole by looking at a part”\n\n\n\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation\n\n\n\n\n\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)\n\n\n\n\n\n\n\n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\nThe Solution: Use confidence intervals to show the range of reasonable values.\n\n\n\n\n\n\n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!\n\n\n\n\n\n\n\n\n\nStandard Deviation (σ): How spread out individual people are\nStandard Error (SE): How spread out sample averages are\n\nThe Formula: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nWhere:\n\n\\(\\sigma\\) = population standard deviation (how varied people are)\n\\(n\\) = sample size (how many people we measured)\n\\(\\sqrt{n}\\) = square root of sample size\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!\n\n\n\n\n\n\n\n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re 95% confident the true average height is between 64.3 and 70.1 inches”\n\n\n\n\n\n\n\n\n\nNext week we’ll learn: - How to test specific claims about populations - When to reject or fail to reject hypotheses\n- The connection between confidence intervals and hypothesis tests\n\n\n\n\n\n\n\n\n\n\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes\n\n\n\n\n\n\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#what-well-learn-today",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#what-well-learn-today",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "What We’ll Learn Today 🎯",
    "text": "What We’ll Learn Today 🎯\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Big Picture: From Sample to Population",
    "text": "The Big Picture: From Sample to Population\n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#why-point-estimates-arent-enough",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#why-point-estimates-arent-enough",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Why Point Estimates Aren’t Enough",
    "text": "Why Point Estimates Aren’t Enough\n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#sampling-distributions",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#the-central-limit-theorem-clt",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#the-central-limit-theorem-clt",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Central Limit Theorem (CLT) 🎯",
    "text": "The Central Limit Theorem (CLT) 🎯\n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#standard-error-the-key-to-everything",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#standard-error-the-key-to-everything",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Standard Error: The Key to Everything",
    "text": "Standard Error: The Key to Everything\n\n\n\nStandard Deviation (σ): How spread out individual people are\nStandard Error (SE): How spread out sample averages are\n\nThe Formula: \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\]\nWhere:\n\n\\(\\sigma\\) = population standard deviation (how varied people are)\n\\(n\\) = sample size (how many people we measured)\n\\(\\sqrt{n}\\) = square root of sample size\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Intuitive Idea",
    "text": "Confidence Intervals: The Intuitive Idea\n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#looking-ahead-hypothesis-testing",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#looking-ahead-hypothesis-testing",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead: Hypothesis Testing 🔮",
    "text": "Looking Ahead: Hypothesis Testing 🔮\n\n\nNext week we’ll learn:\n\nHow to test specific claims about populations\nWhen to reject or fail to reject hypotheses\nThe connection between confidence intervals and hypothesis tests"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#key-takeaways",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Key Takeaways 🎯",
    "text": "Key Takeaways 🎯\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#questions",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions? 🤔",
    "text": "Questions? 🤔\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html",
    "href": "files/lecture_notes/lecture11/lecture11.html",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nFormulate null and alternative hypotheses from research questions\nUnderstand the logic of hypothesis testing\nCalculate and interpret p-values correctly\nMake decisions using significance levels\nRecognize Type I and Type II errors and their consequences\nPerform common hypothesis tests in Python"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#learning-objectives",
    "href": "files/lecture_notes/lecture11/lecture11.html#learning-objectives",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Learning Objectives 🎯",
    "text": "Learning Objectives 🎯\nBy the end of this lecture, you will be able to:\n\nFormulate null and alternative hypotheses from research questions\nUnderstand the logic of hypothesis testing\nCalculate and interpret p-values correctly\nMake decisions using significance levels\nRecognize Type I and Type II errors and their consequences\nPerform common hypothesis tests in Python"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#what-is-hypothesis-testing",
    "href": "files/lecture_notes/lecture11/lecture11.html#what-is-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "What is Hypothesis Testing?",
    "text": "What is Hypothesis Testing?\n\n\nHypothesis testing is a statistical method for making decisions about population parameters based on sample data\nIt helps us answer questions like:\n\n“Is this new drug more effective than the current treatment?”\n“Has customer satisfaction improved after our changes?”\n“Are students’ test scores significantly different from the national average?”\n\n\n\n\nKey Idea: We use sample data to make inferences about populations, acknowledging that our conclusions might be wrong due to random variation."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#the-courtroom-analogy",
    "href": "files/lecture_notes/lecture11/lecture11.html#the-courtroom-analogy",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "The Courtroom Analogy",
    "text": "The Courtroom Analogy\n\n\nCriminal Trial\n\nPresumption: Innocent until proven guilty\nBurden of proof: Prosecution must prove guilt\nStandard: “Beyond reasonable doubt”\nVerdict: Guilty or Not Guilty\n\n\nHypothesis Testing\n\nPresumption: Null hypothesis is true\nBurden of proof: Data must provide evidence against null\nStandard: Significance level (\\(\\alpha = 0.05\\))\nDecision: Reject or Fail to Reject H₀\n\n\n\nJust like in court, we never “prove” innocence or “accept” the null hypothesis, we only determine if there’s sufficient evidence to reject it."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#the-six-steps-of-hypothesis-testing",
    "href": "files/lecture_notes/lecture11/lecture11.html#the-six-steps-of-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "The Six Steps of Hypothesis Testing",
    "text": "The Six Steps of Hypothesis Testing"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-1-state-the-hypotheses",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-1-state-the-hypotheses",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 1: State the Hypotheses",
    "text": "Step 1: State the Hypotheses\n\n\nNull Hypothesis (\\(H_0\\)): The “status quo” or “no effect” statement\n\nUsually includes “=”, “≤”, or “≥”\nWhat we assume to be true until proven otherwise\n\nAlternative Hypothesis (\\(H_1\\) or \\(H_a\\)): The research claim we want to test\n\nUsually includes “≠”, “&lt;”, or “&gt;”\nWhat we’re trying to find evidence for\n\n\n\n\nExample: Testing if a new teaching method improves test scores\n\n\\(H_0: \\mu = 75\\) (no improvement, scores stay the same)\n\\(H_1: \\mu &gt; 75\\) (scores improve with new method)"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#types-of-alternative-hypotheses",
    "href": "files/lecture_notes/lecture11/lecture11.html#types-of-alternative-hypotheses",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Types of Alternative Hypotheses",
    "text": "Types of Alternative Hypotheses"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-2-choose-significance-level-α",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-2-choose-significance-level-α",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 2: Choose Significance Level (α)",
    "text": "Step 2: Choose Significance Level (α)\n\n\nSignificance level (α): The probability of rejecting H₀ when it’s actually true\nCommon choices: α = 0.05, 0.01, or 0.10\nInterpretation: “We’re willing to be wrong 5% of the time”\n\n\n\nHow to choose α: - α = 0.05: Standard for most research - α = 0.01: More conservative, when Type I errors are costly - α = 0.10: Less conservative, when Type II errors are costly\n\n\nImportant: Choose α before collecting data to avoid bias!"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-3-check-assumptions-and-conditions",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-3-check-assumptions-and-conditions",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 3: Check Assumptions and Conditions",
    "text": "Step 3: Check Assumptions and Conditions\nCommon assumptions for many tests:\n\n\nIndependence: Observations don’t influence each other\nNormality: Data comes from a normal distribution (or n ≥ 30)\nEqual variances: When comparing groups\nRandom sampling: Sample represents the population\n\n\n\nWhat if assumptions are violated? - Use non-parametric tests - Transform the data - Use robust methods - Increase sample size"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-4-calculate-the-test-statistic",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-4-calculate-the-test-statistic",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 4: Calculate the Test Statistic",
    "text": "Step 4: Calculate the Test Statistic\n\n\nTest statistic: A standardized measure of how far our sample result is from what we’d expect if H₀ were true\nCommon test statistics:\n\nz-statistic: For means when σ is known\nt-statistic: For means when σ is unknown\nχ² statistic: For categorical data\nF-statistic: For comparing variances\n\n\n\n\nFormula for one-sample t-test: \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\]\nWhere: \\(\\bar{x}\\) = sample mean, \\(\\mu_0\\) = hypothesized mean, \\(s\\) = sample standard deviation, \\(n\\) = sample size"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-5-find-the-p-value",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-5-find-the-p-value",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 5: Find the P-value",
    "text": "Step 5: Find the P-value\n\n\nP-value interpretation: “If H₀ were true, what’s the probability of getting a test statistic at least as extreme as what we observed?”"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#common-p-value-misconceptions",
    "href": "files/lecture_notes/lecture11/lecture11.html#common-p-value-misconceptions",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common P-value Misconceptions",
    "text": "Common P-value Misconceptions\n\n\n\n\n\n\nWhat P-values DON’T tell us\n\n\n❌ WRONG: “P-value is the probability that H₀ is true”\n❌ WRONG: “P-value is the probability of making an error”\n❌ WRONG: “1 - p-value is the probability that H₁ is true”\n❌ WRONG: “Smaller p-values mean larger effects”\n\n\n\n\n✅ CORRECT: “P-value is the probability of observing this result (or more extreme) assuming H₀ is true”"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#step-6-make-a-decision-and-interpret",
    "href": "files/lecture_notes/lecture11/lecture11.html#step-6-make-a-decision-and-interpret",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 6: Make a Decision and Interpret",
    "text": "Step 6: Make a Decision and Interpret\n\nDecision Rule: - If p-value ≤ α: Reject H₀ (statistically significant) - If p-value &gt; α: Fail to reject H₀ (not statistically significant)\n\n\nLanguage matters: - ✅ “Reject H₀” or “Fail to reject H₀” - ❌ “Accept H₀” or “Prove H₁” - ✅ “Evidence suggests…” or “Data supports…” - ❌ “H₁ is true” or “H₀ is false”"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#types-of-errors",
    "href": "files/lecture_notes/lecture11/lecture11.html#types-of-errors",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Types of Errors",
    "text": "Types of Errors"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#real-world-error-consequences",
    "href": "files/lecture_notes/lecture11/lecture11.html#real-world-error-consequences",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Real-World Error Consequences",
    "text": "Real-World Error Consequences\n\n\nType I Error Examples: - Medical: Saying a drug works when it doesn’t - Legal: Convicting an innocent person - Quality Control: Rejecting good products - Marketing: Launching ineffective campaigns\n\nType II Error Examples: - Medical: Missing a disease diagnosis - Legal: Acquitting a guilty person - Quality Control: Accepting defective products - Security: Missing a threat\n\n\nThe Trade-off: Reducing one type of error usually increases the other. We must balance based on the consequences of each error type."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#statistical-power",
    "href": "files/lecture_notes/lecture11/lecture11.html#statistical-power",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Statistical Power",
    "text": "Statistical Power\n\n\nPower (1 - β): The probability of correctly rejecting a false null hypothesis\nWhat affects power?\n\nEffect size: Larger effects are easier to detect\nSample size: More data increases power\nSignificance level: Higher α increases power\nVariability: Less noise increases power"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#example-1-one-sample-t-test",
    "href": "files/lecture_notes/lecture11/lecture11.html#example-1-one-sample-t-test",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: One-Sample t-test",
    "text": "Example 1: One-Sample t-test\nResearch Question: Does a new study technique improve test scores compared to the school average of 75?\n\n\nSample Statistics:\nSample size: 25\nSample mean: 76.69\nSample std: 7.65\n\n\nStep 1: State Hypotheses - H₀: μ = 75 (new method doesn’t improve scores) - H₁: μ &gt; 75 (new method improves scores)\nSteps 2-3: α = 0.05, assume normality (n=25 is borderline, but we’ll proceed)"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#example-1-calculations",
    "href": "files/lecture_notes/lecture11/lecture11.html#example-1-calculations",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: Calculations",
    "text": "Example 1: Calculations\n\n\nTest statistic: t = 1.105\nDegrees of freedom: 24\nP-value: 0.1400\n\nDecision:\nα = 0.05\nP-value (0.1400) &gt; α (0.05): Fail to reject H₀\nConclusion: There is insufficient evidence that the new study technique improves test scores."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#example-1-visualization",
    "href": "files/lecture_notes/lecture11/lecture11.html#example-1-visualization",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: Visualization",
    "text": "Example 1: Visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#using-python-for-hypothesis-testing",
    "href": "files/lecture_notes/lecture11/lecture11.html#using-python-for-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Using Python for Hypothesis Testing",
    "text": "Using Python for Hypothesis Testing\n\n\nUsing scipy.stats.ttest_1samp:\nt-statistic: 1.105\np-value (two-tailed): 0.2799\np-value (one-tailed): 0.1400\n\nUsing statsmodels:\nt-statistic: 1.105\np-value (one-tailed): 0.1400\ndegrees of freedom: 24.0"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#example-2-two-sample-t-test",
    "href": "files/lecture_notes/lecture11/lecture11.html#example-2-two-sample-t-test",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 2: Two-Sample t-test",
    "text": "Example 2: Two-Sample t-test\nResearch Question: Is there a difference in test scores between two teaching methods?\n\n\nMethod A (Traditional):\n  n = 30, mean = 75.45, std = 11.87\nMethod B (New):\n  n = 28, mean = 80.72, std = 14.82\n\nHypotheses:\nH₀: μ_A = μ_B (no difference between methods)\nH₁: μ_A ≠ μ_B (there is a difference)\n\nTwo-sample t-test results:\nt-statistic: -1.500\np-value: 0.1392\n\nDecision: Fail to reject H₀ (p = 0.1392 &gt; 0.05)\nConclusion: No significant difference between teaching methods."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#two-sample-test-visualization",
    "href": "files/lecture_notes/lecture11/lecture11.html#two-sample-test-visualization",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Two-Sample Test Visualization",
    "text": "Two-Sample Test Visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#effect-size-cohens-d",
    "href": "files/lecture_notes/lecture11/lecture11.html#effect-size-cohens-d",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Effect Size: Cohen’s d",
    "text": "Effect Size: Cohen’s d\n\n\nCohen's d: -0.394\nEffect size interpretation: small\n\nCohen's d interpretation:\n  |d| &lt; 0.2: negligible effect\n  0.2 ≤ |d| &lt; 0.5: small effect\n  0.5 ≤ |d| &lt; 0.8: medium effect\n  |d| ≥ 0.8: large effect"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#common-hypothesis-tests-summary",
    "href": "files/lecture_notes/lecture11/lecture11.html#common-hypothesis-tests-summary",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common Hypothesis Tests Summary",
    "text": "Common Hypothesis Tests Summary\n\n\nCommon Hypothesis Tests:\n================================================================================\n\nOne-sample t-test:\n  Purpose: Compare sample mean to known value\n  Data Type: Continuous\n  Python: stats.ttest_1samp()\n\nTwo-sample t-test:\n  Purpose: Compare means of two groups\n  Data Type: Continuous\n  Python: stats.ttest_ind()\n\nPaired t-test:\n  Purpose: Compare paired observations\n  Data Type: Continuous\n  Python: stats.ttest_rel()\n\nOne-sample z-test:\n  Purpose: Compare sample mean (known σ)\n  Data Type: Continuous\n  Python: stats.normaltest()\n\nChi-square goodness of fit:\n  Purpose: Test if data fits distribution\n  Data Type: Categorical\n  Python: stats.chisquare()\n\nChi-square independence:\n  Purpose: Test independence of variables\n  Data Type: Categorical\n  Python: stats.chi2_contingency()"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#activity-practice-problem",
    "href": "files/lecture_notes/lecture11/lecture11.html#activity-practice-problem",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Activity: Practice Problem",
    "text": "Activity: Practice Problem\n\n\n\n\n\n\nYour Turn!\n\n\nA coffee shop claims their average wait time is 5 minutes. You collect data on 20 customers and find: - Sample mean: 5.8 minutes - Sample standard deviation: 2.1 minutes\nQuestions: 1. Set up appropriate hypotheses (use α = 0.05) 2. What type of test should you use? 3. Calculate the test statistic and p-value 4. Make a decision and interpret the results 5. What are the practical implications?\n\n\n\n\nThink about: Is this a one-tailed or two-tailed test? What assumptions do you need to check?"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#practice-problem-solution",
    "href": "files/lecture_notes/lecture11/lecture11.html#practice-problem-solution",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Practice Problem Solution",
    "text": "Practice Problem Solution\n\n\nPractice Problem Solution:\n========================================\n1. Hypotheses:\n   H₀: μ = 5 (average wait time is 5 minutes)\n   H₁: μ ≠ 5 (average wait time is different from 5 minutes)\n   (Two-tailed test - we're testing if it's different, not specifically longer)\n\n2. Test type: One-sample t-test\n   (Population standard deviation unknown, small sample)\n\n3. Calculations:\n   t = (5.8 - 5.0) / (2.1 / √20) = 1.704\n   df = 20 - 1 = 19\n   p-value = 0.1047\n\n4. Decision:\n   Fail to reject H₀ (p = 0.1047, α = 0.05)\n   Conclusion: There is insufficient evidence that wait time differs from 5 minutes.\n\n5. Practical implications:\n   The actual average wait time appears to be about 5.8 minutes,\n   which is 0.7999999999999998 minutes longer than claimed.\n   Management should investigate why wait times exceed the 5-minute target."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#common-mistakes-and-pitfalls",
    "href": "files/lecture_notes/lecture11/lecture11.html#common-mistakes-and-pitfalls",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common Mistakes and Pitfalls",
    "text": "Common Mistakes and Pitfalls\n\n\n\n\n\n\nAvoid These Common Errors\n\n\n\nConfusing practical vs. statistical significance\n\nLarge samples can detect tiny, meaningless differences\nAlways consider effect size and practical importance\n\nP-hacking / Data dredging\n\nTesting multiple hypotheses until finding significance\nSolution: Adjust α, pre-specify analyses\n\nMisinterpreting p-values\n\nP-value ≠ probability that H₀ is true\nP-value ≠ probability of making an error\n\nIgnoring assumptions\n\nCheck normality, independence, equal variances\nUse appropriate alternatives when violated\n\nChoosing α after seeing results\n\nAlways set significance level before analysis\nAvoid changing criteria to get desired results"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#statistical-significance-vs.-practical-significance",
    "href": "files/lecture_notes/lecture11/lecture11.html#statistical-significance-vs.-practical-significance",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Statistical Significance vs. Practical Significance",
    "text": "Statistical Significance vs. Practical Significance\n\n\n\nKey Lesson: Statistical significance ≠ Practical importance\nLeft: Tiny effect (0.1) but significant due to large n\nRight: Large effect (8.7) but not significant due to small n"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#best-practices-for-hypothesis-testing",
    "href": "files/lecture_notes/lecture11/lecture11.html#best-practices-for-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Best Practices for Hypothesis Testing",
    "text": "Best Practices for Hypothesis Testing\n\n\nPlan before you collect data\n\nPre-specify hypotheses, α level, and analysis plan\nCalculate required sample size (power analysis)\n\nCheck your assumptions\n\nUse diagnostic plots and tests\nConsider robust alternatives if violated\n\nReport effect sizes\n\nP-values don’t tell the whole story\nInclude confidence intervals for estimates\n\nConsider practical significance\n\nIs the difference meaningful in context?\nWhat are the costs/benefits of different decisions?\n\nBe honest about multiple testing\n\nAdjust for multiple comparisons when appropriate\nReport all tests performed, not just significant ones"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#summary-the-logic-of-hypothesis-testing",
    "href": "files/lecture_notes/lecture11/lecture11.html#summary-the-logic-of-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Summary: The Logic of Hypothesis Testing",
    "text": "Summary: The Logic of Hypothesis Testing\n\n\nStart with skepticism (assume H₀ is true)\nCollect evidence (sample data)\nQuantify surprise (how unusual is this result if H₀ were true?)\nMake a decision (is the evidence strong enough to reject H₀?)\nAcknowledge uncertainty (we might be wrong!)\n\n\n\nRemember: Hypothesis testing doesn’t prove anything definitively. It provides a framework for making decisions under uncertainty using probabilistic reasoning."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#key-takeaways",
    "href": "files/lecture_notes/lecture11/lecture11.html#key-takeaways",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nHypothesis testing helps us make decisions about populations using sample data\nP-values tell us how surprising our data would be if H₀ were true\nStatistical significance ≠ practical importance\nAlways check assumptions and consider effect sizes\nPlan your analysis before collecting data\nBe aware of Type I and Type II errors\n\n\nNext steps: Practice with different types of tests, learn about confidence intervals, and explore more advanced topics like multiple testing corrections and non-parametric alternatives."
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#appendix-python-code-templates",
    "href": "files/lecture_notes/lecture11/lecture11.html#appendix-python-code-templates",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Appendix: Python Code Templates",
    "text": "Appendix: Python Code Templates\n\n# Template for one-sample t-test\nimport numpy as np\nfrom scipy import stats\n\n# Your data\ndata = [...]  # Replace with your data\nnull_value = 0  # Replace with your null hypothesis value\n\n# Perform test\nt_stat, p_value = stats.ttest_1samp(data, null_value)\n\n# For one-tailed test, divide p-value by 2\np_value_one_tailed = p_value / 2\n\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value (two-tailed): {p_value:.4f}\")\nprint(f\"p-value (one-tailed): {p_value_one_tailed:.4f}\")\n\n# Template for two-sample t-test\ngroup1 = [...]  # Replace with your first group\ngroup2 = [...]  # Replace with your second group\n\n# Perform test\nt_stat, p_value = stats.ttest_ind(group1, group2, equal_var=True)\n\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Effect size (Cohen's d)\ndef cohens_d(x, y):\n    nx, ny = len(x), len(y)\n    dof = nx + ny - 2\n    pooled_std = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / dof)\n    return (np.mean(x) - np.mean(y)) / pooled_std\n\nd = cohens_d(group1, group2)\nprint(f\"Cohen's d: {d:.3f}\")"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "🏠"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#todays-learning-objectives",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nUnderstand sampling distributions and their properties (Section 1.2)\nApply the Central Limit Theorem to sampling (Section 1.4)\nConstruct confidence intervals for population means (Section 1.6)\nConstruct confidence intervals for population proportions (Section 1.8)\nInterpret confidence intervals correctly (Section 1.5)\nDetermine appropriate sample sizes for desired precision\nUse python to calculate confidence intervals\nDistinguish between different types of sampling methods"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-sampling-dist",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-sampling-dist",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Big Picture: Statistical Inference",
    "text": "The Big Picture: Statistical Inference\n\n\n\n\nPopulation vs Sample\n\n\n\nPopulation: All individuals of interest\n\n\nSample: Subset we actually observe\n\n\nParameter: Population characteristic (\\(\\mu\\), \\(p\\))\n\n\nStatistic: Sample characteristic (\\(\\bar{x}\\), \\(\\hat{p}\\))\n\n\n\nGoal: Use sample statistics to estimate population parameters\n\n\n\n\n\n\n\n\n\nWhy Confidence Intervals?\n\n\n\nPoint estimates are rarely exactly correct\n\n\nInterval estimates capture uncertainty\n\n\nConfidence level quantifies our certainty\n\n\nMargin of error shows precision\n\n\n\nKey Insight: We trade precision for confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sampling-distributions",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nA sampling distribution is the distribution of a statistic (like \\(\\bar{x}\\)) across all possible samples of size \\(n\\).\n\n\n\nKey Properties:\nCenter:\n\\(E[\\bar{X}] = \\mu\\) (unbiased)\nSpread:\n\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nShape:\nApproaches normal as \\(n\\) increases (Central Limit Theorem)\nStandard Error vs Standard Deviation:\n\n\\(\\sigma\\): spread of individual observations\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\): spread of sample means\n\n\n\n\n\n\nDrag the slider to see how sample size affects the sampling distribution"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-clt-sampling",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-clt-sampling",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Central Limit Theorem in Action",
    "text": "Central Limit Theorem in Action\n\n\n\n\nNew Population\n\n Uniform Population Exponential Population Bimodal Population Right-Skewed Population  Sample Size:  Collect 1000 Sample Means\n\n\n\nPopulation μ: - | Sample Means μ: - | Standard Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-interpretation",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-interpretation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Concept",
    "text": "Confidence Intervals: The Concept\n\n\nWhat is a Confidence Interval? A confidence interval provides a range of plausible values for a population parameter. 95% Confidence Interval: If we repeated our sampling process many times, about \\(95\\%\\) of the intervals we construct would contain the true population parameter.\n\n\n\n\n\nClick to generate new 95% confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-means",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Means",
    "text": "Confidence Intervals for Population Means\n\n\n\n\n 🎯 When σ is Known:\n\n\n\\[\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\n\n\nWhen \\(\\sigma\\) is Unknown (more common):\n\n\n\\[\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\bar{x}\\): sample mean\n\n\n\\(t^*\\): critical value (df = n-1)\n\n\n\\(\\frac{s}{\\sqrt{n}}\\): standard error\n\n\n\n\n\n\nCommon Confidence Levels:\n\n\n\n90%: z* = 1.645, more precise\n\n\n95%: z* = 1.96, most common\n\n\n99%: z* = 2.576, more confident\n\n\n\nConditions Required:\n\n\n\nRandom sampling\nNearly normal population OR n ≥ 30\nIndependent observations"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Means",
    "text": "Interactive CI Demo: Confidence Intervals for Means\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population μ:  Population σ:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures μ? - | Margin of Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-proportions",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sec-ci-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Proportions",
    "text": "Confidence Intervals for Population Proportions\n\n\n\n 🎯 Formula:\n\n\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\hat{p} = \\frac{x}{n}\\): sample proportion\n\n\n\\(z^*\\): critical value\n\n\n\\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\): standard error\n\n\n\n\nConditions Required:\n\n\n\nRandom sampling\n\n\n\\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\)\n\n\nIndependent observations\n\n\nPopulation at least 10× sample size\n\n\n\nConservative Approach:\n\n\nUse \\(\\hat{p} = 0.5\\) for planning when true proportion unknown (maximizes margin of error)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Proportions",
    "text": "Interactive CI Demo: Confidence Intervals for Proportions\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population p:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures p? - | Sample Proportion: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 1: CI for Mean",
    "text": "Practice Problem 1: CI for Mean\n\nA random sample of 25 college students shows a mean daily screen time of 6.2 hours with a standard deviation of 1.8 hours. (a) Construct a 95% confidence interval for the mean daily screen time. (b) Interpret the confidence interval in context. (c) What would happen to the interval width if we used 99% confidence instead? Show Solution\n\nSolution. (a)\nGiven: \\(n = 25\\), \\(\\bar{x} = 6.2\\), \\(s = 1.8\\), 95% confidence\nFor \\(df = 24\\), \\(t^* = 2.064\\)\n\\(SE = \\frac{s}{\\sqrt{n}} = \\frac{1.8}{\\sqrt{25}} = 0.36\\)\n\\(CI = 6.2 \\pm 2.064 \\times 0.36 = 6.2 \\pm 0.743 = (5.46, 6.94)\\) hours\n(b)\nWe are 95% confident that the true mean daily screen time for all college students is between \\(5.46\\) and \\(6.94\\) hours.\n(c)\nFor 99% confidence, we use \\(t^* = 2.797\\), giving a wider interval: \\((5.19, 7.21)\\) hours."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 2: CI for Proportion",
    "text": "Practice Problem 2: CI for Proportion\n\nIn a survey of 400 voters, 240 support a particular candidate. (a) Construct a 90% confidence interval for the true proportion of supporters. (b) Check if the conditions for inference are met. (c) How large a sample would be needed for a margin of error of 0.03 with 95% confidence? Show Solution\n\nSolution. (a)\n\\(\\hat{p} = \\frac{240}{400} = 0.6\\), \\(n = 400\\), 90% confidence, \\(z^* = 1.645\\)\n\\(SE = \\sqrt{\\frac{0.6 \\times 0.4}{400}} = \\sqrt{\\frac{0.24}{400}} = 0.0245\\)\n\\(CI = 0.6 \\pm 1.645 \\times 0.0245 = 0.6 \\pm 0.0403 = (0.560, 0.640)\\)\n(b)\nCheck conditions: \\(n\\hat{p} = 400 \\times 0.6 = 240 \\geq 10\\) ✓\n\\(n(1-\\hat{p}) = 400 \\times 0.4 = 160 \\geq 10\\) ✓\n(c)\nSample size calculation:\n\\(n = \\frac{(z^*)^2 \\hat{p}(1-\\hat{p})}{ME^2} =\n\\frac{(1.96)^2 \\times 0.6 \\times 0.4}{(0.03)^2} =\n\\frac{0.9216}{0.0009} = 1024\\) people"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 3: Sample Size Planning",
    "text": "Practice Problem 3: Sample Size Planning\n\nA market researcher wants to estimate the average amount spent on coffee per week by college students. (a) How large a sample is needed for a 95% CI with margin of error $2 if \\(\\sigma\\) = $8? (b) If the budget only allows for 100 students, what confidence level gives a $2 margin of error? (c) What’s the trade-off between sample size, confidence level, and precision?\n\nShow Solution\n\n\nSolution. (a)\nFor means:\n\\(n = \\frac{(z^*)^2 \\sigma^2}{ME^2} =\n\\frac{(1.96)^2 \\times 8^2}{2^2} =\n\\frac{245.86}{4} = 62\\) students\n(b)\nWith \\(n = 100\\):\n\\(ME = z^* \\frac{\\sigma}{\\sqrt{n}} =\nz^* \\frac{8}{\\sqrt{100}} =\n0.8 z^*\\)\nFor \\(ME = 2\\):\n\\(z^* = \\frac{2}{0.8} = 2.5\\),\nwhich corresponds to about 98.8% confidence\n(c) Trade-offs:\n\nHigher confidence \\(\\rightarrow\\) wider intervals (less precision)\nLarger sample \\(\\rightarrow\\) narrower intervals (more precision)\nLower margin of error \\(\\rightarrow\\) need larger sample or lower confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes and Misconceptions",
    "text": "Common Mistakes and Misconceptions\n\n\nInterpretation Errors\n❌ Wrong: “\\(95\\%\\) of the data falls in this interval”\n✅ Right: “We’re \\(95\\%\\) confident the parameter is in this interval”\n❌ Wrong: “There’s a \\(95\\%\\) chance \\(\\mu\\) is in this interval”\n✅ Right: “\\(95\\%\\) of such intervals contain \\(\\mu\\)”\n\nTechnical Errors\n\nUsing \\(z*\\) when σ is unknown and \\(n &lt; 30\\)\nForgetting to check conditions\nConfusing standard error with standard deviation\nUsing wrong degrees of freedom for t-distribution\n\n\nRemember: The confidence level refers to the long-run proportion of intervals that capture the parameter!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size and Margin of Error Relationships",
    "text": "Sample Size and Margin of Error Relationships\n\n\nPopulation σ:  Confidence Level:  90% 95% 99%   Desired Margin of Error: \n\n\n\n\n\n\nSample Size vs Margin of Error\n\n\n\n\nRequired Sample Size: - | Resulting ME: -"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Types of Sampling Methods",
    "text": "Types of Sampling Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nSimple Random\nEvery individual has equal chance\nUnbiased, simple\nMay not represent subgroups\n\n\nStratified\nSample from each subgroup\nEnsures representation\nMore complex\n\n\nCluster\nSample entire groups\nCost-effective for spread populations\nHigher variability\n\n\nSystematic\nEvery k-th individual\nSimple to implement\nCan miss patterns\n\n\nConvenience\nEasily accessible individuals\nQuick and cheap\nHighly biased\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling Method Matters: Only probability sampling methods allow for valid statistical inference!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#confidence-intervals-in-practice",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#confidence-intervals-in-practice",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals in Practice",
    "text": "Confidence Intervals in Practice\n\n\n\nWhen to Use Each Type\nMeans: Continuous data (height, income, test scores)\nProportions: Categorical data (yes/no, success/failure)\nChoosing Confidence Level\n\n90%: Quick estimates, less critical decisions\n95%: Standard in most research\n99%: High-stakes decisions, medical trials\n\n\nReal-World Applications\n\nPolitical polls: Proportion confidence intervals\nQuality control: Mean confidence intervals\nMedical research: Both types with high confidence\nBusiness analytics: Varies by decision importance\n\nCommunication Tips\n\nAlways include the confidence level\nState what the interval estimates\nAcknowledge the uncertainty\nConsider practical significance"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#key-takeaways",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nMain Concepts\n\nSampling distributions follow predictable patterns\nConfidence intervals quantify uncertainty\nCentral Limit Theorem makes normal-based inference possible\nSample size directly affects precision\n\n\nPractical Guidelines Choose appropriate methods based on:\n\nData type (continuous vs categorical)\nSample size (use t when σ unknown)\nDesired precision (affects sample size)\nConfidence level (affects interval width)\n\nKey Principle Statistical inference allows us to make informed decisions about populations using sample data, while properly accounting for uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#looking-ahead",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#looking-ahead",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Hypothesis Testing\nTopics we’ll cover:\n\nNull and alternative hypotheses\nTest statistics and p-values\nType I and Type II errors\n\n\nConnection: Confidence intervals and hypothesis tests are two sides of the same statistical inference coin"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#questions",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions?",
    "text": "Questions?\n\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and Statistical Significance"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9-interactive.html#resources",
    "href": "files/lecture_notes/lecture9/lecture9-interactive.html#resources",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Resources",
    "text": "Resources\n\n  \n    \n      \n      Read OpenIntro Statistics Chapter 5 sections 5.1-5.3\n    \n    \n      \n      Khan Academy - Confidence Intervals\n    \n    \n      \n      Seeing Theory - Frequentist Inference\n    \n    \n      \n      Confidence Intervals - Wikipedia\n    \n    \n      \n      Understanding Different Types of Intervals"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html",
    "href": "files/labs/lab5/lab5_sln.html",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "",
    "text": "# Install any missing packages (will skip those already installed)\n#!%pip install --quiet numpy matplotlib scipy pandas statsmodels\n\n# Load our tools (libraries)\nimport numpy as np # numerical computing (arrays, random numbers, etc.)\nimport matplotlib.pyplot as plt # plotting library for static 2D graphs and visualizations\nfrom scipy import stats #  statistical functions (distributions, tests, etc.)\nimport pandas as pd # data structures (DataFrame) and data analysis tools\nimport statsmodels  # statistical modeling (regression, time series, ANOVA, etc.)\n\n# Make our graphs look nice\n#!%matplotlib inline     # embed Matplotlib plots directly in the notebook\nplt.style.use('seaborn-v0_8-whitegrid')  # Apply a clean whitegrid style from Seaborn\n\n# Set random seed for reproducible results\nnp.random.seed(42)    # fix the random seed so results can be reproduced exactly\n\nprint(\"✅ All tools loaded successfully!\") \n\n✅ All tools loaded successfully!"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#working-with-different-normal-distributions",
    "href": "files/labs/lab5/lab5_sln.html#working-with-different-normal-distributions",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "Working with Different Normal Distributions",
    "text": "Working with Different Normal Distributions\n\n# Create and compare different normal distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Different means, same std dev\nx = np.linspace(-10, 20, 1000)\nfor i, mean in enumerate([0, 5, 10]):\n    y = stats.norm(mean, 2).pdf(x)\n    axes[0,0].plot(x, y, label=f'μ={mean}, σ=2')\naxes[0,0].set_title('Different Means, Same Standard Deviation')\naxes[0,0].legend()\naxes[0,0].grid(True, alpha=0.3)\n\n# Same mean, different std devs  \nx = np.linspace(-15, 15, 1000)\nfor i, std in enumerate([1, 2, 4]):\n    y = stats.norm(0, std).pdf(x)\n    axes[0,1].plot(x, y, label=f'μ=0, σ={std}')\naxes[0,1].set_title('Same Mean, Different Standard Deviations')\naxes[0,1].legend()\naxes[0,1].grid(True, alpha=0.3)\n\n# Probability calculations example\nnormal_dist = stats.norm(100, 15)  # IQ scores: mean=100, std=15\nx = np.linspace(40, 160, 1000)\ny = normal_dist.pdf(x)\n\naxes[1,0].plot(x, y, 'b-', linewidth=2)\naxes[1,0].fill_between(x, y, alpha=0.3)\n\n# Highlight specific regions\nx_high = x[x &gt;= 130]\ny_high = normal_dist.pdf(x_high)\naxes[1,0].fill_between(x_high, y_high, alpha=0.7, color='red', \n                      label=f'P(IQ ≥ 130) = {1-normal_dist.cdf(130):.3f}')\naxes[1,0].set_title('IQ Scores Distribution')\naxes[1,0].legend()\naxes[1,0].grid(True, alpha=0.3)\n\n# Z-score example\nraw_scores = [85, 100, 115, 130]\nz_scores = [(score - 100) / 15 for score in raw_scores]\n\naxes[1,1].scatter(raw_scores, z_scores, s=100, c='red')\nfor i, (raw, z) in enumerate(zip(raw_scores, z_scores)):\n    axes[1,1].annotate(f'({raw}, {z:.1f})', (raw, z), xytext=(5, 5), \n                      textcoords='offset points')\naxes[1,1].set_xlabel('Raw Score')\naxes[1,1].set_ylabel('Z-Score')\naxes[1,1].set_title('Raw Scores vs Z-Scores')\naxes[1,1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Z-Score Transformations:\")\nfor raw, z in zip(raw_scores, z_scores):\n    print(f\"Raw score {raw} → Z-score {z:.2f}\")\n\n\n\n\n\n\n\n\nZ-Score Transformations:\nRaw score 85 → Z-score -1.00\nRaw score 100 → Z-score 0.00\nRaw score 115 → Z-score 1.00\nRaw score 130 → Z-score 2.00"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#confidence-interval-simulation",
    "href": "files/labs/lab5/lab5_sln.html#confidence-interval-simulation",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "Confidence Interval Simulation",
    "text": "Confidence Interval Simulation\n\n# Simulate many confidence intervals to verify coverage probability\ndef simulate_confidence_intervals(true_mean, true_std, sample_size, confidence_level, n_simulations=100):\n    \"\"\"\n    Simulate many confidence intervals and check coverage rate\n    \"\"\"\n    alpha = 1 - confidence_level\n    z_critical = stats.norm.ppf(1 - alpha/2)\n    \n    coverage_count = 0\n    intervals = []\n    \n    for i in range(n_simulations):\n        # Generate a sample\n        sample = np.random.normal(true_mean, true_std, sample_size)\n        sample_mean = np.mean(sample)\n        sample_std = np.std(sample, ddof=1)\n        \n        # Calculate confidence interval\n        se = sample_std / np.sqrt(sample_size)\n        margin = z_critical * se\n        ci_lower = sample_mean - margin\n        ci_upper = sample_mean + margin\n        \n        # Check if interval captures true mean\n        captures = ci_lower &lt;= true_mean &lt;= ci_upper\n        if captures:\n            coverage_count += 1\n            \n        intervals.append((ci_lower, ci_upper, captures))\n    \n    coverage_rate = coverage_count / n_simulations\n    return intervals, coverage_rate\n\n# Run simulation\ntrue_mean, true_std = 50, 10\nsample_size = 30\nconfidence_level = 0.95\nn_sims = 100\n\nintervals, coverage_rate = simulate_confidence_intervals(\n    true_mean, true_std, sample_size, confidence_level, n_sims\n)\n\nprint(f\"Simulation Results:\")\nprint(f\"True population mean: {true_mean}\")\nprint(f\"Sample size: {sample_size}\")\nprint(f\"Confidence level: {confidence_level*100}%\")\nprint(f\"Number of simulations: {n_sims}\")\nprint(f\"Coverage rate: {coverage_rate*100:.1f}%\")\nprint(f\"Expected coverage rate: {confidence_level*100}%\")\n\n# Plot first 20 intervals\nplt.figure(figsize=(12, 8))\nplt.axvline(true_mean, color='red', linewidth=3, label=f'True Mean = {true_mean}')\n\nfor i in range(min(20, len(intervals))):\n    lower, upper, captures = intervals[i]\n    color = 'green' if captures else 'red'\n    alpha = 0.6 if captures else 1.0\n    \n    plt.plot([lower, upper], [i, i], color=color, linewidth=2, alpha=alpha)\n    plt.plot((lower + upper)/2, i, 'o', color=color, markersize=4)\n\nplt.xlabel('Value')\nplt.ylabel('Simulation Number')\nplt.title(f'First 20 Confidence Intervals\\n{confidence_level*100}% Confidence Level')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nSimulation Results:\nTrue population mean: 50\nSample size: 30\nConfidence level: 95.0%\nNumber of simulations: 100\nCoverage rate: 96.0%\nExpected coverage rate: 95.0%"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#summary-statistics-functions",
    "href": "files/labs/lab5/lab5_sln.html#summary-statistics-functions",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "Summary Statistics Functions",
    "text": "Summary Statistics Functions\n\n# Useful functions for continuous distributions\ndef distribution_summary(dist, dist_name):\n    \"\"\"\n    Print summary statistics for a distribution\n    \"\"\"\n    print(f\"\\n{dist_name} Distribution Summary:\")\n    print(\"=\" * 40)\n    print(f\"Mean: {dist.mean():.3f}\")\n    print(f\"Standard deviation: {dist.std():.3f}\")\n    print(f\"Variance: {dist.var():.3f}\")\n    \n    # Key percentiles\n    percentiles = [0.05, 0.25, 0.50, 0.75, 0.95]\n    print(\"\\nKey Percentiles:\")\n    for p in percentiles:\n        value = dist.ppf(p)\n        print(f\"{p*100:4.0f}th percentile: {value:.3f}\")\n\n# Example usage\nnormal_example = stats.norm(100, 15)\nuniform_example = stats.uniform(0, 100)\nexponential_example = stats.expon(scale=5)\n\ndistribution_summary(normal_example, \"Normal(100, 15)\")\ndistribution_summary(uniform_example, \"Uniform(0, 100)\")\ndistribution_summary(exponential_example, \"Exponential(scale=5)\")\n\n\nNormal(100, 15) Distribution Summary:\n========================================\nMean: 100.000\nStandard deviation: 15.000\nVariance: 225.000\n\nKey Percentiles:\n   5th percentile: 75.327\n  25th percentile: 89.883\n  50th percentile: 100.000\n  75th percentile: 110.117\n  95th percentile: 124.673\n\nUniform(0, 100) Distribution Summary:\n========================================\nMean: 50.000\nStandard deviation: 28.868\nVariance: 833.333\n\nKey Percentiles:\n   5th percentile: 5.000\n  25th percentile: 25.000\n  50th percentile: 50.000\n  75th percentile: 75.000\n  95th percentile: 95.000\n\nExponential(scale=5) Distribution Summary:\n========================================\nMean: 5.000\nStandard deviation: 5.000\nVariance: 25.000\n\nKey Percentiles:\n   5th percentile: 0.256\n  25th percentile: 1.438\n  50th percentile: 3.466\n  75th percentile: 6.931\n  95th percentile: 14.979"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#key-formulas-reference",
    "href": "files/labs/lab5/lab5_sln.html#key-formulas-reference",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "Key Formulas Reference",
    "text": "Key Formulas Reference\n\n# Reference: Important formulas and calculations\n\nprint(\"KEY FORMULAS FOR CONTINUOUS DISTRIBUTIONS\")\nprint(\"=\" * 50)\n\nprint(\"\\n1. CONFIDENCE INTERVAL FOR MEAN (σ unknown):\")\nprint(\"   CI = x̄ ± t* × (s/√n)\")\nprint(\"   Where:\")\nprint(\"   - x̄ = sample mean\")\nprint(\"   - t* = critical value from t-distribution\")\nprint(\"   - s = sample standard deviation\") \nprint(\"   - n = sample size\")\n\nprint(\"\\n2. STANDARDIZATION (Z-SCORE):\")\nprint(\"   Z = (X - μ) / σ\")\nprint(\"   Where:\")\nprint(\"   - X = raw score\")\nprint(\"   - μ = population mean\")\nprint(\"   - σ = population standard deviation\")\n\nprint(\"\\n3. CENTRAL LIMIT THEOREM:\")\nprint(\"   X̄ ~ Normal(μ, σ/√n)\")\nprint(\"   Where:\")\nprint(\"   - X̄ = sample mean\")\nprint(\"   - μ = population mean\")\nprint(\"   - σ = population standard deviation\")\nprint(\"   - n = sample size\")\n\nprint(\"\\n4. MARGIN OF ERROR:\")\nprint(\"   ME = z* × (σ/√n)  or  ME = t* × (s/√n)\")\nprint(\"   Where:\")\nprint(\"   - z* or t* = critical value\")\nprint(\"   - σ or s = standard deviation\")\nprint(\"   - n = sample size\")\n\nprint(\"\\n5. SAMPLE SIZE FOR DESIRED MARGIN OF ERROR:\")\nprint(\"   n = (z* × σ / ME)²\")\nprint(\"   Where:\")\nprint(\"   - z* = critical value\") \nprint(\"   - σ = population standard deviation\")\nprint(\"   - ME = desired margin of error\")\n\n# Example calculations\nprint(\"\\n\" + \"=\"*50)\nprint(\"EXAMPLE CALCULATIONS:\")\nprint(\"=\"*50)\n\n# Example: Sample size calculation\ndesired_me = 2  # Want margin of error of 2 inches\nsigma_est = 4   # Estimated standard deviation\nconfidence = 0.95\nz_star = stats.norm.ppf(1 - (1-confidence)/2)\n\nrequired_n = (z_star * sigma_est / desired_me)**2\nprint(f\"\\nSample size needed for ME = {desired_me}:\")\nprint(f\"n = ({z_star:.2f} × {sigma_est} / {desired_me})² = {required_n:.0f}\")\n\nKEY FORMULAS FOR CONTINUOUS DISTRIBUTIONS\n==================================================\n\n1. CONFIDENCE INTERVAL FOR MEAN (σ unknown):\n   CI = x̄ ± t* × (s/√n)\n   Where:\n   - x̄ = sample mean\n   - t* = critical value from t-distribution\n   - s = sample standard deviation\n   - n = sample size\n\n2. STANDARDIZATION (Z-SCORE):\n   Z = (X - μ) / σ\n   Where:\n   - X = raw score\n   - μ = population mean\n   - σ = population standard deviation\n\n3. CENTRAL LIMIT THEOREM:\n   X̄ ~ Normal(μ, σ/√n)\n   Where:\n   - X̄ = sample mean\n   - μ = population mean\n   - σ = population standard deviation\n   - n = sample size\n\n4. MARGIN OF ERROR:\n   ME = z* × (σ/√n)  or  ME = t* × (s/√n)\n   Where:\n   - z* or t* = critical value\n   - σ or s = standard deviation\n   - n = sample size\n\n5. SAMPLE SIZE FOR DESIRED MARGIN OF ERROR:\n   n = (z* × σ / ME)²\n   Where:\n   - z* = critical value\n   - σ = population standard deviation\n   - ME = desired margin of error\n\n==================================================\nEXAMPLE CALCULATIONS:\n==================================================\n\nSample size needed for ME = 2:\nn = (1.96 × 4 / 2)² = 15"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#bonus-practice-problems",
    "href": "files/labs/lab5/lab5_sln.html#bonus-practice-problems",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "Bonus Practice Problems",
    "text": "Bonus Practice Problems\n\n# Additional practice problems with solutions\n\nprint(\"BONUS PRACTICE PROBLEMS\")\nprint(\"=\" * 30)\n\nprint(\"\\nProblem 1: SAT Scores\")\nprint(\"SAT scores are normally distributed with mean=1200, std=200\")\n\nsat_scores = stats.norm(1200, 200)\n\n# Questions and solutions\nq1_score = 1400\nprob_above_1400 = 1 - sat_scores.cdf(q1_score)\nprint(f\"P(SAT &gt; {q1_score}) = {prob_above_1400:.4f}\")\n\nq2_lower, q2_upper = 1000, 1400\nprob_between = sat_scores.cdf(q2_upper) - sat_scores.cdf(q2_lower)\nprint(f\"P({q2_lower} &lt; SAT &lt; {q2_upper}) = {prob_between:.4f}\")\n\ntop_10_percent = sat_scores.ppf(0.90)\nprint(f\"Score for top 10%: {top_10_percent:.0f}\")\n\nprint(\"\\nProblem 2: Quality Control\")\nprint(\"Light bulb lifetimes follow exponential distribution, mean=1000 hours\")\n\nbulb_life = stats.expon(scale=1000)\n\nwarranty_period = 500\nprob_warranty_claim = bulb_life.cdf(warranty_period)\nprint(f\"P(bulb fails within {warranty_period} hours) = {prob_warranty_claim:.4f}\")\n\nlong_life = 2000\nprob_long_life = 1 - bulb_life.cdf(long_life)\nprint(f\"P(bulb lasts &gt; {long_life} hours) = {prob_long_life:.4f}\")\n\nprint(\"\\nProblem 3: Manufacturing Process\")\nprint(\"Widget weights: Normal(500g, 50g). Control limits at ±3σ\")\n\nwidget_weights = stats.norm(500, 50)\nlower_limit = 500 - 3*50\nupper_limit = 500 + 3*50\n\nprob_in_control = widget_weights.cdf(upper_limit) - widget_weights.cdf(lower_limit)\nprint(f\"P(weight within control limits) = {prob_in_control:.4f}\")\nprint(f\"P(out of control) = {1 - prob_in_control:.4f}\")\n\nBONUS PRACTICE PROBLEMS\n==============================\n\nProblem 1: SAT Scores\nSAT scores are normally distributed with mean=1200, std=200\nP(SAT &gt; 1400) = 0.1587\nP(1000 &lt; SAT &lt; 1400) = 0.6827\nScore for top 10%: 1456\n\nProblem 2: Quality Control\nLight bulb lifetimes follow exponential distribution, mean=1000 hours\nP(bulb fails within 500 hours) = 0.3935\nP(bulb lasts &gt; 2000 hours) = 0.1353\n\nProblem 3: Manufacturing Process\nWidget weights: Normal(500g, 50g). Control limits at ±3σ\nP(weight within control limits) = 0.9973\nP(out of control) = 0.0027"
  },
  {
    "objectID": "files/labs/lab5/lab5_sln.html#lab-5-complete-solutions-summary",
    "href": "files/labs/lab5/lab5_sln.html#lab-5-complete-solutions-summary",
    "title": "Lab 5 Solutions: Continuous Random Variables & Confidence Intervals",
    "section": "🎯 Lab 5 Complete Solutions Summary",
    "text": "🎯 Lab 5 Complete Solutions Summary\n\n✅ Task 1: Normal distribution calculations with human heights\n✅ Task 2: Exponential distribution for bus waiting times\n\n✅ Task 3: Central Limit Theorem verification with uniform distribution\n✅ Task 4: 90% confidence interval construction for homework data\n✅ Task 5: Distribution matching exercise with reasoning\n\nKey Takeaways:\n\nContinuous distributions use PDFs and calculate probabilities as areas\nNormal distribution is fundamental and appears everywhere via CLT\nConfidence intervals provide ranges of plausible values for parameters\nSample size affects precision; confidence level affects interval width\nPython’s scipy.stats provides powerful tools for distribution analysis"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#from-observation-to-experimentation",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#from-observation-to-experimentation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "From Observation to Experimentation",
    "text": "From Observation to Experimentation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#building-confidence-intervals-step-by-step",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#building-confidence-intervals-step-by-step",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Building Confidence Intervals Step-by-Step",
    "text": "Building Confidence Intervals Step-by-Step\n\n\nFor Population Means (Most Common Case)\nWhen we DON’T know the population standard deviation (\\(\\sigma\\)):\n\n\n                            \n                                            \n\n\n\nThe Formula: \\(\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\)\nBreaking it down:\n\n\\(\\bar{x}\\) = our sample average (the center of our guess)\n\\(t^*\\) = critical value (how many standard errors to go out)\n\\(\\frac{s}{\\sqrt{n}}\\) = standard error (our uncertainty measure)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#the-t-distribution-when-σ-is-unknown",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#the-t-distribution-when-σ-is-unknown",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The t-Distribution: When σ is Unknown",
    "text": "The t-Distribution: When σ is Unknown\n\n\nWhy not use the normal distribution? Because when we estimate \\(\\sigma\\) with \\(s\\), we add extra uncertainty!\n\n\n                            \n                                            \n\n\n\nKey Points:\n\nSmall samples (\\(n &lt; 30\\)): Use t-distribution\nLarge samples (\\(n ≥ 30\\)): \\(t\\) ≈ normal\nDegrees of freedom (df)= \\(n - 1\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\n\nFor Yes/No questions like: “What percentage of students prefer online classes?”\n\n\n                            \n                                            \n\n\n\nThe Formula: \\(\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nResults: Sample: 60% prefer online (120/200)\n95% CI: (53.2%, 66.8%) - We’re 95% confident the true percentage is in this range."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#what-does-95-confident-really-mean",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#what-does-95-confident-really-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "What Does “95% Confident” Really Mean? 🤔",
    "text": "What Does “95% Confident” Really Mean? 🤔\n\n\nThe Biggest Misconception: “There’s a 95% chance the true mean is in our interval”\nActually: “If we repeated this study 100 times, about 95 of our intervals would contain the true mean”\n\n\n                            \n                                            \n\n\n\nRemember: The interval either contains the true value or it doesn’t - there’s no probability involved for a single interval!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#sample-size-planning-getting-the-precision-you-want",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#sample-size-planning-getting-the-precision-you-want",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size Planning: Getting the Precision You Want",
    "text": "Sample Size Planning: Getting the Precision You Want\n\n\nThe Question: “How many people do we need to survey?”\n\n\n                            \n                                            \n\n\n\nKey Formula for Means: \\(n = \\left(\\frac{z^* \\sigma}{ME}\\right)^2\\)\nTrade-offs: - Want smaller margin of error? Need bigger sample - Want higher confidence? Need bigger sample\n- Want to save money? Accept wider intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#real-example-student-sleep-study",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#real-example-student-sleep-study",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Real Example: Student Sleep Study 😴",
    "text": "Real Example: Student Sleep Study 😴\n\n\nResearch Question: How many hours do UCSB students sleep per night?\n\n\n                            \n                                            \n\n\n\nBottom Line: We’re 95% confident that UCSB students sleep between 6.73 and 7.47 hours per night on average."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#common-mistakes-to-avoid",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#common-mistakes-to-avoid",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes to Avoid ⚠️",
    "text": "Common Mistakes to Avoid ⚠️\n\n\n❌ Wrong Interpretations\n“95% of students sleep in this range” - NO! This is about the population mean, not individual students\n“There’s a 95% chance μ is in our interval” - NO! μ is fixed; our interval varies\n“We can be 95% certain” - NO! Use “confident” not “certain”\n\n✅ Correct Approach\n“We are 95% confident the population mean is in this interval”\nKey Reminders: - Check conditions before using formulas - Use t-distribution when σ is unknown - Larger samples give narrower intervals - Higher confidence gives wider intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#practice-problems",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#practice-problems",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problems 📝",
    "text": "Practice Problems 📝\n\n\nProblem 1: Coffee Shop Revenue\nA coffee shop owner samples 36 days and finds average daily revenue of $850 with standard deviation $120.\nYour turn: Calculate a 90% confidence interval for the true average daily revenue.\n\nSolution. Given: n = 36, x̄ = $850, s = $120, 90% confidence\nStep 1: Check conditions ✓ (large sample)\nStep 2: Find critical value\ndf = 35, t* = 1.690 (90% confidence)\nStep 3: Calculate standard error\nSE = 120/√36 = 120/6 = $20\nStep 4: Build interval\nCI = 850 ± 1.690 × 20 = 850 ± 33.8 = ($816.20, $883.80)\nInterpretation: We are 90% confident the true average daily revenue is between $816.20 and $883.80.\n\n\nProblem 2: Student Survey\nIn a survey of 400 students, 280 say they would recommend their major to a friend.\nYour turn: 1. Calculate the sample proportion 2. Build a 95% confidence interval\n3. Check if conditions are met\n\n\n\n\n\n\nSolution\n\n\nStep 1: Sample proportion\np̂ = 280/400 = 0.70 (70%)\nStep 2: Check conditions\nnp̂ = 400(0.70) = 280 ≥ 10 ✓\nn(1-p̂) = 400(0.30) = 120 ≥ 10 ✓\nStep 3: Build CI\nSE = √[0.70(0.30)/400] = √[0.000525] = 0.0229\nz* = 1.96 (95% confidence)\nCI = 0.70 ± 1.96(0.0229) = 0.70 ± 0.045 = (0.655, 0.745)\nInterpretation: We are 95% confident that between 65.5% and 74.5% of all students would recommend their major."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#the-t-distribution-when-sigma-is-unknown",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#the-t-distribution-when-sigma-is-unknown",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The t-Distribution: When \\(\\sigma\\) is Unknown",
    "text": "The t-Distribution: When \\(\\sigma\\) is Unknown\n\n\nWhy not use the normal distribution? Because when we estimate \\(\\sigma\\) with \\(s\\), we add extra uncertainty!\n\n\n                            \n                                            \n\n\n\nKey Points:\n\nSmall samples (\\(n &lt; 30\\)): Use t-distribution\nLarge samples (\\(n ≥ 30\\)): \\(t\\) ≈ normal\nDegrees of freedom (df)= \\(n - 1\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#standard-error",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#standard-error",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Standard Error",
    "text": "Standard Error\n\n\nWhat it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#announcements",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#announcements",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "📢 Announcements",
    "text": "📢 Announcements\n📝 Quiz 2\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#problem-1-coffee-shop-revenue",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#problem-1-coffee-shop-revenue",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Problem 1: Coffee Shop Revenue",
    "text": "Problem 1: Coffee Shop Revenue\nA coffee shop owner samples 36 days and finds average daily revenue of $850 with standard deviation $120.\nYour turn: Calculate a 90% confidence interval for the true average daily revenue.\n\n\nShow Solution\n\n\nGiven (from the prompt)\n\\(n = 36,\\; \\bar{x} = \\$850,\\; s = \\$120,\\; \\text{confidence level} = 90\\%\\)\n\nStep 1 – Conditions\n\n\\(n \\ge 30\\) ⇒ a \\(t\\)‑interval is justified by the Central Limit Theorem.\n\nAssume daily revenues are independent.\n\nStep 2 – Critical value\n\\(\\alpha = 1-0.90 = 0.10 \\;\\Rightarrow\\; \\alpha/2 = 0.05\\)\nDegrees of freedom: \\(df = n-1 = 35\\)\n\\(\\displaystyle t^{\\star}_{0.90,\\,35} \\approx 1.690\\)\nStep 3 – Standard error\n\\[SE = \\frac{s}{\\sqrt{n}}\n        = \\frac{120}{\\sqrt{36}}\n        = \\frac{120}{6}\n        = \\$20\\]\nStep 4 – Margin of error\n\\[ME = t^{\\star}\\; SE\n       = 1.690 \\times \\$20\n       = \\$33.8\\]\nStep 5 – Confidence interval\n\\[\\bar{x} \\pm ME\n     = 850 \\pm 33.8\n     \\;\\Longrightarrow\\;\n     (\\$816.2,\\; \\$883.8)\\]\nInterpretation – We are 90 % confident that the true mean daily revenue lies between $816.20 and $883.80."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#problem-2-student-survey",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#problem-2-student-survey",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Problem 2: Student Survey",
    "text": "Problem 2: Student Survey\nIn a survey of 400 students, 280 say they would recommend their major to a friend.\nYour turn:\n\nCalculate the sample proportion\nBuild a \\(95\\%\\) confidence interval\nCheck if conditions are met\n\n\n\nShow Solution\n\n\nGiven (from the survey)\n\\(n = 400,\\; x = 280\\) “yes” responses\n\nStep 1 – Sample proportion\n\\[\\hat{p} = \\frac{x}{n} = \\frac{280}{400} = 0.70\\]\nStep 2 – Conditions for a \\(z\\)‑interval\n\\(n\\hat{p} = 400(0.70)=280 \\ge 10\\)\n\\(n(1-\\hat{p}) = 400(0.30)=120 \\ge 10\\)\nBoth counts ≥ 10, so the normal approximation is appropriate.\nStep 3 – Standard error\n\\[SE = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n        = \\sqrt{\\frac{0.70(0.30)}{400}}\n        = \\sqrt{0.000525}\n        \\approx 0.0229\\]\nStep 4 – Critical value & margin of error\nFor 95 % confidence, \\(z^{\\star} = 1.96\\)\n\\[ME = z^{\\star}\\; SE\n       = 1.96 \\times 0.0229\n       \\approx 0.045\\]\nStep 5 – Confidence interval\n\\[\\hat{p} \\pm ME\n     = 0.70 \\pm 0.045\n     \\;\\Longrightarrow\\;\n     (0.655,\\; 0.745)\\]\nInterpretation – We are 95 % confident that between 65.5 % and 74.5 % of all students would recommend their major to a friend."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "From Observation ➡️ Experimentation: Why Design Matters",
    "text": "From Observation ➡️ Experimentation: Why Design Matters\n\n\nObservational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#what-exactly-is-a-confidence-interval",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#what-exactly-is-a-confidence-interval",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "What Exactly Is a Confidence Interval? 🤓",
    "text": "What Exactly Is a Confidence Interval? 🤓\n\n\n\nA confidence interval (CI) is point estimate \\(\\pm\\) margin of error\n\\[\n  \\text{CI} = \\text{statistic} \\;\\pm\\; \\bigl(\\text{critical value}\\bigr)\\times\\bigl(\\text{SE}\\bigr)\n\\]\nThe “critical value” comes from a probability model (e.g., \\(z^{\\star}\\) or \\(t^{\\star}\\)).\nThe standard error (SE) captures sampling variation.\n\nFrequentist meaning\n\nIf we repeated the study infinitely many times and built a \\(95 \\%\\) CI each time, about \\(95 \\%\\) of those intervals would cover the true parameter.\n\n(For any one computed interval the parameter is fixed, the process has a \\(95 \\%\\) success rate, not the individual interval.)\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\n\nWhat controls the width?\n\n\n\nVariability in the data: larger \\(\\sigma\\) or \\(s\\) ⇒ wider CI\nSample size \\(n\\): width shrinks at rate \\(1/\\sqrt{n}\\)\nConfidence level: 99 % CIs are wider than 90 % CIs\n\n\n\n\n\n\n\n\n\n\nCommon pitfalls\n\n\n\nSaying “there is a \\(95 \\%\\) probability that \\(\\mu\\) lies in this interval” (wrong)\nInterpreting the CI as covering \\(95 \\%\\) of future observations (it does not)\nIgnoring conditions (normality or CLT) before using the formulae"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-the-intuitive-idea-1",
    "href": "files/lecture_notes/lecture9/lecture9_intro.html#confidence-intervals-the-intuitive-idea-1",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Intuitive Idea",
    "text": "Confidence Intervals: The Intuitive Idea\n\n\n\n\n                            \n                                            \n\n\n\nWhat Exactly Is a Confidence Interval? 🤓\n\n\nA confidence interval (CI) is point estimate \\(\\pm\\) margin of error\n\\[\n  \\text{CI} = \\text{statistic} \\;\\pm\\; \\bigl(\\text{critical value}\\bigr)\\times\\bigl(\\text{SE}\\bigr)\n\\]\nThe “critical value” comes from a probability model (e.g., \\(z^{\\star}\\) or \\(t^{\\star}\\)).\nThe standard error (SE) captures sampling variation.\n\nFrequentist meaning\n\nIf we repeated the study infinitely many times and built a \\(95 \\%\\) CI each time, about \\(95 \\%\\) of those intervals would cover the true parameter.\n\n(For any one computed interval the parameter is fixed, the process has a \\(95 \\%\\) success rate, not the individual interval.)\n\n\n\n\n\n\n\nWhat controls the width?\n\n\n\nVariability in the data: larger \\(\\sigma\\) or \\(s\\) ⇒ wider CI\nSample size \\(n\\): width shrinks at rate \\(1/\\sqrt{n}\\)\nConfidence level: 99 % CIs are wider than 90 % CIs\n\n\n\n\n\n\n\n\n\n\nCommon pitfalls\n\n\n\nSaying “there is a \\(95 \\%\\) probability that \\(\\mu\\) lies in this interval” (wrong)\nInterpreting the CI as covering \\(95 \\%\\) of future observations (it does not)\nIgnoring conditions (normality or CLT) before using the formulae"
  },
  {
    "objectID": "files/lecture_notes/lecture11/lecture11.html#announcements",
    "href": "files/lecture_notes/lecture11/lecture11.html#announcements",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "📢 Announcements",
    "text": "📢 Announcements\n📝 Quiz 2\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/worksheets/worksheet5.html",
    "href": "files/worksheets/worksheet5.html",
    "title": "PSTAT 5A Practice Worksheet 5",
    "section": "",
    "text": "Instructions and Overview\n⏰ Time Allocation:\n\nIntro & Setup : 10 minutes\nSection A (Continuous Distributions): 20 minutes\nSection B (Confidence Intervals): 20 minutes\nOptional Questions: Do on your own\nTotal: 50 minutes\n\n\n📝 Important Instructions:\n\nUse the formulas and tables provided for guidance\nRound final answers to 4 decimal places unless otherwise specified\nFor confidence intervals, always interpret your results in context\nUse z-table or t-table as appropriate\nShow your work for all calculations\n\n\n\n📚 Key Formulas Reference:\nContinuous Random Variables:\nNormal Distribution: \\(X \\sim N(\\mu, \\sigma^2)\\)\n\nPDF: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\)\nStandardization: \\(Z = \\frac{X - \\mu}{\\sigma}\\) where \\(Z \\sim N(0,1)\\)\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\)\n\nUniform Distribution: \\(X \\sim \\text{Uniform}(a,b)\\)\n\nPDF: \\(f(x) = \\frac{1}{b-a}\\) for \\(a \\leq x \\leq b\\)\nMean: \\(E[X] = \\frac{a+b}{2}\\)\nVariance: \\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\n\nExponential Distribution: \\(X \\sim \\text{Exponential}(\\lambda)\\)\n\nPDF: \\(f(x) = \\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\)\nMean: \\(E[X] = \\frac{1}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\n\nConfidence Intervals:\nFor Population Mean (σ known): \\(\\bar{x} \\pm z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\nFor Population Mean (σ unknown): \\(\\bar{x} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\\)\nMargin of Error: \\(E = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\) or \\(E = t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\\)\nSample Size: \\(n = \\left(\\frac{z_{\\alpha/2} \\cdot \\sigma}{E}\\right)^2\\)\n\n\n\nSection A: Continuous Random Variables\n⏱️ Estimated time: 20 minutes\n\nProblem A1: Distribution Identification and Properties\nFor each scenario below, identify the appropriate continuous distribution and find the requested values:\n(a) The time (in minutes) between arrivals at a coffee shop follows an exponential distribution with an average of 2 minutes between arrivals.\n\nWhat is the parameter \\(\\lambda\\)?\nWhat is the probability that the next customer arrives within 1 minute?\n\n(b) A random number generator produces values uniformly between 10 and 30.\n\nWhat are the parameters a and b?\nWhat is the expected value and variance?\n\n\nWork Space:\n\n\nProblem A2: Normal Distribution Calculations\nThe heights of adult women in the US are normally distributed with \\(\\mu = 64\\) inches and \\(\\sigma = 2.5\\) inches.\n(a) What is the probability that a randomly selected woman is taller than \\(67\\) inches?\n(b) What height represents the \\(25\\)th percentile?\n(c) What is the probability that a randomly selected woman has a height between \\(62\\) and \\(68\\) inches?\n\n\n\n\n\n\nTip\n\n\n\nRemember to standardize: Convert to \\(Z\\)-scores using \\(Z = \\frac{X - \\mu}{\\sigma}\\)\nFor part (b), you’re looking for the value \\(x\\) such that \\(P(X ≤ x) = 0.25\\)\n\n\n\nWork Space:\n\n\n\nSection B: Confidence Intervals\n⏱️ Estimated time: 20 minutes\n\nProblem B1: Understanding Confidence Intervals\n(a) Explain in your own words what a \\(95\\%\\) confidence interval means.\n(b) A \\(90\\%\\) confidence interval for the mean weight of apples is (150g, 170g). What is the sample mean and margin of error?\n(c) True or False: “There is a \\(95\\%\\) probability that the population mean lies within our calculated \\(95\\%\\) confidence interval.” Explain your reasoning.\n\nWork Space:\n\n\nProblem B2: Constructing Confidence Intervals\nA sample of \\(36\\) students has a mean test score of \\(78.5\\) with a standard deviation of \\(12\\).\n(a) Construct a \\(95\\%\\) confidence interval for the population mean test score.\n(b) Interpret this interval in the context of the problem.\n(c) What would happen to the width of the interval if:\n\nWe increased the confidence level to \\(99\\%\\)?\nWe increased the sample size to \\(144\\)?\n\n\n\n\n\n\n\nTip\n\n\n\nDecision Guide:\n\nUse \\(z\\)-distribution when \\(\\sigma\\) is known OR \\(n ≥ 30\\)\nUse \\(t\\)-distribution when \\(\\sigma\\) is unknown AND \\(n &lt; 30\\)\nFor \\(95\\%\\) CI: \\(z_{0.025} = 1.96\\)\n\n\n\n\nWork Space:\n\n\n\nOptional Questions\n\nOptional Problem: Conceptual Understanding\n(a) Explain the key difference between discrete and continuous random variables in terms of:\n\nThe values they can take\nHow we calculate probabilities\n\n(b) Why do we use \\(P(X = x) = 0\\) for any specific value \\(x\\) in a continuous distribution?\n(c) What’s the relationship between PDF and CDF for continuous distributions?\n\nWork Space:\n\n\n📋 Quick Reference:\nCommon Z-values:\n\n\\(90\\%\\) CI: \\(z_{0.05} = 1.645\\)\n\\(95\\%\\) CI: \\(z_{0.025}\\) = 1.96$\n\\(99\\%\\) CI: \\(z_{0.005}\\) = 2.576$\n\nCommon t-values (selected):\n\n\\(df = 24, \\alpha = 0.05: t_{0.025} = 2.064\\)\n\\(df = 35, \\alpha = 0.05: t_{0.025} = 2.030\\)"
  },
  {
    "objectID": "files/worksheets/worksheet5_sln.html",
    "href": "files/worksheets/worksheet5_sln.html",
    "title": "PSTAT 5A Practice Worksheet 5 - SOLUTIONS",
    "section": "",
    "text": "Section A Solutions: Continuous Random Variables\n\nSolution. Solution A1: Distribution Identification and Properties\n(a) Exponential Distribution\nSince the average time between arrivals is \\(2\\) minutes, we have:\n\nParameter \\(\\lambda\\): The rate parameter \\(\\lambda = \\frac{1}{\\mu} = \\frac{1}{2} = 0.5\\) arrivals per minute\nProbability calculation: \\(P(X ≤ 1)\\) where \\(X \\sim Exponential(0.5)\\)\nFor exponential distribution: \\(P(X ≤ x) = 1 - e^{(- \\lambda x)}\\)\n\\(P(X ≤ 1) = 1 - e^{(-0.5×1)} = 1 - e^{(-0.5)} = 1 - 0.6065 = \\boxed{0.3935}\\)\n\n(b) Uniform Distribution\n\nParameters: \\(a = 10, b = 30\\)\nExpected Value: \\(E[X] = (a + b)/2 = \\frac{(10 + 30)}{2} = \\boxed{20}\\)\nVariance: \\(Var(X) = \\frac{(b - a)^2}{12} = \\frac{(30 - 10)^2}{12} = \\frac{400}{12} = \\boxed{33.3333}\\)\n\n\n\nSolution. Solution A2: Normal Distribution Calculations\nGiven: \\(X \\sim N(64, 2.5^2)\\)\n(a) \\(P(X &gt; 67)\\)\nStep 1: Standardize\n\\(Z = (67 - 64)/2.5 = 3/2.5 = 1.2\\)\nStep 2: Find probability\n\\(P(X &gt; 67) = P(Z &gt; 1.2) = 1 - P(Z ≤ 1.2) = 1 - 0.8849 = \\boxed{0.1151}\\)\n(b) 25th percentile\nStep 1: Find \\(z\\)-value for \\(25\\) -th percentile\n\\(P(Z ≤ z) = 0.25\\), so \\(z_{0.25} = -0.6745\\)\nStep 2: Convert back to \\(X\\)\n\\(x = \\mu + z \\sigma = 64 + (-0.6745)(2.5) = 64 - 1.6863 = \\boxed{62.3137} \\quad \\text{inches}\\)\n(c) P(62 &lt; X &lt; 68)\nStep 1: Standardize both values\n\\(Z_1 = (62 - 64)/2.5 = -0.8\\) \\(Z_2 = (68 - 64)/2.5 = 1.6\\)\nStep 2: Find probability\n\\(P(62 &lt; X &lt; 68) = P(-0.8 &lt; Z &lt; 1.6) = P(Z &lt; 1.6) - P(Z &lt; -0.8)\\)\n\\(= 0.9452 - 0.2119 = \\boxed{0.7333}\\)\n\n\n\nSection B Solutions: Confidence Intervals\n\nSolution. Solution B1: Understanding Confidence Intervals\n(a) Explanation of \\(95\\%\\) Confidence Interval:\nA \\(95\\%\\) confidence interval means that if we were to repeat our sampling process many times (say \\(100\\) times) and construct a confidence interval each time using the same method, approximately \\(95\\) of those intervals would contain the true population mean. It does NOT mean there’s a \\(95\\%\\) probability that the population mean lies in any one specific interval.\n(b) Sample mean and margin of error:\nGiven \\(CI\\): (\\(150g, 170g\\))\n\nSample mean: \\(\\bar x = (150 + 170)/2 = \\boxed{160g}\\)\nMargin of error: \\(E = (170 - 150)/2 = \\boxed{10g}\\)\n\n(c) True or False statement:\nFALSE. Once we calculate a specific confidence interval, the population mean either is or isn’t in that interval, there’s no probability involved for that specific interval. The \\(95\\%\\) refers to the long-run success rate of the method, not the probability for any individual interval.\n\n\nSolution. Solution B2: Constructing Confidence Intervals\nGiven: \\(n = 36, \\bar x = 78.5, s = 12\\)\n(a) 95% Confidence Interval:\nStep 1: Check conditions\n\n\\(n = 36 ≥ 30\\), so we can use \\(z\\)-distribution\nFor \\(95% CI: z{0.025} = 1.96\\)\n\nStep 2: Calculate margin of error\n\\(E = z_{0.025} × (\\frac{s}{\\sqrt{n}}) = 1.96 × (\\frac{12}{\\sqrt{36}}) = 1.96 × (\\frac{12}{6}) = 1.96 × 2 = 3.92\\)\nStep 3: Construct interval\n\\(CI = \\bar x ± E = 78.5 ± 3.92 = \\boxed{(74.58, 82.42)}\\)\n(b) Interpretation:\nWe are \\(95\\%\\) confident that the true population mean test score is between \\(74.58\\) and \\(82.42\\) points.\n(c) Effects on interval width:\n\nIncreasing confidence level to 99%: The interval would become wider because we need \\(z_{0.005} = 2.576 &gt; 1.96\\)\nIncreasing sample size to 144: The interval would become narrower because the margin of error would be \\(E = 1.96 × (\\frac{12}{\\sqrt{144}}) = 1.96 × 1 = 1.96\\) (smaller than \\(3.92\\))\n\n\n\nSolution. Solution B3: Sample Size Determination\nGiven: \\(E = \\$5\\), confidence = \\(95\\%, \\sigma = \\$25\\)\n(a) Required sample size:\nStep 1: Use sample size formula\n\\(n = (z_{0.025} × \\frac{\\sigma}{E})^2\\)\nStep 2: Substitute values\n\\(n = (1.96 × 25 / 5)^2 = (49/5)^2 = 9.8^2 = 96.04\\)\nStep 3: Round up\n\\(\\boxed{n = 97}\\) customers (always round up for sample size)\n(b) For margin of error = $3:\n\\(n = (1.96 × 25 / 3)^2 = (49/3)^2 = 16.333^2 = 266.67\\)\n\\(\\boxed{n = 267}\\) customers\n\n\n\nOptional Problem Solutions\n\nSolution. Optional Solution 1: Conceptual Understanding\n(a) Differences between discrete and continuous:\nValues they can take:\n\nDiscrete: Countable values (integers, specific points)\nContinuous: Uncountably infinite values (any real number in an interval)\n\nHow we calculate probabilities:\n\nDiscrete: \\(P(X = x)\\) can be non-zero; we sum probabilities\nContinuous: \\(P(X = x) = 0\\) for any specific \\(x\\); we integrate over intervals\n\n(b) Why \\(P(X = x) = 0\\) for continuous distributions:\nIn continuous distributions, there are infinitely many possible values in any interval. The probability of hitting any one exact value is infinitesimally small, hence zero. We instead calculate \\(P(a &lt; X &lt; b)\\) by integrating the PDF over the interval \\([a,b]\\).\n(c) Relationship between PDF and CDF:\n\nPDF (f(x)): The probability density function gives the “density” of probability at each point\nCDF (F(x)): The cumulative distribution function gives \\(P(X ≤ x)\\)\nRelationship: \\(F(x) = \\int_{-∞}^x f(t)dt,\\quad  \\text{and} \\quad f(x) = F'(x)\\)\n\n\n\nKey Takeaways:\n\nAlways standardize normal distribution problems using \\(Z = \\frac{(X - μ)}{\\sigma}\\)\nInterpret confidence intervals in context, they’re about the method’s reliability, not individual interval probabilities\nChoose the right distribution use \\(t\\) when \\(\\sigma\\) is unknown and \\(n &lt; 30\\)\nRound up sample sizes to ensure you meet the margin of error requirement\nFor continuous distributions, focus on intervals, not individual points"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#announcements",
    "href": "files/worksheets/lecture9-part1.html#announcements",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "📢 Announcements",
    "text": "📢 Announcements\n📝 Quiz 2\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#what-well-learn-today",
    "href": "files/worksheets/lecture9-part1.html#what-well-learn-today",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "What We’ll Learn Today 🎯",
    "text": "What We’ll Learn Today 🎯\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#from-observation-experimentation-why-design-matters",
    "href": "files/worksheets/lecture9-part1.html#from-observation-experimentation-why-design-matters",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "From Observation ➡️ Experimentation: Why Design Matters",
    "text": "From Observation ➡️ Experimentation: Why Design Matters\n\n\nObservational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty."
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#the-big-picture-from-sample-to-population",
    "href": "files/worksheets/lecture9-part1.html#the-big-picture-from-sample-to-population",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "The Big Picture: From Sample to Population",
    "text": "The Big Picture: From Sample to Population\n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#why-point-estimates-arent-enough",
    "href": "files/worksheets/lecture9-part1.html#why-point-estimates-arent-enough",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Why Point Estimates Aren’t Enough",
    "text": "Why Point Estimates Aren’t Enough\n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values."
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#sampling-distributions",
    "href": "files/worksheets/lecture9-part1.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#the-central-limit-theorem-clt",
    "href": "files/worksheets/lecture9-part1.html#the-central-limit-theorem-clt",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "The Central Limit Theorem (CLT) 🎯",
    "text": "The Central Limit Theorem (CLT) 🎯\n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#standard-error",
    "href": "files/worksheets/lecture9-part1.html#standard-error",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Standard Error",
    "text": "Standard Error\n\n\nWhat it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\nDoubling your sample size doesn’t halve the error, you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#confidence-intervals-the-intuitive-idea",
    "href": "files/worksheets/lecture9-part1.html#confidence-intervals-the-intuitive-idea",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Confidence Intervals: The Intuitive Idea",
    "text": "Confidence Intervals: The Intuitive Idea\n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#key-takeaways",
    "href": "files/worksheets/lecture9-part1.html#key-takeaways",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Key Takeaways 🎯",
    "text": "Key Takeaways 🎯\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. The CLT For independent, identically distributed observations with finite mean \\(\\mu\\) and variance \\(\\sigma^{2}\\), the sampling distribution of the sample mean \\(\\bar X\\) becomes approximately normal with mean \\(\\mu\\) and standard deviation \\(\\sigma/\\sqrt{n}\\) as the sample size \\(n\\) grows large.\n\nPractical Skills:\n\nDraw a simple random sample (Python’s numpy.random, pandas.sample)\nCompute \\(\\bar{x}\\), \\(s\\), and the standard error \\(\\displaystyle SE = \\tfrac{s}{\\sqrt{n}}\\)\nFind the correct critical value (\\(z^*\\) or \\(t^*\\)) for a chosen confidence level"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#resources",
    "href": "files/worksheets/lecture9-part1.html#resources",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Resources",
    "text": "Resources\n\nRead OpenIntro Statistics 1.3 and 3.3"
  },
  {
    "objectID": "files/worksheets/lecture9-part1.html#questions",
    "href": "files/worksheets/lecture9-part1.html#questions",
    "title": "PSTAT 5A: Sampling principles and strategies",
    "section": "Questions? 🤔",
    "text": "Questions? 🤔\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#important-announcements",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#important-announcements",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "📢 Important Announcements",
    "text": "📢 Important Announcements\n\n\n📝 Quiz 2 Details\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: 7 AM – 12 AM\n- ⏳ Duration: 1 hour once started\nWhere: 💻 Online via Canvas\nCovers: Material from Weeks 3-4\n\n📚 What to Expect\n\nDiscrete & continuous distributions\nProbability calculations\nExpected value & variance\nNormal distribution applications\nNote: Upload photos of written work for calculation problems"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#todays-learning-journey",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#todays-learning-journey",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Today’s Learning Journey 🎯",
    "text": "Today’s Learning Journey 🎯\n\n\n🧠 Big Ideas We’ll Explore\n\nWhy sampling? The power and necessity of statistical inference\nSample behavior - How sample means form predictable patterns\nUncertainty quantification - From point estimates to intervals\nThe CLT magic - Why normal distributions appear everywhere\nConfidence intervals - Our bridge from samples to populations\n\n\n🛠️ Skills You’ll Master\n\nDesign effective sampling strategies\nCalculate and interpret standard errors\nApply the Central Limit Theorem\nConstruct and interpret confidence intervals\nChoose appropriate sample sizes for desired precision\nRecognize and avoid sampling bias"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#the-foundation-why-do-we-sample",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#the-foundation-why-do-we-sample",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "The Foundation: Why Do We Sample? 🤔",
    "text": "The Foundation: Why Do We Sample? 🤔\n\n\n🌍 Real-World Constraints\nPopulation vs. Sample Realities:\n\nTime: Surveying 40,000 UCSB students takes months\nCost: Each measurement costs money and resources\nLogistics: Some populations are impossible to reach entirely\nFeasibility: Testing every light bulb would destroy the product\n\n💡 The Statistical Solution\nUse a representative sample to make valid inferences about the entire population"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#study-design-the-foundation-of-good-inference",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#study-design-the-foundation-of-good-inference",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Study Design: The Foundation of Good Inference",
    "text": "Study Design: The Foundation of Good Inference\n\n\n                            \n                                            \n\n\n\n\n🔍 Observational Studies: - Observe what naturally occurs - Good for identifying associations - ⚠️ Cannot establish causation due to confounding\n\n🧪 Randomized Experiments: - Actively assign treatments randomly - Controls for confounding variables - ✅ Can establish causal relationships"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Types of Sampling Methods 🎯",
    "text": "Types of Sampling Methods 🎯\n\n📋 Probability Sampling Methods\n\n1. Simple Random Sampling (SRS)\nEvery individual has equal chance of selection\nGold standard for inference\n2. Stratified Sampling\nDivide population into groups (strata)\nSample randomly within each group\nEnsures representation of subgroups\n\n3. Cluster Sampling\nDivide into clusters, randomly select clusters\nSample all/some individuals within chosen clusters\nCost-effective for large populations\n4. Systematic Sampling\nSelect every \\(kth\\) individual from ordered list\nSimple but can introduce bias if pattern exists"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#sampling-bias-what-can-go-wrong",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#sampling-bias-what-can-go-wrong",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Sampling Bias: What Can Go Wrong? ⚠️",
    "text": "Sampling Bias: What Can Go Wrong? ⚠️\n\n\n🚨 Common Types of Bias\nSelection Bias - Systematic exclusion of certain groups - Example: Online surveys miss non-internet users\nResponse Bias\n- Who chooses to respond affects results - Example: Satisfaction surveys - unhappy customers more likely to respond\nNonresponse Bias - Missing data isn’t random - Example: Wealthy people less likely to disclose income\nConvenience Sampling - Sampling whoever is easiest to reach - Example: Surveying only students in your dorm\n\n\n\n                            \n                                            \n\n\n💡 Key Insight: Bias can’t be fixed by increasing sample size!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#the-magic-of-sample-means-from-chaos-to-order",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#the-magic-of-sample-means-from-chaos-to-order",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "The Magic of Sample Means: From Chaos to Order",
    "text": "The Magic of Sample Means: From Chaos to Order\n\n\n🎲 The Setup:\n\nTake many samples from the same population\nCalculate the mean of each sample\nPlot all these sample means\nObserve the magic!\n\n🎯 What We Discover:\n\nSample means cluster around the true population mean\nThey form a predictable pattern (normal distribution!)\nLarger samples give more consistent results"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#the-central-limit-theorem-natures-most-beautiful-pattern",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#the-central-limit-theorem-natures-most-beautiful-pattern",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "The Central Limit Theorem: Nature’s Most Beautiful Pattern 🌟",
    "text": "The Central Limit Theorem: Nature’s Most Beautiful Pattern 🌟\n\n\n📐 The Mathematical Statement\nFor a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), when sample size \\(n\\) is large enough:\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nOr equivalently: \\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\n✨ The Magic Rules\n\nRule of Thumb: \\(n \\geq 30\\) usually works\nShape doesn’t matter: Works for ANY population distribution\n\nLarger \\(n\\) = Better approximation"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#standard-error-measuring-our-uncertainty",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#standard-error-measuring-our-uncertainty",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Standard Error: Measuring Our Uncertainty 📏",
    "text": "Standard Error: Measuring Our Uncertainty 📏\n\n\n🎯 What is Standard Error?\nStandard Error (SE) measures how much sample means vary from sample to sample.\n\\[SE = \\frac{\\sigma}{\\sqrt{n}} \\text{ (when } \\sigma \\text{ is known)}\\]\n\\[SE = \\frac{s}{\\sqrt{n}} \\text{ (usual case, } \\sigma \\text{ unknown)}\\]\n🔍 Key Insights\n\nSmaller SE = More precise estimates\nSE decreases as sample size increases\nRate of decrease: \\(SE \\propto 1/\\sqrt{n}\\)\n4× larger sample = ½ the uncertainty!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#confidence-intervals-our-bridge-to-the-population",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#confidence-intervals-our-bridge-to-the-population",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Confidence Intervals: Our Bridge to the Population 🌉",
    "text": "Confidence Intervals: Our Bridge to the Population 🌉\n\n\n🎯 What Are Confidence Intervals?\nA confidence interval gives us a range of plausible values for the population parameter.\nFor a population mean: \\[\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n🔢 Common Confidence Levels\n\n90% CI: \\(z^* = 1.645\\)\n95% CI: \\(z^* = 1.96\\)\n99% CI: \\(z^* = 2.576\\)\n\n💭 Correct Interpretation\n“We are 95% confident that the true population mean lies between [lower bound] and [upper bound]”"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#sample-size-planning-getting-it-right",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#sample-size-planning-getting-it-right",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Sample Size Planning: Getting It Right 🎯",
    "text": "Sample Size Planning: Getting It Right 🎯\n\n\n📐 The Formula\nTo achieve margin of error \\(E\\) with confidence level \\((1-\\alpha)\\):\n\\[n = \\left(\\frac{z^*\\sigma}{E}\\right)^2\\]\n🎯 Key Considerations\nMargin of Error Trade-offs: - Smaller \\(E\\) requires larger \\(n\\) - Higher confidence requires larger \\(n\\)\n- More variable population requires larger \\(n\\)\nPractical Constraints: - Budget limitations - Time constraints\n- Availability of participants"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#putting-it-all-together-a-real-example",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#putting-it-all-together-a-real-example",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Putting It All Together: A Real Example 📊",
    "text": "Putting It All Together: A Real Example 📊\n\n\n🎯 Research Question\n“What is the average height of UCSB students?”\nOur Approach:\n\nPopulation: All 26,000 UCSB students\nSample: Random sample of 100 students\nMeasurement: Height in inches\nGoal: 95% confidence interval for population mean\n\nResults:\n\nSample mean: \\(\\bar{x} = 68.2\\) inches\nSample std dev: \\(s = 4.1\\) inches\nSample size: \\(n = 100\\)\n\n\n\n\n                            \n                                            \n\n\n🎯 Interpretation: We are 95% confident that the true average height of UCSB students is between 67.40 and 69.00 inches."
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#key-takeaways-your-statistical-toolkit",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#key-takeaways-your-statistical-toolkit",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Key Takeaways: Your Statistical Toolkit 🎯",
    "text": "Key Takeaways: Your Statistical Toolkit 🎯\n\n\n🧠 Fundamental Concepts\n1. Sampling Wisdom\n\nRepresentative samples beat large biased samples\nRandomization is your best friend\nBias can’t be fixed with larger samples\n\n2. The CLT Magic\n\nSample means are approximately normal (\\(n ≥ 30\\))\nWorks for ANY population distribution\nEnables powerful statistical inference\n\n3. Standard Error\n\nMeasures precision of our estimates\nDecreases with \\(\\sqrt{n}\\), not \\(n\\)\nKey ingredient in confidence intervals\n\n\n🛠️ Practical Skills\n4. Confidence Intervals\n\nQuantify uncertainty in our estimates\nCorrect interpretation is crucial\nWidth depends on confidence level and sample size\n\n5. Sample Size Planning\n\nBalance precision needs with resources\nConsider margin of error requirements\nAccount for practical constraints\n\n6. Quality Control\n\nAlways check for potential bias\nVerify assumptions (normality, independence)\nConsider the broader context"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#common-misconceptions-to-avoid",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#common-misconceptions-to-avoid",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Common Misconceptions to Avoid ⚠️",
    "text": "Common Misconceptions to Avoid ⚠️"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#interactive-practice-test-your-understanding",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#interactive-practice-test-your-understanding",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Interactive Practice: Test Your Understanding 🧪",
    "text": "Interactive Practice: Test Your Understanding 🧪\n\n\n🤔 Check Questions\n1. Sample Size Question: If we want to halve our margin of error, by what factor should we increase our sample size?\n2. CLT Application:\nA population has a right-skewed distribution. What can we say about the distribution of sample means when n = 50?\n3. CI Interpretation: We calculated a 95% CI as (45, 55). What does this mean?\n4. Bias Detection: An online survey about internet usage gets 10,000 responses. What type of bias might be present?\n\n✅ Answers\n1. Increase by factor of 4 (since \\(SE \\propto \\frac{1}{\\sqrt{n}}\\))\n2. Sample means will be approximately normal regardless of population shape\n3. We’re 95% confident the true population parameter is between 45 and 55\n4. Selection bias - excludes people without internet access"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#comprehensive-resources",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#comprehensive-resources",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Comprehensive Resources 📚",
    "text": "Comprehensive Resources 📚\n\n\n📖 Required Reading\n\nOpenIntro Statistics\n\nSection 1.3: Sampling principles and strategies\nSection 3.3: Confidence intervals for a mean\nSection 4.1: Central Limit Theorem\n\n\n🎥 Video Resources\n\nKhan Academy: Central Limit Theorem\nStatQuest: Confidence Intervals Explained\n3Blue1Brown: Central Limit Theorem Visualization\n\n\n💻 Interactive Tools\n\nSeeing Theory: Probability Visualizations\nRossman & Chance Applets: Sampling Distributions\nCentral Limit Theorem Simulator\n\n🤝 Getting Help\n\nOffice Hours: Thursday 11 AM-12 PM (Zoom link on Canvas)\nEmail: nmathlouthi@ucsb.edu\n\n🎯 What’s Next?\nNext Lecture: Hypothesis Testing and p-values"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#questions-discussion",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#questions-discussion",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "Questions & Discussion 🤔",
    "text": "Questions & Discussion 🤔\n\n\n💭 Think About This…\n“The goal is not to eliminate uncertainty, but to understand and quantify it intelligently”\nKey Questions for Reflection: - How do we balance precision with practicality?\n\nWhen might a larger sample actually be worse?\nWhat makes a “good” confidence interval?\nHow do we communicate uncertainty to non-statisticians?\n\n\n🎯 Prepare for Next Class\nComing Up: Hypothesis Testing\n\nWhat are null and alternative hypotheses?\nHow do we make decisions with data?\nWhat does a p-value really mean?\nType I and Type II errors\n\nRecommended Prep:\n\nReview today’s confidence interval concepts\nThink about yes/no questions you’d test with data\nConsider what “statistical significance” means to you"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture10-part1.html#the-central-limit-theorem",
    "href": "files/lecture_notes/lecture10-/lecture10-part1.html#the-central-limit-theorem",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "The Central Limit Theorem 🌟",
    "text": "The Central Limit Theorem 🌟\n\n\n📐 The Statement\nFor a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), when sample size \\(n\\) is large enough:\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nOr equivalently: \\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\n✨ The Magic Rules\n\nRule of Thumb: \\(n \\geq 30\\) usually works\nShape doesn’t matter: Works for ANY population distribution\n\nLarger \\(n\\) = Better approximation"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#real-world-connection",
    "href": "files/lecture_notes/lecture9/lecture9.html#real-world-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Real-World Connection",
    "text": "Real-World Connection"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#activity-your-research-field",
    "href": "files/lecture_notes/lecture9/lecture9.html#activity-your-research-field",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Activity: Your Research Field",
    "text": "Activity: Your Research Field\n\n\n\n\n\n\n\n\nThink About Your Major/Research Area\n\n\nTake 2 minutes to brainstorm:\n\nWhat random phenomena occur in your field?\nHow might you assign numbers to these outcomes?\nWhat questions could you answer with this data?\n\n\n\n\n\n\n\n\n\n\n\n\nExamples by Field\n\n\n\nPsychology: Reaction times, survey responses\nBiology: Species counts, gene expression levels\nEconomics: Stock prices, unemployment rates\nEngineering: System failures, signal strength"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#discrete-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#discrete-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\n\nDefinition: Takes on countable values (finite or countably infinite)\nExamples:\n\nNumber of emails received per day\nNumber of defective products in a batch\nStudent enrollment in courses\nNumber of research papers published per year\n\n\n\n\nNote: If X is discrete, then X can take values \\(x_1, x_2, x_3, \\cdot\\) where we can list all possible values."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#continuous-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#continuous-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\n\nDefinition: Takes on uncountably infinite values (any value in an interval)\nExamples:\n\nHeight of students\nTime until equipment failure\nTemperature measurements\nGPA (technically discrete, but often treated as continuous)\n\n\n\n\nNote: If \\(X\\) is continuous, then \\(X\\) can take any value in an interval \\([a,b]\\) or \\((-\\infty, \\infty)\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#sampling-and-random-variables",
    "href": "files/lecture_notes/lecture9/lecture9.html#sampling-and-random-variables",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Sampling and Random Variables",
    "text": "Sampling and Random Variables"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-connection",
    "href": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-connection",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\nWhen we repeatedly sample from a population, the sample mean becomes a random variable\nFormula: \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)\nEach time we sample, we get a different \\(\\bar{X}\\)\nThe distribution of \\(\\bar{X}\\) has special properties!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-connection-1",
    "href": "files/lecture_notes/lecture9/lecture9.html#central-limit-theorem-connection-1",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Central Limit Theorem Connection",
    "text": "Central Limit Theorem Connection\n\n\n\nThese properties are :\n\nCenter (Unbiased): \\(E[\\bar{X}] = \\mu\\).\nSpread Shrinks with \\(n\\): \\(\\mathrm{Var}(\\bar{X}) = \\sigma^2/n\\); \\(\\mathrm{SE}(\\bar{X}) = \\sigma/\\sqrt{n}\\) (estimate with \\(s/\\sqrt{n}\\)).\nShape:\n\nIf the population is Normal, then \\(\\bar{X} \\sim \\text{Normal}(\\mu, \\sigma^2/n)\\) exactly.\n\nOtherwise, CLT: for large \\(n\\), \\(\\bar{X}\\) is approximately Normal even when the data aren’t.\n\n\n\n\n\n\nConsistency / Law of Large Numbers: \\(\\bar{X} \\xrightarrow{P} \\mu\\) as \\(n \\to \\infty\\) (estimates get closer to the truth with more data).\n(If sampling w/out replacement, pop size \\(N\\)): Apply finite population correction (FPC):\n\\(\\mathrm{SE}(\\bar{X}) = \\dfrac{\\sigma}{\\sqrt{n}}\\sqrt{\\dfrac{N-n}{N-1}}\\)."
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-discrete-distributions",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-discrete-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Discrete Distributions",
    "text": "Common Discrete Distributions\nBinomial Distribution - Characteristics\n\n\n\n\nFixed number of trials (n)\nEach trial has two outcomes\nConstant probability of success\nTrials are independent\n\nExample: Number of successful research grants out of 10 applications"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#poisson-distribution---characteristics",
    "href": "files/lecture_notes/lecture9/lecture9.html#poisson-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Poisson Distribution - Characteristics",
    "text": "Poisson Distribution - Characteristics\n\n\n\nModels rare events\nEvents occur independently\nConstant average rate\nUseful for counts over time/space\n\nExample: Number of emails received per hour, number of mutations in DNA sequences"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#common-continuous-distributions",
    "href": "files/lecture_notes/lecture9/lecture9.html#common-continuous-distributions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Common Continuous Distributions",
    "text": "Common Continuous Distributions\nNormal Distribution - Characteristics\n\n\n\nBell-shaped curve\nSymmetric around mean\nParameters: \\(\\mu\\) (mean), \\(\\sigma\\) (standard deviation)\nMany natural phenomena follow this pattern\n\nExample: Heights, test scores, measurement errors"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#exponential-distribution---characteristics",
    "href": "files/lecture_notes/lecture9/lecture9.html#exponential-distribution---characteristics",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Exponential Distribution - Characteristics",
    "text": "Exponential Distribution - Characteristics\n\n\n\nModels waiting times\nMemoryless property\nParameter: \\(\\lambda\\) (rate)\nRight-skewed\n\nExample: Time between arrivals, equipment lifespan, time to next earthquake"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#interactive-activity-choose-your-distribution",
    "href": "files/lecture_notes/lecture9/lecture9.html#interactive-activity-choose-your-distribution",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Interactive Activity: Choose Your Distribution",
    "text": "Interactive Activity: Choose Your Distribution\n\n\n\n\n\n\nGroup Discussion (5 minutes)\n\n\nFor each scenario, identify: 1. Is the random variable discrete or continuous? 2. What distribution might it follow? 3. What are the parameters?\nScenarios: - Number of students attending office hours per week - Time spent studying for an exam - Number of typos in a research paper - Body temperature of patients in a hospital"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#application-research-design",
    "href": "files/lecture_notes/lecture9/lecture9.html#application-research-design",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Application: Research Design",
    "text": "Application: Research Design\n\nConsider your research question:\n\nIdentify your random variable(s)\n\nWhat are you measuring?\nWhat values can it take?\n\nChoose appropriate distribution\n\nBased on the nature of your data\nConsider the underlying process\n\nPlan your analysis\n\nHow will you collect data?\nWhat statistical tests are appropriate?"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#probability-mass-vs.-density-functions",
    "href": "files/lecture_notes/lecture9/lecture9.html#probability-mass-vs.-density-functions",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Probability Mass vs. Density Functions",
    "text": "Probability Mass vs. Density Functions\n\n\nDiscrete: Probability Mass Function (PMF)\n\n\\(P(X = x)\\) for specific values\nSums to 1 over all possible values\nCan find exact probabilities\n\nExample: \\(P(X = 3) = 0.2\\)\n\nContinuous: Probability Density Function (PDF)\n\n\\(f(x)\\) represents density\nArea under curve = 1\n\\(P(X = x) = 0\\) for any specific value\nFind probabilities over intervals\n\nExample: \\(P(a &lt; X &lt; b) =  \\int_{a}^{b} f(x)dx\\)"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#comparing-distributions-side-by-side",
    "href": "files/lecture_notes/lecture9/lecture9.html#comparing-distributions-side-by-side",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Comparing Distributions Side-by-Side",
    "text": "Comparing Distributions Side-by-Side"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture9/lecture9.html#confidence-intervals-for-means",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Intervals for Means",
    "text": "Confidence Intervals for Means\n\n\nProblem: We have one sample mean, but want to estimate the population mean\nSolution: Use the sampling distribution to create a confidence interval\nKey Insight: If we know how \\(\\bar{X}\\) varies, we can make probabilistic statements about μ\n\n\n\n95% Confidence Interval Formula: \\(\\bar{x} \\pm 1.96 \\times \\frac{\\sigma}{\\sqrt{n}}\\)\nInterpretation: “We are 95% confident that the true population mean lies within this interval”"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#visualizing-confidence-intervals",
    "href": "files/lecture_notes/lecture9/lecture9.html#visualizing-confidence-intervals",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#confidence-interval-interpretation",
    "href": "files/lecture_notes/lecture9/lecture9.html#confidence-interval-interpretation",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Confidence Interval Interpretation",
    "text": "Confidence Interval Interpretation\n\n\n\n\n\n\nCommon Misconceptions\n\n\n❌ WRONG: “There’s a 95% probability that μ is in this specific interval”\n✅ CORRECT: “If we repeated this process many times, 95% of the intervals we construct would contain the true μ”\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe interval is random, not the population parameter\nBefore collecting data: 95% chance our method will work\nAfter collecting data: The interval either contains μ or it doesn’t\nConfidence level = Long-run success rate of the method"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#factors-affecting-confidence-interval-width",
    "href": "files/lecture_notes/lecture9/lecture9.html#factors-affecting-confidence-interval-width",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Factors Affecting Confidence Interval Width",
    "text": "Factors Affecting Confidence Interval Width"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#next-steps",
    "href": "files/lecture_notes/lecture9/lecture9.html#next-steps",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Next Steps",
    "text": "Next Steps\n\n\n\n\n\n\nFor Your Research/Interests\n\n\n\nIdentify random variables in your field\nThink about appropriate distributions\nConsider data collection methods\nPlan statistical analyses\nConnect theory to practice"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#questions-and-discussion",
    "href": "files/lecture_notes/lecture9/lecture9.html#questions-and-discussion",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Questions and Discussion",
    "text": "Questions and Discussion\n\nShare with the class:\n\nWhat random variables are important in your field of study/major?\nWhich distributions might be most relevant?\nWhat challenges do you anticipate in data collection?\n\n\n\nThank you for your participation!"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#appendix-python-code-examples",
    "href": "files/lecture_notes/lecture9/lecture9.html#appendix-python-code-examples",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Appendix: Python Code Examples",
    "text": "Appendix: Python Code Examples\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport seaborn as sns\n\n# Generate random samples from different distributions\n\n# Binomial\nbinom_data = np.random.binomial(n=10, p=0.3, size=100)\n\n# Poisson  \npoisson_data = np.random.poisson(lam=3, size=100)\n\n# Normal\nnormal_data = np.random.normal(loc=0, scale=1, size=100)\n\n# Exponential\nexp_data = np.random.exponential(scale=1/1.5, size=100)\n\n# Create histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0,0].hist(binom_data, bins=11, alpha=0.7, color='steelblue')\naxes[0,0].set_title('Binomial Sample')\n\naxes[0,1].hist(poisson_data, bins=15, alpha=0.7, color='coral')\naxes[0,1].set_title('Poisson Sample')\n\naxes[1,0].hist(normal_data, bins=20, alpha=0.7, color='lightblue')\naxes[1,0].set_title('Normal Sample')\n\naxes[1,1].hist(exp_data, bins=20, alpha=0.7, color='lightgreen')\naxes[1,1].set_title('Exponential Sample')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "files/lecture_notes/lecture9/lecture9.html#additional-resources",
    "href": "files/lecture_notes/lecture9/lecture9.html#additional-resources",
    "title": "From Random Variables to Sampling & Confidence Intervals",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n# Useful Python libraries for statistics and probability\nimport numpy as np           # Numerical computing\nimport scipy.stats as stats  # Statistical functions\nimport matplotlib.pyplot as plt  # Plotting\nimport seaborn as sns        # Statistical visualization\nimport pandas as pd          # Data manipulation\n\n# Quick reference for common distributions:\n# stats.binom.pmf(k, n, p)     # Binomial PMF\n# stats.poisson.pmf(k, lam)    # Poisson PMF  \n# stats.norm.pdf(x, mu, sigma) # Normal PDF\n# stats.expon.pdf(x, scale)    # Exponential PDF\n\n# Generate random samples:\n# np.random.binomial(n, p, size)\n# np.random.poisson(lam, size)\n# np.random.normal(mu, sigma, size)\n# np.random.exponential(scale, size)"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "🏠"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#todays-learning-objectives",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nUnderstand sampling distributions and their properties (Section 1.2)\nApply the Central Limit Theorem to sampling (Section 1.4)\nConstruct confidence intervals for population means (Section 1.6)\nConstruct confidence intervals for population proportions (Section 1.8)\nInterpret confidence intervals correctly (Section 1.5)\nDetermine appropriate sample sizes for desired precision\nUse python to calculate confidence intervals\nDistinguish between different types of sampling methods"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-sampling-dist",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-sampling-dist",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Big Picture: Statistical Inference",
    "text": "The Big Picture: Statistical Inference\n\n\n\n\nPopulation vs Sample\n\n\n\nPopulation: All individuals of interest\n\n\nSample: Subset we actually observe\n\n\nParameter: Population characteristic (\\(\\mu\\), \\(p\\))\n\n\nStatistic: Sample characteristic (\\(\\bar{x}\\), \\(\\hat{p}\\))\n\n\n\nGoal: Use sample statistics to estimate population parameters\n\n\n\n\n\n\n\n\n\nWhy Confidence Intervals?\n\n\n\nPoint estimates are rarely exactly correct\n\n\nInterval estimates capture uncertainty\n\n\nConfidence level quantifies our certainty\n\n\nMargin of error shows precision\n\n\n\nKey Insight: We trade precision for confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sampling-distributions",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nA sampling distribution is the distribution of a statistic (like \\(\\bar{x}\\)) across all possible samples of size \\(n\\).\n\n\n\nKey Properties:\nCenter:\n\\(E[\\bar{X}] = \\mu\\) (unbiased)\nSpread:\n\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nShape:\nApproaches normal as \\(n\\) increases (Central Limit Theorem)\nStandard Error vs Standard Deviation:\n\n\\(\\sigma\\): spread of individual observations\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\): spread of sample means\n\n\n\n\n\n\nDrag the slider to see how sample size affects the sampling distribution"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-clt-sampling",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-clt-sampling",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Central Limit Theorem in Action",
    "text": "Central Limit Theorem in Action\n\n\n\n\nNew Population\n\n Uniform Population Exponential Population Bimodal Population Right-Skewed Population  Sample Size:  Collect 1000 Sample Means\n\n\n\nPopulation μ: - | Sample Means μ: - | Standard Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-interpretation",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-interpretation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Concept",
    "text": "Confidence Intervals: The Concept\n\n\nWhat is a Confidence Interval? A confidence interval provides a range of plausible values for a population parameter. 95% Confidence Interval: If we repeated our sampling process many times, about \\(95\\%\\) of the intervals we construct would contain the true population parameter.\n\n\n\n\n\nClick to generate new 95% confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-means",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Means",
    "text": "Confidence Intervals for Population Means\n\n\n\n\n 🎯 When σ is Known:\n\n\n\\[\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\n\n\nWhen \\(\\sigma\\) is Unknown (more common):\n\n\n\\[\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\bar{x}\\): sample mean\n\n\n\\(t^*\\): critical value (df = n-1)\n\n\n\\(\\frac{s}{\\sqrt{n}}\\): standard error\n\n\n\n\n\n\nCommon Confidence Levels:\n\n\n\n90%: z* = 1.645, more precise\n\n\n95%: z* = 1.96, most common\n\n\n99%: z* = 2.576, more confident\n\n\n\nConditions Required:\n\n\n\nRandom sampling\nNearly normal population OR n ≥ 30\nIndependent observations"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Means",
    "text": "Interactive CI Demo: Confidence Intervals for Means\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population μ:  Population σ:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures μ? - | Margin of Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-proportions",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sec-ci-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Proportions",
    "text": "Confidence Intervals for Population Proportions\n\n\n\n 🎯 Formula:\n\n\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\hat{p} = \\frac{x}{n}\\): sample proportion\n\n\n\\(z^*\\): critical value\n\n\n\\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\): standard error\n\n\n\n\nConditions Required:\n\n\n\nRandom sampling\n\n\n\\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\)\n\n\nIndependent observations\n\n\nPopulation at least 10× sample size\n\n\n\nConservative Approach:\n\n\nUse \\(\\hat{p} = 0.5\\) for planning when true proportion unknown (maximizes margin of error)"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Proportions",
    "text": "Interactive CI Demo: Confidence Intervals for Proportions\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population p:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures p? - | Sample Proportion: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 1: CI for Mean",
    "text": "Practice Problem 1: CI for Mean\n\nA random sample of 25 college students shows a mean daily screen time of 6.2 hours with a standard deviation of 1.8 hours. (a) Construct a 95% confidence interval for the mean daily screen time. (b) Interpret the confidence interval in context. (c) What would happen to the interval width if we used 99% confidence instead? Show Solution\n\nSolution. (a)\nGiven: \\(n = 25\\), \\(\\bar{x} = 6.2\\), \\(s = 1.8\\), 95% confidence\nFor \\(df = 24\\), \\(t^* = 2.064\\)\n\\(SE = \\frac{s}{\\sqrt{n}} = \\frac{1.8}{\\sqrt{25}} = 0.36\\)\n\\(CI = 6.2 \\pm 2.064 \\times 0.36 = 6.2 \\pm 0.743 = (5.46, 6.94)\\) hours\n(b)\nWe are 95% confident that the true mean daily screen time for all college students is between \\(5.46\\) and \\(6.94\\) hours.\n(c)\nFor 99% confidence, we use \\(t^* = 2.797\\), giving a wider interval: \\((5.19, 7.21)\\) hours."
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 2: CI for Proportion",
    "text": "Practice Problem 2: CI for Proportion\n\nIn a survey of 400 voters, 240 support a particular candidate. (a) Construct a 90% confidence interval for the true proportion of supporters. (b) Check if the conditions for inference are met. (c) How large a sample would be needed for a margin of error of 0.03 with 95% confidence? Show Solution\n\nSolution. (a)\n\\(\\hat{p} = \\frac{240}{400} = 0.6\\), \\(n = 400\\), 90% confidence, \\(z^* = 1.645\\)\n\\(SE = \\sqrt{\\frac{0.6 \\times 0.4}{400}} = \\sqrt{\\frac{0.24}{400}} = 0.0245\\)\n\\(CI = 0.6 \\pm 1.645 \\times 0.0245 = 0.6 \\pm 0.0403 = (0.560, 0.640)\\)\n(b)\nCheck conditions: \\(n\\hat{p} = 400 \\times 0.6 = 240 \\geq 10\\) ✓\n\\(n(1-\\hat{p}) = 400 \\times 0.4 = 160 \\geq 10\\) ✓\n(c)\nSample size calculation:\n\\(n = \\frac{(z^*)^2 \\hat{p}(1-\\hat{p})}{ME^2} =\n\\frac{(1.96)^2 \\times 0.6 \\times 0.4}{(0.03)^2} =\n\\frac{0.9216}{0.0009} = 1024\\) people"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 3: Sample Size Planning",
    "text": "Practice Problem 3: Sample Size Planning\n\nA market researcher wants to estimate the average amount spent on coffee per week by college students. (a) How large a sample is needed for a 95% CI with margin of error $2 if \\(\\sigma\\) = $8? (b) If the budget only allows for 100 students, what confidence level gives a $2 margin of error? (c) What’s the trade-off between sample size, confidence level, and precision?\n\nShow Solution\n\n\nSolution. (a)\nFor means:\n\\(n = \\frac{(z^*)^2 \\sigma^2}{ME^2} =\n\\frac{(1.96)^2 \\times 8^2}{2^2} =\n\\frac{245.86}{4} = 62\\) students\n(b)\nWith \\(n = 100\\):\n\\(ME = z^* \\frac{\\sigma}{\\sqrt{n}} =\nz^* \\frac{8}{\\sqrt{100}} =\n0.8 z^*\\)\nFor \\(ME = 2\\):\n\\(z^* = \\frac{2}{0.8} = 2.5\\),\nwhich corresponds to about 98.8% confidence\n(c) Trade-offs:\n\nHigher confidence \\(\\rightarrow\\) wider intervals (less precision)\nLarger sample \\(\\rightarrow\\) narrower intervals (more precision)\nLower margin of error \\(\\rightarrow\\) need larger sample or lower confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes and Misconceptions",
    "text": "Common Mistakes and Misconceptions\n\n\nInterpretation Errors\n❌ Wrong: “\\(95\\%\\) of the data falls in this interval”\n✅ Right: “We’re \\(95\\%\\) confident the parameter is in this interval”\n❌ Wrong: “There’s a \\(95\\%\\) chance \\(\\mu\\) is in this interval”\n✅ Right: “\\(95\\%\\) of such intervals contain \\(\\mu\\)”\n\nTechnical Errors\n\nUsing \\(z*\\) when σ is unknown and \\(n &lt; 30\\)\nForgetting to check conditions\nConfusing standard error with standard deviation\nUsing wrong degrees of freedom for t-distribution\n\n\nRemember: The confidence level refers to the long-run proportion of intervals that capture the parameter!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size and Margin of Error Relationships",
    "text": "Sample Size and Margin of Error Relationships\n\n\nPopulation σ:  Confidence Level:  90% 95% 99%   Desired Margin of Error: \n\n\n\n\n\n\nSample Size vs Margin of Error\n\n\n\n\nRequired Sample Size: - | Resulting ME: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Types of Sampling Methods",
    "text": "Types of Sampling Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nSimple Random\nEvery individual has equal chance\nUnbiased, simple\nMay not represent subgroups\n\n\nStratified\nSample from each subgroup\nEnsures representation\nMore complex\n\n\nCluster\nSample entire groups\nCost-effective for spread populations\nHigher variability\n\n\nSystematic\nEvery k-th individual\nSimple to implement\nCan miss patterns\n\n\nConvenience\nEasily accessible individuals\nQuick and cheap\nHighly biased\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling Method Matters: Only probability sampling methods allow for valid statistical inference!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#confidence-intervals-in-practice",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#confidence-intervals-in-practice",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals in Practice",
    "text": "Confidence Intervals in Practice\n\n\n\nWhen to Use Each Type\nMeans: Continuous data (height, income, test scores)\nProportions: Categorical data (yes/no, success/failure)\nChoosing Confidence Level\n\n90%: Quick estimates, less critical decisions\n95%: Standard in most research\n99%: High-stakes decisions, medical trials\n\n\nReal-World Applications\n\nPolitical polls: Proportion confidence intervals\nQuality control: Mean confidence intervals\nMedical research: Both types with high confidence\nBusiness analytics: Varies by decision importance\n\nCommunication Tips\n\nAlways include the confidence level\nState what the interval estimates\nAcknowledge the uncertainty\nConsider practical significance"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#key-takeaways",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nMain Concepts\n\nSampling distributions follow predictable patterns\nConfidence intervals quantify uncertainty\nCentral Limit Theorem makes normal-based inference possible\nSample size directly affects precision\n\n\nPractical Guidelines Choose appropriate methods based on:\n\nData type (continuous vs categorical)\nSample size (use t when σ unknown)\nDesired precision (affects sample size)\nConfidence level (affects interval width)\n\nKey Principle Statistical inference allows us to make informed decisions about populations using sample data, while properly accounting for uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#looking-ahead",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#looking-ahead",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Hypothesis Testing\nTopics we’ll cover:\n\nNull and alternative hypotheses\nTest statistics and p-values\nType I and Type II errors\n\n\nConnection: Confidence intervals and hypothesis tests are two sides of the same statistical inference coin"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#questions",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions?",
    "text": "Questions?\n\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and Statistical Significance"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9-interactive.html#resources",
    "href": "files/lecture_notes/lecture10-/lecture9-interactive.html#resources",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Resources",
    "text": "Resources\n\n  \n    \n      \n      Read OpenIntro Statistics Chapter 5 sections 5.1-5.3\n    \n    \n      \n      Khan Academy - Confidence Intervals\n    \n    \n      \n      Seeing Theory - Frequentist Inference\n    \n    \n      \n      Confidence Intervals - Wikipedia\n    \n    \n      \n      Understanding Different Types of Intervals"
  },
  {
    "objectID": "files/worksheets/drafts/worksheet4draft.html",
    "href": "files/worksheets/drafts/worksheet4draft.html",
    "title": "PSTAT 5A Practice Worksheet 3",
    "section": "",
    "text": "Instructions and Overview\n⏰ Time Allocation:\n\nSection A (Warm-up): 8 minutes\nSection B (Intermediate): 15 minutes\nSection C (Advanced): 15 minutes\nSection D (Review): 12 minutes\nTotal: 50 minutes\n\n\n📝 Important Instructions:\n\nUse the formulas provided for guidance\nRound final answers to 4 decimal places unless otherwise specified\nIdentify your approach before calculating\nUse calculator as needed\n\n\n\n📚 Key Formulas Reference:\nBasic Probability:\n\nConditional Probability: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\nBayes’ Theorem: \\(P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\\)\nLaw of Total Probability: \\(P(A) = \\sum P(A|B_i) \\cdot P(B_i)\\)\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\nMultiplication Rule: \\(P(A \\cap B) = P(A) \\cdot P(B|A) = P(B) \\cdot P(A|B)\\)\n\nCounting:\n\nPermutations: \\(P(n,r) = \\frac{n!}{(n-r)!}\\)\nCombinations: \\(C(n,r) = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\n\n\n\n\nSection A: Probability\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions\nEach row in the table below is a proposed grade distribution for a class. Identify each as a valid or invalid probability distribution, and explain your reasoning.\n\n\n\nClass\nA\nB\nC\nD\nF\n\n\n\n\n(a)\n0.3\n0.3\n0.3\n0.2\n0.1\n\n\n(b)\n0\n0\n1\n0\n0\n\n\n(c)\n0.3\n0.3\n0.3\n0\n0\n\n\n(d)\n0.3\n0.5\n0.2\n0.1\n-0.1\n\n\n(e)\n0.2\n0.4\n0.2\n0.1\n0.1\n\n\n(f)\n0\n-0.1\n1.1\n0\n0\n\n\n\n\nWork Space:\n\n\n\nSection B: Permutations and Combination\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations\nA cybersecurity team needs to create a secure access protocol.\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\n\n\n\n\n\n\nTip\n\n\n\nSince letters must come before digits, think of this as two separate arrangement problems:\n\nFirst, arrange the 3 letters in the first 3 positions\nThen, arrange the 3 digits in the last 3 positions\nUse the multiplication principle to combine these results\n\n\n\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\n\n\n\n\n\n\nTip\n\n\n\nSince order doesn’t matter, this is a combination problem. Ask yourself:\n\nAre we arranging people in specific positions, or just selecting a group?\nWhich formula should you use: \\(P(n,r)\\) or \\(C(n,r)\\)?\n\n\n\n\nWork Space:\n\n\n\nSection C: Conditional Probability\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Conditional Probability and Medical Testing\nA new COVID variant test has the following characteristics:\n\nThe variant affects 3% of the tested population\nThe test correctly identifies 95% of people with the variant (sensitivity)\nThe test correctly identifies 92% of people without the variant (specificity)\n\nPart (a): What is the probability that a randomly selected person tests positive?\nPart (b): If someone tests positive, what is the probability they actually have the variant?\nPart (c): If someone tests negative, what is the probability they actually don’t have the variant?\nPart (d) [Challenge]: The health department wants to reduce false positives. They decide to require two consecutive positive tests for a positive diagnosis. Assuming test results are independent, what is the new probability that someone with two positive tests actually has the variant?\n\nWork Space:\n\n\n\nSection C: Conditional Probability\n⏱️ Estimated time: 15 minutes\n\nProblem C1: Advanced Counting with Restrictions\nA restaurant offers a prix fixe menu where customers must choose:\n\n1 appetizer from 6 options\n1 main course from 8 options\n1 dessert from 5 options\n\nHowever, there are restrictions:\n\nIf you choose the seafood appetizer, you cannot choose the vegetarian main course\nIf you choose the chocolate dessert, you must choose either the beef or chicken main course (3 of the 8 main courses)\n\nPart (a): How many valid meal combinations are possible?\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\n\nWork Space:\n\n\n\nSection D: Review\n⏱️ Estimated time: 12 minutes\n\nProblem B3: Daily Expenses\nSally gets a cup of coffee and a muffin every day for breakfast from one of the many coffee shops in her neighborhood. She picks a coffee shop each morning at random and independently of previous days. The average price of a cup of coffee is $1.40 with a standard deviation of 30¢ ($0.30), the average price of a muffin is $2.50 with a standard deviation of 15¢, and the two prices are independent of each other.\nPart (a): What is the mean and standard deviation of the amount she spends on breakfast daily?\nPart (b): What is the mean and standard deviation of the amount she spends on breakfast weekly (7 days)?\n\nWork Space:"
  },
  {
    "objectID": "files/worksheets/drafts/worksheet4_sln_draft.html",
    "href": "files/worksheets/drafts/worksheet4_sln_draft.html",
    "title": "PSTAT 5A Practice Worksheet 3 - SOLUTIONS",
    "section": "",
    "text": "Section A: Probability - SOLUTIONS\n⏱️ Estimated time: 8 minutes\n\nProblem A1: Probability Distributions - SOLUTION\nFor a valid probability distribution, two conditions must be met:\n\nAll probabilities must be non-negative (≥ 0)\nThe sum of all probabilities must equal 1\n\nAnalysis:\n(a) Invalid\n\nSum = 0.3 + 0.3 + 0.3 + 0.2 + 0.1 = 1.2 &gt; 1 The probabilities sum to more than 1, violating the second condition.\n\n(b) Valid\n\nSum = 0 + 0 + 1 + 0 + 0 = 1 All probabilities are non-negative and sum to 1. This represents a class where everyone receives a C.\n\n(c) Invalid\n\nSum = 0.3 + 0.3 + 0.3 + 0 + 0 = 0.9 &lt; 1 The probabilities sum to less than 1, violating the second condition.\n\n(d) Invalid\n\nContains F = -0.1 &lt; 0 Although the sum would equal 1.0, the probability for grade F is negative, violating the first condition.\n\n(e) Valid\n\nSum = 0.2 + 0.4 + 0.2 + 0.1 + 0.1 = 1.0 All probabilities are non-negative and sum to 1.\n\n(f) Invalid\n\nContains B = -0.1 &lt; 0 Although the sum equals 1.0, the probability for grade B is negative, violating the first condition.\n\n\n\n\nSection B: Permutations and Combinations - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Permutations and Combinations - SOLUTION\nPart (a): How many 6-character passwords can be formed using 3 specific letters and 3 specific digits if repetitions are not allowed and letters must come before digits?\nSolution: Since letters must come before digits, we have a fixed structure: LLL DDD\n\nStep 1: Arrange 3 letters in the first 3 positions\n\nThis is a permutation: P(3,3) = 3! = 6 ways\n\nStep 2: Arrange 3 digits in the last 3 positions\n\nThis is a permutation: P(3,3) = 3! = 6 ways\n\nStep 3: Apply multiplication principle\n\nTotal passwords = 6 × 6 = 36 passwords\n\n\nPart (b): If the team wants to select 4 people from 12 employees to form a security committee where order doesn’t matter, how many ways can this be done?\nSolution: Since order doesn’t matter, this is a combination problem.\n\\[C(12,4) = \\binom{12}{4} = \\frac{12!}{4!(12-4)!} = \\frac{12!}{4! \\cdot 8!}\\]\n\\[= \\frac{12 \\times 11 \\times 10 \\times 9}{4 \\times 3 \\times 2 \\times 1} = \\frac{11880}{24} = \\textbf{495 ways}\\]\n\n\n\nSection C: Conditional Probability - SOLUTIONS\n⏱️ Estimated time: 15 minutes\n\nProblem B1: Conditional Probability and Medical Testing - SOLUTION\nGiven Information:\n\nP(has variant) = 0.03\nP(test positive | has variant) = 0.95 (sensitivity)\nP(test negative | no variant) = 0.92 (specificity)\nTherefore: P(test positive | no variant) = 1 - 0.92 = 0.08\n\nPart (a): What is the probability that a randomly selected person tests positive?\nSolution:\nUsing the Law of Total Probability:\n\\[P(\\text{test positive}) = P(\\text{test positive | has variant}) \\times P(\\text{has variant}) + P(\\text{test positive | no variant}) \\times P(\\text{no variant})\\]\n\\[P(\\text{test positive}) = 0.95 \\times 0.03 + 0.08 \\times 0.97\\] \\[= 0.0285 + 0.0776 = \\textbf{0.1061}\\]\nPart (b): If someone tests positive, what is the probability they actually have the variant?\nSolution: Using Bayes’ Theorem:\n\\[P(\\text{has variant | test positive}) = \\frac{P(\\text{test positive | has variant}) \\times P(\\text{has variant})}{P(\\text{test positive})}\\]\n\\[= \\frac{0.95 \\times 0.03}{0.1061} = \\frac{0.0285}{0.1061} = \\textbf{0.2686}\\]\nPart (c): If someone tests negative, what is the probability they actually don’t have the variant?\nSolution: First, find P(test negative): \\[P(\\text{test negative}) = 1 - P(\\text{test positive}) = 1 - 0.1061 = 0.8939\\]\nUsing Bayes’ Theorem: \\[P(\\text{no variant | test negative}) = \\frac{P(\\text{test negative | no variant}) \\times P(\\text{no variant})}{P(\\text{test negative})}\\]\n\\[= \\frac{0.92 \\times 0.97}{0.8939} = \\frac{0.8924}{0.8939} = \\textbf{0.9983}\\]\nPart (d) [Challenge]: Two consecutive positive tests - what is the probability they actually have the variant?\nSolution: Assuming independence between tests:\n\\[P(\\text{two positive | has variant}) = 0.95^2 = 0.9025\\] \\[P(\\text{two positive | no variant}) = 0.08^2 = 0.0064\\]\n\\[P(\\text{two positive}) = 0.9025 \\times 0.03 + 0.0064 \\times 0.97 = 0.027075 + 0.006208 = 0.033283\\]\n\\[P(\\text{has variant | two positive}) = \\frac{0.027075}{0.033283} = \\textbf{0.8134}\\]\n\n\nProblem C1: Advanced Counting with Restrictions - SOLUTION\nPart (a): How many valid meal combinations are possible?\nSolution: We need to consider cases based on the restrictions.\nCase 1: Seafood appetizer is chosen\n\n1 appetizer option (seafood)\n7 main course options (cannot choose vegetarian)\n5 dessert options\nCombinations: 1 × 7 × 5 = 35\n\nCase 2: Non-seafood appetizer + chocolate dessert\n\n5 appetizer options (non-seafood)\n3 main course options (only beef or chicken allowed with chocolate)\n1 dessert option (chocolate)\nCombinations: 5 × 3 × 1 = 15\n\nCase 3: Non-seafood appetizer + non-chocolate dessert - 5 appetizer options (non-seafood)\n\n8 main course options (no restrictions)\n4 dessert options (non-chocolate)\nCombinations: 5 × 8 × 4 = 160\n\nTotal valid combinations: 35 + 15 + 160 = 210 combinations\nPart (b): If customers choose randomly among valid combinations, what is the probability someone chooses the chocolate dessert?\nSolution: Combinations with chocolate dessert: 15 (from Case 2 above) Total valid combinations: 210\n\\[P(\\text{chocolate dessert}) = \\frac{15}{210} = \\frac{1}{14} = \\textbf{0.0714}\\]\n\n\n\nSection D: Review - SOLUTIONS\n⏱️ Estimated time: 12 minutes\n\nProblem B3: Daily Expenses - SOLUTION\nGiven:\n\nCoffee: Mean = $1.40, SD = $0.30\nMuffin: Mean = $2.50, SD = $0.15\nPrices are independent\n\nPart (a): What is the mean and standard deviation of the amount she spends on breakfast daily?\nSolution: For the sum of independent random variables:\nMean of daily expenses: \\[E[\\text{Daily}] = E[\\text{Coffee}] + E[\\text{Muffin}] = \\$1.40 + \\$2.50 = \\textbf{\\$3.90}\\]\nVariance of daily expenses: \\[\\text{Var}[\\text{Daily}] = \\text{Var}[\\text{Coffee}] + \\text{Var}[\\text{Muffin}] = (0.30)^2 + (0.15)^2 = 0.09 + 0.0225 = 0.1125\\]\nStandard deviation of daily expenses: \\[SD[\\text{Daily}] = \\sqrt{0.1125} = \\textbf{\\$0.3354}\\]\nPart (b): What is the mean and standard deviation of the amount she spends on breakfast weekly (7 days)?\nSolution: For the sum of 7 independent daily expenses:\nMean of weekly expenses: \\[E[\\text{Weekly}] = 7 \\times E[\\text{Daily}] = 7 \\times \\$3.90 = \\textbf{\\$27.30}\\]\nVariance of weekly expenses: \\[\\text{Var}[\\text{Weekly}] = 7 \\times \\text{Var}[\\text{Daily}] = 7 \\times 0.1125 = 0.7875\\]\nStandard deviation of weekly expenses: \\[SD[\\text{Weekly}] = \\sqrt{0.7875} = \\textbf{\\$0.8874}\\]"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "From Samples to Populations: Understanding Uncertainty\n\n\n\n\n\n\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions\n\n\n\n\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation\n\n\n\n\n\n\n\n\n\nObservational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty.\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)\n\n\n\n\n\n\n\n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values.\n\n\n\n\n\n\n\n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!\n\n\n\n\n\n\n\n\nWhat it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!\n\n\n\n\n\n\n\n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”\n\n\n\n\n\n\n\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes\n\n\n\n\n\n\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#announcements",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#announcements",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "When:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#what-well-learn-today",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#what-well-learn-today",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Big Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Observational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Think of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#why-point-estimates-arent-enough",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#why-point-estimates-arent-enough",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Imagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values."
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#sampling-distributions",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Think of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#the-central-limit-theorem-clt",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#the-central-limit-theorem-clt",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Central Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#standard-error",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#standard-error",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "What it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Imagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#key-takeaways",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Big Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes"
  },
  {
    "objectID": "files/lecture_notes/lecture10-/lecture9_intro.html#questions",
    "href": "files/lecture_notes/lecture10-/lecture9_intro.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Office Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html",
    "href": "files/lecture_notes/lecture12/lecture12.html",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "",
    "text": "When:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#announcements",
    "href": "files/lecture_notes/lecture12/lecture12.html#announcements",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "",
    "text": "When:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#learning-objectives",
    "href": "files/lecture_notes/lecture12/lecture12.html#learning-objectives",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Learning Objectives 🎯",
    "text": "Learning Objectives 🎯\nBy the end of this lecture, you will be able to:\n\nFormulate null and alternative hypotheses from research questions\nUnderstand the logic of hypothesis testing\nCalculate and interpret p-values correctly\nMake decisions using significance levels\nRecognize Type I and Type II errors and their consequences\nPerform common hypothesis tests in Python"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#what-is-hypothesis-testing",
    "href": "files/lecture_notes/lecture12/lecture12.html#what-is-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "What is Hypothesis Testing?",
    "text": "What is Hypothesis Testing?\n\n\nHypothesis testing is a statistical method for making decisions about population parameters based on sample data\nIt helps us answer questions like:\n\n“Is this new drug more effective than the current treatment?”\n“Has customer satisfaction improved after our changes?”\n“Are students’ test scores significantly different from the national average?”\n\n\n\n\nKey Idea: We use sample data to make inferences about populations, acknowledging that our conclusions might be wrong due to random variation."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#the-courtroom-analogy",
    "href": "files/lecture_notes/lecture12/lecture12.html#the-courtroom-analogy",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "The Courtroom Analogy",
    "text": "The Courtroom Analogy\n\n\nCriminal Trial\n\nPresumption: Innocent until proven guilty\nBurden of proof: Prosecution must prove guilt\nStandard: “Beyond reasonable doubt”\nVerdict: Guilty or Not Guilty\n\n\nHypothesis Testing\n\nPresumption: Null hypothesis is true\nBurden of proof: Data must provide evidence against null\nStandard: Significance level (\\(\\alpha = 0.05\\))\nDecision: Reject or Fail to Reject H₀\n\n\n\n\nJust like in court, we never “prove” innocence or “accept” the null hypothesis, we only determine if there’s sufficient evidence to reject it."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#the-six-steps-of-hypothesis-testing",
    "href": "files/lecture_notes/lecture12/lecture12.html#the-six-steps-of-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "The Six Steps of Hypothesis Testing",
    "text": "The Six Steps of Hypothesis Testing"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-1-state-the-hypotheses",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-1-state-the-hypotheses",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 1: State the Hypotheses",
    "text": "Step 1: State the Hypotheses\n\n\nNull Hypothesis (\\(H_0\\)): The “status quo” or “no effect” statement\n\nUsually includes “=”, “≤”, or “≥”\nWhat we assume to be true until proven otherwise\n\nAlternative Hypothesis (\\(H_1\\) or \\(H_a\\)): The research claim we want to test\n\nUsually includes “≠”, “&lt;”, or “&gt;”\nWhat we’re trying to find evidence for\n\n\n\n\nExample: Testing if a new teaching method improves test scores\n\n\\(H_0: \\mu = 75\\) (no improvement, scores stay the same)\n\\(H_1: \\mu &gt; 75\\) (scores improve with new method)"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#types-of-alternative-hypotheses",
    "href": "files/lecture_notes/lecture12/lecture12.html#types-of-alternative-hypotheses",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Types of Alternative Hypotheses",
    "text": "Types of Alternative Hypotheses"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-2-choose-significance-level-α",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-2-choose-significance-level-α",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 2: Choose Significance Level (α)",
    "text": "Step 2: Choose Significance Level (α)\n\n\nSignificance level (α): The probability of rejecting H₀ when it’s actually true\nCommon choices: α = 0.05, 0.01, or 0.10\nInterpretation: “We’re willing to be wrong 5% of the time”\n\n\n\nHow to choose α: - α = 0.05: Standard for most research - α = 0.01: More conservative, when Type I errors are costly - α = 0.10: Less conservative, when Type II errors are costly\n\n\nImportant: Choose α before collecting data to avoid bias!"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-3-check-assumptions-and-conditions",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-3-check-assumptions-and-conditions",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 3: Check Assumptions and Conditions",
    "text": "Step 3: Check Assumptions and Conditions\nCommon assumptions for many tests:\n\n\nIndependence: Observations don’t influence each other\nNormality: Data comes from a normal distribution (or n ≥ 30)\nEqual variances: When comparing groups\nRandom sampling: Sample represents the population\n\n\n\nWhat if assumptions are violated? - Use non-parametric tests - Transform the data - Use robust methods - Increase sample size"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-4-calculate-the-test-statistic",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-4-calculate-the-test-statistic",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 4: Calculate the Test Statistic",
    "text": "Step 4: Calculate the Test Statistic\n\n\nTest statistic: A standardized measure of how far our sample result is from what we’d expect if H₀ were true\nCommon test statistics:\n\nz-statistic: For means when σ is known\nt-statistic: For means when σ is unknown\nχ² statistic: For categorical data\nF-statistic: For comparing variances\n\n\n\n\nFormula for one-sample t-test: \\[t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}\\]\nWhere: \\(\\bar{x}\\) = sample mean, \\(\\mu_0\\) = hypothesized mean, \\(s\\) = sample standard deviation, \\(n\\) = sample size"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-5-find-the-p-value",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-5-find-the-p-value",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 5: Find the P-value",
    "text": "Step 5: Find the P-value\n\n\n\n\n\n\n\n\n\n\nP-value interpretation: “If H₀ were true, what’s the probability of getting a test statistic at least as extreme as what we observed?”"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#common-p-value-misconceptions",
    "href": "files/lecture_notes/lecture12/lecture12.html#common-p-value-misconceptions",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common P-value Misconceptions",
    "text": "Common P-value Misconceptions\n\n\n\n\n\n\nWhat P-values DON’T tell us\n\n\n\n❌ WRONG: “P-value is the probability that H₀ is true”\n❌ WRONG: “P-value is the probability of making an error”\n❌ WRONG: “1 - p-value is the probability that H₁ is true”\n❌ WRONG: “Smaller p-values mean larger effects”\n\n\n\n✅ CORRECT: “P-value is the probability of observing this result (or more extreme) assuming H₀ is true”"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#step-6-make-a-decision-and-interpret",
    "href": "files/lecture_notes/lecture12/lecture12.html#step-6-make-a-decision-and-interpret",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Step 6: Make a Decision and Interpret",
    "text": "Step 6: Make a Decision and Interpret\n\nDecision Rule: - If p-value ≤ α: Reject H₀ (statistically significant) - If p-value &gt; α: Fail to reject H₀ (not statistically significant)\n\n\nLanguage matters: - ✅ “Reject H₀” or “Fail to reject H₀” - ❌ “Accept H₀” or “Prove H₁” - ✅ “Evidence suggests…” or “Data supports…” - ❌ “H₁ is true” or “H₀ is false”"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#types-of-errors",
    "href": "files/lecture_notes/lecture12/lecture12.html#types-of-errors",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Types of Errors",
    "text": "Types of Errors"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#real-world-error-consequences",
    "href": "files/lecture_notes/lecture12/lecture12.html#real-world-error-consequences",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Real-World Error Consequences",
    "text": "Real-World Error Consequences\n\n\nType I Error Examples: - Medical: Saying a drug works when it doesn’t - Legal: Convicting an innocent person - Quality Control: Rejecting good products - Marketing: Launching ineffective campaigns\n\nType II Error Examples: - Medical: Missing a disease diagnosis - Legal: Acquitting a guilty person - Quality Control: Accepting defective products - Security: Missing a threat\n\n\n\nThe Trade-off: Reducing one type of error usually increases the other. We must balance based on the consequences of each error type."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#statistical-power",
    "href": "files/lecture_notes/lecture12/lecture12.html#statistical-power",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Statistical Power",
    "text": "Statistical Power\n\n\nPower (1 - β): The probability of correctly rejecting a false null hypothesis\nWhat affects power?\n\nEffect size: Larger effects are easier to detect\nSample size: More data increases power\nSignificance level: Higher α increases power\nVariability: Less noise increases power"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#example-1-one-sample-t-test",
    "href": "files/lecture_notes/lecture12/lecture12.html#example-1-one-sample-t-test",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: One-Sample t-test",
    "text": "Example 1: One-Sample t-test\nResearch Question: Does a new study technique improve test scores compared to the school average of 75?\n\n\nSample Statistics:\nSample size: 25\nSample mean: 76.69\nSample std: 7.65\n\n\nStep 1: State Hypotheses - H₀: μ = 75 (new method doesn’t improve scores) - H₁: μ &gt; 75 (new method improves scores)\nSteps 2-3: α = 0.05, assume normality (n=25 is borderline, but we’ll proceed)"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#example-1-calculations",
    "href": "files/lecture_notes/lecture12/lecture12.html#example-1-calculations",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: Calculations",
    "text": "Example 1: Calculations\n\n\nTest statistic: t = 1.105\nDegrees of freedom: 24\nP-value: 0.1400\n\nDecision:\nα = 0.05\nP-value (0.1400) &gt; α (0.05): Fail to reject H₀\nConclusion: There is insufficient evidence that the new study technique improves test scores."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#example-1-visualization",
    "href": "files/lecture_notes/lecture12/lecture12.html#example-1-visualization",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 1: Visualization",
    "text": "Example 1: Visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#using-python-for-hypothesis-testing",
    "href": "files/lecture_notes/lecture12/lecture12.html#using-python-for-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Using Python for Hypothesis Testing",
    "text": "Using Python for Hypothesis Testing\n\n\nUsing scipy.stats.ttest_1samp:\nt-statistic: 1.105\np-value (two-tailed): 0.2799\np-value (one-tailed): 0.1400\n\nUsing statsmodels:\nt-statistic: 1.105\np-value (one-tailed): 0.1400\ndegrees of freedom: 24.0"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#example-2-two-sample-t-test",
    "href": "files/lecture_notes/lecture12/lecture12.html#example-2-two-sample-t-test",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Example 2: Two-Sample t-test",
    "text": "Example 2: Two-Sample t-test\nResearch Question: Is there a difference in test scores between two teaching methods?\n\n\nMethod A (Traditional):\n  n = 30, mean = 75.45, std = 11.87\nMethod B (New):\n  n = 28, mean = 80.72, std = 14.82\n\nHypotheses:\nH₀: μ_A = μ_B (no difference between methods)\nH₁: μ_A ≠ μ_B (there is a difference)\n\nTwo-sample t-test results:\nt-statistic: -1.500\np-value: 0.1392\n\nDecision: Fail to reject H₀ (p = 0.1392 &gt; 0.05)\nConclusion: No significant difference between teaching methods."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#two-sample-test-visualization",
    "href": "files/lecture_notes/lecture12/lecture12.html#two-sample-test-visualization",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Two-Sample Test Visualization",
    "text": "Two-Sample Test Visualization"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#effect-size-cohens-d",
    "href": "files/lecture_notes/lecture12/lecture12.html#effect-size-cohens-d",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Effect Size: Cohen’s d",
    "text": "Effect Size: Cohen’s d\n\n\nCohen's d: -0.394\nEffect size interpretation: small\n\nCohen's d interpretation:\n  |d| &lt; 0.2: negligible effect\n  0.2 ≤ |d| &lt; 0.5: small effect\n  0.5 ≤ |d| &lt; 0.8: medium effect\n  |d| ≥ 0.8: large effect"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#common-hypothesis-tests-summary",
    "href": "files/lecture_notes/lecture12/lecture12.html#common-hypothesis-tests-summary",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common Hypothesis Tests Summary",
    "text": "Common Hypothesis Tests Summary\n\n\nCommon Hypothesis Tests:\n================================================================================\n\nOne-sample t-test:\n  Purpose: Compare sample mean to known value\n  Data Type: Continuous\n  Python: stats.ttest_1samp()\n\nTwo-sample t-test:\n  Purpose: Compare means of two groups\n  Data Type: Continuous\n  Python: stats.ttest_ind()\n\nPaired t-test:\n  Purpose: Compare paired observations\n  Data Type: Continuous\n  Python: stats.ttest_rel()\n\nOne-sample z-test:\n  Purpose: Compare sample mean (known σ)\n  Data Type: Continuous\n  Python: stats.normaltest()\n\nChi-square goodness of fit:\n  Purpose: Test if data fits distribution\n  Data Type: Categorical\n  Python: stats.chisquare()\n\nChi-square independence:\n  Purpose: Test independence of variables\n  Data Type: Categorical\n  Python: stats.chi2_contingency()"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#activity-practice-problem",
    "href": "files/lecture_notes/lecture12/lecture12.html#activity-practice-problem",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Activity: Practice Problem",
    "text": "Activity: Practice Problem\n\n\n\n\n\n\nYour Turn!\n\n\n\nA coffee shop claims their average wait time is 5 minutes. You collect data on 20 customers and find: - Sample mean: 5.8 minutes - Sample standard deviation: 2.1 minutes\nQuestions: 1. Set up appropriate hypotheses (use α = 0.05) 2. What type of test should you use? 3. Calculate the test statistic and p-value 4. Make a decision and interpret the results 5. What are the practical implications?\n\n\n\nThink about: Is this a one-tailed or two-tailed test? What assumptions do you need to check?"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#practice-problem-solution",
    "href": "files/lecture_notes/lecture12/lecture12.html#practice-problem-solution",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Practice Problem Solution",
    "text": "Practice Problem Solution\n\n\nPractice Problem Solution:\n========================================\n1. Hypotheses:\n   H₀: μ = 5 (average wait time is 5 minutes)\n   H₁: μ ≠ 5 (average wait time is different from 5 minutes)\n   (Two-tailed test - we're testing if it's different, not specifically longer)\n\n2. Test type: One-sample t-test\n   (Population standard deviation unknown, small sample)\n\n3. Calculations:\n   t = (5.8 - 5.0) / (2.1 / √20) = 1.704\n   df = 20 - 1 = 19\n   p-value = 0.1047\n\n4. Decision:\n   Fail to reject H₀ (p = 0.1047, α = 0.05)\n   Conclusion: There is insufficient evidence that wait time differs from 5 minutes.\n\n5. Practical implications:\n   The actual average wait time appears to be about 5.8 minutes,\n   which is 0.7999999999999998 minutes longer than claimed.\n   Management should investigate why wait times exceed the 5-minute target."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#common-mistakes-and-pitfalls",
    "href": "files/lecture_notes/lecture12/lecture12.html#common-mistakes-and-pitfalls",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Common Mistakes and Pitfalls",
    "text": "Common Mistakes and Pitfalls\n\n\n\n\n\n\nAvoid These Common Errors\n\n\n\n\nConfusing practical vs. statistical significance\n\nLarge samples can detect tiny, meaningless differences\nAlways consider effect size and practical importance\n\nP-hacking / Data dredging\n\nTesting multiple hypotheses until finding significance\nSolution: Adjust α, pre-specify analyses\n\nMisinterpreting p-values\n\nP-value ≠ probability that H₀ is true\nP-value ≠ probability of making an error\n\nIgnoring assumptions\n\nCheck normality, independence, equal variances\nUse appropriate alternatives when violated\n\nChoosing α after seeing results\n\nAlways set significance level before analysis\nAvoid changing criteria to get desired results"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#statistical-significance-vs.-practical-significance",
    "href": "files/lecture_notes/lecture12/lecture12.html#statistical-significance-vs.-practical-significance",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Statistical Significance vs. Practical Significance",
    "text": "Statistical Significance vs. Practical Significance\n\n\n\n\n\n\n\n\n\nKey Lesson: Statistical significance ≠ Practical importance\nLeft: Tiny effect (0.1) but significant due to large n\nRight: Large effect (8.7) but not significant due to small n"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#best-practices-for-hypothesis-testing",
    "href": "files/lecture_notes/lecture12/lecture12.html#best-practices-for-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Best Practices for Hypothesis Testing",
    "text": "Best Practices for Hypothesis Testing\n\n\nPlan before you collect data\n\nPre-specify hypotheses, α level, and analysis plan\nCalculate required sample size (power analysis)\n\nCheck your assumptions\n\nUse diagnostic plots and tests\nConsider robust alternatives if violated\n\nReport effect sizes\n\nP-values don’t tell the whole story\nInclude confidence intervals for estimates\n\nConsider practical significance\n\nIs the difference meaningful in context?\nWhat are the costs/benefits of different decisions?\n\nBe honest about multiple testing\n\nAdjust for multiple comparisons when appropriate\nReport all tests performed, not just significant ones"
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#summary-the-logic-of-hypothesis-testing",
    "href": "files/lecture_notes/lecture12/lecture12.html#summary-the-logic-of-hypothesis-testing",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Summary: The Logic of Hypothesis Testing",
    "text": "Summary: The Logic of Hypothesis Testing\n\n\nStart with skepticism (assume H₀ is true)\nCollect evidence (sample data)\nQuantify surprise (how unusual is this result if H₀ were true?)\nMake a decision (is the evidence strong enough to reject H₀?)\nAcknowledge uncertainty (we might be wrong!)\n\n\n\nRemember: Hypothesis testing doesn’t prove anything definitively. It provides a framework for making decisions under uncertainty using probabilistic reasoning."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#key-takeaways",
    "href": "files/lecture_notes/lecture12/lecture12.html#key-takeaways",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nHypothesis testing helps us make decisions about populations using sample data\nP-values tell us how surprising our data would be if H₀ were true\nStatistical significance ≠ practical importance\nAlways check assumptions and consider effect sizes\nPlan your analysis before collecting data\nBe aware of Type I and Type II errors\n\n\nNext steps: Practice with different types of tests, learn about confidence intervals, and explore more advanced topics like multiple testing corrections and non-parametric alternatives."
  },
  {
    "objectID": "files/lecture_notes/lecture12/lecture12.html#appendix-python-code-templates",
    "href": "files/lecture_notes/lecture12/lecture12.html#appendix-python-code-templates",
    "title": "Hypothesis Testing: Making Decisions with Data",
    "section": "Appendix: Python Code Templates",
    "text": "Appendix: Python Code Templates\n\n# Template for one-sample t-test\nimport numpy as np\nfrom scipy import stats\n\n# Your data\ndata = [...]  # Replace with your data\nnull_value = 0  # Replace with your null hypothesis value\n\n# Perform test\nt_stat, p_value = stats.ttest_1samp(data, null_value)\n\n# For one-tailed test, divide p-value by 2\np_value_one_tailed = p_value / 2\n\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value (two-tailed): {p_value:.4f}\")\nprint(f\"p-value (one-tailed): {p_value_one_tailed:.4f}\")\n\n# Template for two-sample t-test\ngroup1 = [...]  # Replace with your first group\ngroup2 = [...]  # Replace with your second group\n\n# Perform test\nt_stat, p_value = stats.ttest_ind(group1, group2, equal_var=True)\n\nprint(f\"t-statistic: {t_stat:.3f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# Effect size (Cohen's d)\ndef cohens_d(x, y):\n    nx, ny = len(x), len(y)\n    dof = nx + ny - 2\n    pooled_std = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / dof)\n    return (np.mean(x) - np.mean(y)) / pooled_std\n\nd = cohens_d(group1, group2)\nprint(f\"Cohen's d: {d:.3f}\")"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "From Samples to Populations: Understanding Uncertainty\n\n\n\n\n\n\nWhen:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions\n\n\n\n\n\n\nBig Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation\n\n\n\n\n\n\n\n\n\nObservational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty.\n\n\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n\n\n\n\nThink of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)\n\n\n\n\n\n\n\n\n\nImagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values.\n\n\n\n\n\n\n\n\n\nThink of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!\n\n\n\n\n\n\n\n\n\n\n\n                            \n                                            \nCentral Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!\n\n\n\n\n\n\n\n\nWhat it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!\n\n\n\n\n\n\n\n\n\nImagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”\n\n\n\n\n\n\n\n\n\nBig Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes\n\n\n\n\n\n\n\n\nOffice Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#announcements",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#announcements",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "When:\n- 📅 Date: Friday, July 25\n- ⏰ Window: Available 7 AM – 12 AM\n- ⏳ Duration: 1 hour (once you begin)\nWhere:\n- 💻 Online on Canvas\nCovers:\n- 📚 Material from Weeks 3-4\nNote:\n- 📸 Upload a photo of your written work for some questions"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#what-well-learn-today",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#what-well-learn-today",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Big Ideas:\n\nHow sample means behave (they’re surprisingly predictable!)\nWhy we use intervals instead of single numbers\nHow confident we can be in our estimates\nPlanning studies for the right precision\n\n\nSkills You’ll Gain:\n\nBuild confidence intervals step-by-step\nInterpret what confidence really means\nChoose appropriate sample sizes\nAvoid common mistakes in interpretation"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#from-observation-experimentation-why-design-matters",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Observational Study: Passively record what already happens — good for spotting patterns, but hidden confounders can fool us.\nControlled Experiment: Actively assign treatments; randomization balances the lurking variables so we can talk about cause.\nRandomization & Replication: Twin shields that protect us from bias and one‑off flukes.\nProbability Framework: Sample space, events, and probabilities let us quantify uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#the-big-picture-from-sample-to-population",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Think of this like trying to understand a huge library by reading just a few books\n\n\n                            \n                                            \n\n\n\nKey Terms Made Simple:\n\nPopulation (\\(\\mu\\)): Everyone we care about (like all students at UCSB)\nSample (\\(\\bar x\\)): The people we actually measured (like 100 students we surveyed)\nParameter: The true answer we want (population mean)\nStatistic: Our best guess (sample mean)"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#why-point-estimates-arent-enough",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#why-point-estimates-arent-enough",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Imagine asking: “What’s the average height of UCSB students?”\n\n\n                            \n                                            \n\n\n\nThe Problem: Each sample gives a slightly different answer!\n\nThe Solution: Use confidence intervals to show the range of reasonable values."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#sampling-distributions",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Think of sampling distributions like “What if we repeated our study 1000 times?”\n\n\n                            \n                                            \n\n\n\nKey Insights:\n\nPopulation can be any shape (skewed, normal, weird)\nOne sample might not look like the population\nSample means vary from sample to sample\nMany sample means form a beautiful normal distribution!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#the-central-limit-theorem-clt",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#the-central-limit-theorem-clt",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Central Limit Theorem: Larger Samples → More Normal!\n\n\n\nThe Magic Rule: No matter what shape your population is, sample means will be approximately normal!\nThe Rule of Thumb: With n ≥ 30, sample means will be approximately normal, regardless of population shape!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#standard-error",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#standard-error",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "What it measures\n\nStandard deviation (\\(\\sigma\\) or \\(s\\)): spread of individual data points\n\nStandard error (SE): spread of sample means\n\n\\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\qquad(\\text{if } \\sigma \\text{ is known})\n\\]\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\qquad(\\text{usual case, } \\sigma \\text{ unknown})\n\\]\nKey facts\n\nSE shrinks at rate \\(1/\\sqrt{n}\\) — every 4× more observations ⇒ ½ the SE\n\nSmaller SE ⇒ narrower confidence intervals and more powerful tests\n\n\n\n\n\n                            \n                                            \n\n\n\n\n\n\n\n\nImportant\n\n\n\nDoubling your sample size doesn’t halve the error - you need 4 times the sample for half the error!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#confidence-intervals-the-intuitive-idea",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Imagine you’re trying to guess someone’s height just by looking at their shadow…\n\n\n                            \n                                            \n\n\n\nSimple Interpretation: “We’re \\(95\\\\%\\) confident the true average height is between \\(64.3\\) and \\(70.1\\) inches.”"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#key-takeaways",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Big Ideas: 1. Samples vary - confidence intervals capture this uncertainty 2. Larger samples give more precise estimates\n3. Higher confidence means wider intervals 4. The CLT makes normal-based inference possible\n\nPractical Skills: - Build CIs for means and proportions - Interpret confidence correctly - Plan sample sizes for desired precision - Avoid common interpretation mistakes"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9_intro.html#questions",
    "href": "files/lecture_notes/lecture10/lecture9_intro.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "Office Hours: Thursday 11AM (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and p-values\n“The goal is not to eliminate uncertainty, but to understand and work with it”"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "",
    "text": "🏠"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#todays-learning-objectives",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#todays-learning-objectives",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Today’s Learning Objectives",
    "text": "Today’s Learning Objectives\nBy the end of this lecture, you will be able to:\n\nUnderstand sampling distributions and their properties (Section 1.2)\nApply the Central Limit Theorem to sampling (Section 1.4)\nConstruct confidence intervals for population means (Section 1.6)\nConstruct confidence intervals for population proportions (Section 1.8)\nInterpret confidence intervals correctly (Section 1.5)\nDetermine appropriate sample sizes for desired precision\nUse python to calculate confidence intervals\nDistinguish between different types of sampling methods"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-sampling-dist",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-sampling-dist",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "The Big Picture: Statistical Inference",
    "text": "The Big Picture: Statistical Inference\n\n\n\n\nPopulation vs Sample\n\n\n\nPopulation: All individuals of interest\n\n\nSample: Subset we actually observe\n\n\nParameter: Population characteristic (\\(\\mu\\), \\(p\\))\n\n\nStatistic: Sample characteristic (\\(\\bar{x}\\), \\(\\hat{p}\\))\n\n\n\nGoal: Use sample statistics to estimate population parameters\n\n\n\n\n\n\n\n\n\nWhy Confidence Intervals?\n\n\n\nPoint estimates are rarely exactly correct\n\n\nInterval estimates capture uncertainty\n\n\nConfidence level quantifies our certainty\n\n\nMargin of error shows precision\n\n\n\nKey Insight: We trade precision for confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sampling-distributions",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sampling-distributions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nA sampling distribution is the distribution of a statistic (like \\(\\bar{x}\\)) across all possible samples of size \\(n\\).\n\n\n\nKey Properties:\nCenter:\n\\(E[\\bar{X}] = \\mu\\) (unbiased)\nSpread:\n\\(SE(\\bar{X}) = \\frac{\\sigma}{\\sqrt{n}}\\)\nShape:\nApproaches normal as \\(n\\) increases (Central Limit Theorem)\nStandard Error vs Standard Deviation:\n\n\\(\\sigma\\): spread of individual observations\n\\(SE = \\frac{\\sigma}{\\sqrt{n}}\\): spread of sample means\n\n\n\n\n\n\nDrag the slider to see how sample size affects the sampling distribution"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-clt-sampling",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-clt-sampling",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Central Limit Theorem in Action",
    "text": "Central Limit Theorem in Action\n\n\n\n\nNew Population\n\n Uniform Population Exponential Population Bimodal Population Right-Skewed Population  Sample Size:  Collect 1000 Sample Means\n\n\n\nPopulation μ: - | Sample Means μ: - | Standard Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-interpretation",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-interpretation",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals: The Concept",
    "text": "Confidence Intervals: The Concept\n\n\nWhat is a Confidence Interval? A confidence interval provides a range of plausible values for a population parameter. 95% Confidence Interval: If we repeated our sampling process many times, about \\(95\\%\\) of the intervals we construct would contain the true population parameter.\n\n\n\n\n\nClick to generate new 95% confidence intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-means",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Means",
    "text": "Confidence Intervals for Population Means\n\n\n\n\n 🎯 When σ is Known:\n\n\n\\[\\bar{x} \\pm z^* \\cdot \\frac{\\sigma}{\\sqrt{n}}\\]\n\n\nWhen \\(\\sigma\\) is Unknown (more common):\n\n\n\\[\\bar{x} \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\bar{x}\\): sample mean\n\n\n\\(t^*\\): critical value (df = n-1)\n\n\n\\(\\frac{s}{\\sqrt{n}}\\): standard error\n\n\n\n\n\n\nCommon Confidence Levels:\n\n\n\n90%: z* = 1.645, more precise\n\n\n95%: z* = 1.96, most common\n\n\n99%: z* = 2.576, more confident\n\n\n\nConditions Required:\n\n\n\nRandom sampling\nNearly normal population OR n ≥ 30\nIndependent observations"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-means",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Means",
    "text": "Interactive CI Demo: Confidence Intervals for Means\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population μ:  Population σ:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures μ? - | Margin of Error: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-proportions",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sec-ci-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals for Population Proportions",
    "text": "Confidence Intervals for Population Proportions\n\n\n\n 🎯 Formula:\n\n\n\\[\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\n\nKey Components:\n\n\n\n\\(\\hat{p} = \\frac{x}{n}\\): sample proportion\n\n\n\\(z^*\\): critical value\n\n\n\\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\): standard error\n\n\n\n\nConditions Required:\n\n\n\nRandom sampling\n\n\n\\(n\\hat{p} \\geq 10\\) and \\(n(1-\\hat{p}) \\geq 10\\)\n\n\nIndependent observations\n\n\nPopulation at least 10× sample size\n\n\n\nConservative Approach:\n\n\nUse \\(\\hat{p} = 0.5\\) for planning when true proportion unknown (maximizes margin of error)"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#interactive-ci-demo-confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Interactive CI Demo: Confidence Intervals for Proportions",
    "text": "Interactive CI Demo: Confidence Intervals for Proportions\n\n\n\nSample Size:  Confidence Level:  90% 95% 99%   Population p:  Generate New Sample & CI Simulate 100 CIs\n\n\n\nCurrent CI: Generate a sample to see CI Captures p? - | Sample Proportion: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-1-ci-for-mean",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 1: CI for Mean",
    "text": "Practice Problem 1: CI for Mean\n\nA random sample of 25 college students shows a mean daily screen time of 6.2 hours with a standard deviation of 1.8 hours. (a) Construct a 95% confidence interval for the mean daily screen time. (b) Interpret the confidence interval in context. (c) What would happen to the interval width if we used 99% confidence instead? Show Solution\n\nSolution. (a)\nGiven: \\(n = 25\\), \\(\\bar{x} = 6.2\\), \\(s = 1.8\\), 95% confidence\nFor \\(df = 24\\), \\(t^* = 2.064\\)\n\\(SE = \\frac{s}{\\sqrt{n}} = \\frac{1.8}{\\sqrt{25}} = 0.36\\)\n\\(CI = 6.2 \\pm 2.064 \\times 0.36 = 6.2 \\pm 0.743 = (5.46, 6.94)\\) hours\n(b)\nWe are 95% confident that the true mean daily screen time for all college students is between \\(5.46\\) and \\(6.94\\) hours.\n(c)\nFor 99% confidence, we use \\(t^* = 2.797\\), giving a wider interval: \\((5.19, 7.21)\\) hours."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-2-ci-for-proportion",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 2: CI for Proportion",
    "text": "Practice Problem 2: CI for Proportion\n\nIn a survey of 400 voters, 240 support a particular candidate. (a) Construct a 90% confidence interval for the true proportion of supporters. (b) Check if the conditions for inference are met. (c) How large a sample would be needed for a margin of error of 0.03 with 95% confidence? Show Solution\n\nSolution. (a)\n\\(\\hat{p} = \\frac{240}{400} = 0.6\\), \\(n = 400\\), 90% confidence, \\(z^* = 1.645\\)\n\\(SE = \\sqrt{\\frac{0.6 \\times 0.4}{400}} = \\sqrt{\\frac{0.24}{400}} = 0.0245\\)\n\\(CI = 0.6 \\pm 1.645 \\times 0.0245 = 0.6 \\pm 0.0403 = (0.560, 0.640)\\)\n(b)\nCheck conditions: \\(n\\hat{p} = 400 \\times 0.6 = 240 \\geq 10\\) ✓\n\\(n(1-\\hat{p}) = 400 \\times 0.4 = 160 \\geq 10\\) ✓\n(c)\nSample size calculation:\n\\(n = \\frac{(z^*)^2 \\hat{p}(1-\\hat{p})}{ME^2} =\n\\frac{(1.96)^2 \\times 0.6 \\times 0.4}{(0.03)^2} =\n\\frac{0.9216}{0.0009} = 1024\\) people"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#practice-problem-3-sample-size-planning",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Practice Problem 3: Sample Size Planning",
    "text": "Practice Problem 3: Sample Size Planning\n\nA market researcher wants to estimate the average amount spent on coffee per week by college students. (a) How large a sample is needed for a 95% CI with margin of error $2 if \\(\\sigma\\) = $8? (b) If the budget only allows for 100 students, what confidence level gives a $2 margin of error? (c) What’s the trade-off between sample size, confidence level, and precision?\n\nShow Solution\n\n\nSolution. (a)\nFor means:\n\\(n = \\frac{(z^*)^2 \\sigma^2}{ME^2} =\n\\frac{(1.96)^2 \\times 8^2}{2^2} =\n\\frac{245.86}{4} = 62\\) students\n(b)\nWith \\(n = 100\\):\n\\(ME = z^* \\frac{\\sigma}{\\sqrt{n}} =\nz^* \\frac{8}{\\sqrt{100}} =\n0.8 z^*\\)\nFor \\(ME = 2\\):\n\\(z^* = \\frac{2}{0.8} = 2.5\\),\nwhich corresponds to about 98.8% confidence\n(c) Trade-offs:\n\nHigher confidence \\(\\rightarrow\\) wider intervals (less precision)\nLarger sample \\(\\rightarrow\\) narrower intervals (more precision)\nLower margin of error \\(\\rightarrow\\) need larger sample or lower confidence"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#common-mistakes-and-misconceptions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Common Mistakes and Misconceptions",
    "text": "Common Mistakes and Misconceptions\n\n\nInterpretation Errors\n❌ Wrong: “\\(95\\%\\) of the data falls in this interval”\n✅ Right: “We’re \\(95\\%\\) confident the parameter is in this interval”\n❌ Wrong: “There’s a \\(95\\%\\) chance \\(\\mu\\) is in this interval”\n✅ Right: “\\(95\\%\\) of such intervals contain \\(\\mu\\)”\n\nTechnical Errors\n\nUsing \\(z*\\) when σ is unknown and \\(n &lt; 30\\)\nForgetting to check conditions\nConfusing standard error with standard deviation\nUsing wrong degrees of freedom for t-distribution\n\n\nRemember: The confidence level refers to the long-run proportion of intervals that capture the parameter!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#sample-size-and-margin-of-error-relationships",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Sample Size and Margin of Error Relationships",
    "text": "Sample Size and Margin of Error Relationships\n\n\nPopulation σ:  Confidence Level:  90% 95% 99%   Desired Margin of Error: \n\n\n\n\n\n\nSample Size vs Margin of Error\n\n\n\n\nRequired Sample Size: - | Resulting ME: -"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Types of Sampling Methods",
    "text": "Types of Sampling Methods\n\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nAdvantages\nDisadvantages\n\n\n\n\nSimple Random\nEvery individual has equal chance\nUnbiased, simple\nMay not represent subgroups\n\n\nStratified\nSample from each subgroup\nEnsures representation\nMore complex\n\n\nCluster\nSample entire groups\nCost-effective for spread populations\nHigher variability\n\n\nSystematic\nEvery k-th individual\nSimple to implement\nCan miss patterns\n\n\nConvenience\nEasily accessible individuals\nQuick and cheap\nHighly biased\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSampling Method Matters: Only probability sampling methods allow for valid statistical inference!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#confidence-intervals-in-practice",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#confidence-intervals-in-practice",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Confidence Intervals in Practice",
    "text": "Confidence Intervals in Practice\n\n\n\nWhen to Use Each Type\nMeans: Continuous data (height, income, test scores)\nProportions: Categorical data (yes/no, success/failure)\nChoosing Confidence Level\n\n90%: Quick estimates, less critical decisions\n95%: Standard in most research\n99%: High-stakes decisions, medical trials\n\n\nReal-World Applications\n\nPolitical polls: Proportion confidence intervals\nQuality control: Mean confidence intervals\nMedical research: Both types with high confidence\nBusiness analytics: Varies by decision importance\n\nCommunication Tips\n\nAlways include the confidence level\nState what the interval estimates\nAcknowledge the uncertainty\nConsider practical significance"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#key-takeaways",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#key-takeaways",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nMain Concepts\n\nSampling distributions follow predictable patterns\nConfidence intervals quantify uncertainty\nCentral Limit Theorem makes normal-based inference possible\nSample size directly affects precision\n\n\nPractical Guidelines Choose appropriate methods based on:\n\nData type (continuous vs categorical)\nSample size (use t when σ unknown)\nDesired precision (affects sample size)\nConfidence level (affects interval width)\n\nKey Principle Statistical inference allows us to make informed decisions about populations using sample data, while properly accounting for uncertainty."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#looking-ahead",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#looking-ahead",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Looking Ahead",
    "text": "Looking Ahead\n\nNext Lecture: Hypothesis Testing\nTopics we’ll cover:\n\nNull and alternative hypotheses\nTest statistics and p-values\nType I and Type II errors\n\n\nConnection: Confidence intervals and hypothesis tests are two sides of the same statistical inference coin"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#questions",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#questions",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Questions?",
    "text": "Questions?\n\nOffice Hours: 11AM on Thursday (link on Canvas)\nEmail: nmathlouthi@ucsb.edu\nNext Class: Hypothesis Testing and Statistical Significance"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture9-interactive.html#resources",
    "href": "files/lecture_notes/lecture10/lecture9-interactive.html#resources",
    "title": "PSTAT 5A: Sampling and Confidence Intervals",
    "section": "Resources",
    "text": "Resources\n\n  \n    \n      \n      Read OpenIntro Statistics Chapter 5 sections 5.1-5.3\n    \n    \n      \n      Khan Academy - Confidence Intervals\n    \n    \n      \n      Seeing Theory - Frequentist Inference\n    \n    \n      \n      Confidence Intervals - Wikipedia\n    \n    \n      \n      Understanding Different Types of Intervals"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#important-announcements",
    "href": "files/lecture_notes/lecture10/lecture10.html#important-announcements",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "When:\n- 📅 Date: Friday, July 25\n- ⏰ Window: 7 AM – 12 AM\n- ⏳ Duration: 1 hour once started\nWhere: 💻 Online via Canvas\nCovers: Material from Weeks 3-4\n\n\n\n\n\nDiscrete & continuous distributions\nProbability calculations\nExpected value & variance\nNormal distribution applications\nNote: Upload photos of written work for calculation problems"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#todays-learning-journey",
    "href": "files/lecture_notes/lecture10/lecture10.html#todays-learning-journey",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "Why sampling? The power and necessity of statistical inference\nSample behavior - How sample means form predictable patterns\nUncertainty quantification - From point estimates to intervals\nThe CLT magic - Why normal distributions appear everywhere\nConfidence intervals - Our bridge from samples to populations\n\n\n\n\n\n\nDesign effective sampling strategies\nCalculate and interpret standard errors\nApply the Central Limit Theorem\nConstruct and interpret confidence intervals\nChoose appropriate sample sizes for desired precision\nRecognize and avoid sampling bias"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#the-foundation-why-do-we-sample",
    "href": "files/lecture_notes/lecture10/lecture10.html#the-foundation-why-do-we-sample",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "Population vs. Sample Realities:\n\nTime: Surveying 40,000 UCSB students takes months\nCost: Each measurement costs money and resources\nLogistics: Some populations are impossible to reach entirely\nFeasibility: Testing every light bulb would destroy the product\n\n\n\n\nUse a representative sample to make valid inferences about the entire population"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#study-design-the-foundation-of-good-inference",
    "href": "files/lecture_notes/lecture10/lecture10.html#study-design-the-foundation-of-good-inference",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "🔍 Observational Studies: - Observe what naturally occurs - Good for identifying associations - ⚠️ Cannot establish causation due to confounding\n\n🧪 Randomized Experiments: - Actively assign treatments randomly - Controls for confounding variables - ✅ Can establish causal relationships"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#types-of-sampling-methods",
    "href": "files/lecture_notes/lecture10/lecture10.html#types-of-sampling-methods",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "1. Simple Random Sampling (SRS)\nEvery individual has equal chance of selection\nGold standard for inference\n2. Stratified Sampling\nDivide population into groups (strata)\nSample randomly within each group\nEnsures representation of subgroups\n\n\n3. Cluster Sampling\nDivide into clusters, randomly select clusters\nSample all/some individuals within chosen clusters\nCost-effective for large populations\n4. Systematic Sampling\nSelect every \\(kth\\) individual from ordered list\nSimple but can introduce bias if pattern exists"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#sampling-bias-what-can-go-wrong",
    "href": "files/lecture_notes/lecture10/lecture10.html#sampling-bias-what-can-go-wrong",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "Selection Bias - Systematic exclusion of certain groups - Example: Online surveys miss non-internet users\nResponse Bias\n- Who chooses to respond affects results - Example: Satisfaction surveys - unhappy customers more likely to respond\nNonresponse Bias - Missing data isn’t random - Example: Wealthy people less likely to disclose income\nConvenience Sampling - Sampling whoever is easiest to reach - Example: Surveying only students in your dorm\n\n\n\n\n                            \n                                            \n\n\n💡 Key Insight: Bias can’t be fixed by increasing sample size!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#the-magic-of-sample-means-from-chaos-to-order",
    "href": "files/lecture_notes/lecture10/lecture10.html#the-magic-of-sample-means-from-chaos-to-order",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "🎲 The Setup:\n\nTake many samples from the same population\nCalculate the mean of each sample\nPlot all these sample means\nObserve the magic!\n\n🎯 What We Discover:\n\nSample means cluster around the true population mean\nThey form a predictable pattern (normal distribution!)\nLarger samples give more consistent results"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#the-central-limit-theorem",
    "href": "files/lecture_notes/lecture10/lecture10.html#the-central-limit-theorem",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "For a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), when sample size \\(n\\) is large enough:\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\]\nOr equivalently: \\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\n\n\n\n\nRule of Thumb: \\(n \\geq 30\\) usually works\nShape doesn’t matter: Works for ANY population distribution\n\nLarger \\(n\\) = Better approximation"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#standard-error-measuring-our-uncertainty",
    "href": "files/lecture_notes/lecture10/lecture10.html#standard-error-measuring-our-uncertainty",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "Standard Error (SE) measures how much sample means vary from sample to sample.\n\\[SE = \\frac{\\sigma}{\\sqrt{n}} \\text{ (when } \\sigma \\text{ is known)}\\]\n\\[SE = \\frac{s}{\\sqrt{n}} \\text{ (usual case, } \\sigma \\text{ unknown)}\\]\n\n\n\n\nSmaller SE = More precise estimates\nSE decreases as sample size increases\nRate of decrease: \\(SE \\propto 1/\\sqrt{n}\\)\n4× larger sample = ½ the uncertainty!"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#confidence-intervals-our-bridge-to-the-population",
    "href": "files/lecture_notes/lecture10/lecture10.html#confidence-intervals-our-bridge-to-the-population",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "A confidence interval gives us a range of plausible values for the population parameter.\nFor a population mean: \\[\\bar{x} \\pm z^* \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\n\n\n90% CI: \\(z^* = 1.645\\)\n95% CI: \\(z^* = 1.96\\)\n99% CI: \\(z^* = 2.576\\)\n\n\n\n\n“We are 95% confident that the true population mean lies between [lower bound] and [upper bound]”"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#sample-size-planning-getting-it-right",
    "href": "files/lecture_notes/lecture10/lecture10.html#sample-size-planning-getting-it-right",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "To achieve margin of error \\(E\\) with confidence level \\((1-\\alpha)\\):\n\\[n = \\left(\\frac{z^*\\sigma}{E}\\right)^2\\]\n\n\n\nMargin of Error Trade-offs: - Smaller \\(E\\) requires larger \\(n\\) - Higher confidence requires larger \\(n\\)\n- More variable population requires larger \\(n\\)\nPractical Constraints: - Budget limitations - Time constraints\n- Availability of participants"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#putting-it-all-together-a-real-example",
    "href": "files/lecture_notes/lecture10/lecture10.html#putting-it-all-together-a-real-example",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "“What is the average height of UCSB students?”\nOur Approach:\n\nPopulation: All 26,000 UCSB students\nSample: Random sample of 100 students\nMeasurement: Height in inches\nGoal: 95% confidence interval for population mean\n\nResults:\n\nSample mean: \\(\\bar{x} = 68.2\\) inches\nSample std dev: \\(s = 4.1\\) inches\nSample size: \\(n = 100\\)\n\n\n\n\n\n                            \n                                            \n\n\n🎯 Interpretation: We are 95% confident that the true average height of UCSB students is between 67.40 and 69.00 inches."
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#key-takeaways-your-statistical-toolkit",
    "href": "files/lecture_notes/lecture10/lecture10.html#key-takeaways-your-statistical-toolkit",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "1. Sampling Wisdom\n\nRepresentative samples beat large biased samples\nRandomization is your best friend\nBias can’t be fixed with larger samples\n\n2. The CLT Magic\n\nSample means are approximately normal (\\(n ≥ 30\\))\nWorks for ANY population distribution\nEnables powerful statistical inference\n\n3. Standard Error\n\nMeasures precision of our estimates\nDecreases with \\(\\sqrt{n}\\), not \\(n\\)\nKey ingredient in confidence intervals\n\n\n\n\n\n4. Confidence Intervals\n\nQuantify uncertainty in our estimates\nCorrect interpretation is crucial\nWidth depends on confidence level and sample size\n\n5. Sample Size Planning\n\nBalance precision needs with resources\nConsider margin of error requirements\nAccount for practical constraints\n\n6. Quality Control\n\nAlways check for potential bias\nVerify assumptions (normality, independence)\nConsider the broader context"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#interactive-practice-test-your-understanding",
    "href": "files/lecture_notes/lecture10/lecture10.html#interactive-practice-test-your-understanding",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "1. Sample Size Question: If we want to halve our margin of error, by what factor should we increase our sample size?\n2. CLT Application:\nA population has a right-skewed distribution. What can we say about the distribution of sample means when n = 50?\n3. CI Interpretation: We calculated a 95% CI as (45, 55). What does this mean?\n4. Bias Detection: An online survey about internet usage gets 10,000 responses. What type of bias might be present?\n\n\n\n\n1. Increase by factor of 4 (since \\(SE \\propto \\frac{1}{\\sqrt{n}}\\))\n2. Sample means will be approximately normal regardless of population shape\n3. We’re 95% confident the true population parameter is between 45 and 55\n4. Selection bias - excludes people without internet access"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#comprehensive-resources",
    "href": "files/lecture_notes/lecture10/lecture10.html#comprehensive-resources",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "OpenIntro Statistics\n\nSection 1.3: Sampling principles and strategies\nSection 3.3: Confidence intervals for a mean\nSection 4.1: Central Limit Theorem\n\n\n\n\n\n\nKhan Academy: Central Limit Theorem\nStatQuest: Confidence Intervals Explained\n3Blue1Brown: Central Limit Theorem Visualization\n\n\n\n\n\n\nSeeing Theory: Probability Visualizations\nRossman & Chance Applets: Sampling Distributions\nCentral Limit Theorem Simulator\n\n\n\n\n\nOffice Hours: Thursday 11 AM-12 PM (Zoom link on Canvas)\nEmail: nmathlouthi@ucsb.edu\n\n\n\n\nNext Lecture: Hypothesis Testing and p-values"
  },
  {
    "objectID": "files/lecture_notes/lecture10/lecture10.html#questions-discussion",
    "href": "files/lecture_notes/lecture10/lecture10.html#questions-discussion",
    "title": "PSTAT 5A: Sampling Principles and Strategies",
    "section": "",
    "text": "“The goal is not to eliminate uncertainty, but to understand and quantify it intelligently”\nKey Questions for Reflection: - How do we balance precision with practicality?\n\nWhen might a larger sample actually be worse?\nWhat makes a “good” confidence interval?\nHow do we communicate uncertainty to non-statisticians?\n\n\n\n\n\nComing Up: Hypothesis Testing\n\nWhat are null and alternative hypotheses?\nHow do we make decisions with data?\nWhat does a p-value really mean?\nType I and Type II errors\n\nRecommended Prep:\n\nReview today’s confidence interval concepts\nThink about yes/no questions you’d test with data\nConsider what “statistical significance” means to you"
  }
]