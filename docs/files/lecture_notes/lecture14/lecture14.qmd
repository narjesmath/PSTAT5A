---
title: "Linear Regression"
subtitle: "A Comprehensive Guide to Predictive Modeling"
author: "Narjes Mathlouthi"
date: today
format:
  revealjs:
    preview-links: auto
    logo: /img/logo.png
    theme: default
    css: new-style.css
    slide-number: true
    footer: "Linear Regression - Understanding relationships between variables"
    transition: slide
    background-transition: fade
jupyter: pstat5a
execute:
  echo: false
  warning: false
  message: false
---

```{python setup, include=False}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
```

## What We'll Cover {.smaller}

::::: {.columns}

:::: {.column width="50%"}
**Core Concepts:**

- Simple & Multiple Regression
- Least Squares Method  
- Model Assumptions
- Coefficient Interpretation
- Model Evaluation

::::

:::: {.column width="50%"}
**Advanced Topics:**

- Hypothesis Testing
- Residual Analysis
- Confidence Intervals  
- Common Problems
- Practical Examples

::::

:::::

::: {.notes}
This comprehensive presentation will take you from the basics of linear regression through advanced diagnostic techniques and real-world applications using Python.
:::

## What is Linear Regression?

**Linear regression** is a statistical method for modeling the relationship between a dependent variable (Y) and one or more independent variables (X).

### Key Objectives:

- **Prediction:** Estimate values of Y for given values of X
- **Explanation:** Understand how X variables affect Y  
- **Relationship Quantification:** Measure strength and direction of relationships

::: {.callout-note}
## The Linear Relationship Concept
$$Y = \beta_0 + \beta_1X + \varepsilon$$

*"Y changes linearly as X changes, plus some random error"*
:::

## Real-World Applications

::: {.incremental}
- **Business:** Sales revenue vs. advertising spend
- **Economics:** GDP growth vs. unemployment rate  
- **Medicine:** Blood pressure vs. age and weight
- **Real Estate:** House price vs. size, location, age
- **Education:** Test scores vs. study hours and attendance
:::

## Types of Linear Regression

::::: {.columns}

:::: {.column width="50%"}
### Simple Linear Regression
$$Y = \beta_0 + \beta_1X + \varepsilon$$

- One dependent variable (Y)
- One independent variable (X)  
- Creates a straight line
- Easier to interpret and visualize

::: {.callout-tip}
**Example:** Predicting house price based only on square footage
:::

::::

:::: {.column width="50%"}
### Multiple Linear Regression
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \varepsilon$$

- One dependent variable (Y)
- Multiple independent variables
- More realistic for complex relationships  
- Better predictive accuracy

::: {.callout-tip}
**Example:** Predicting house price based on size, bedrooms, age, location
:::

::::

:::::

## The Linear Regression Model

### Population Model (True Relationship)
$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \varepsilon$$

### Sample Model (Estimated Relationship)  
$$\hat{Y} = b_0 + b_1X_1 + b_2X_2 + ... + b_kX_k$$

## Model Components Explained

| Symbol | Name | Description |
|--------|------|-------------|
| Y | Dependent Variable | What we're trying to predict or explain |
| $X_1, X_2, ...$ | Independent Variables | Predictor variables, features, or explanatory variables |
| $\beta_0$ | Intercept | Value of Y when all X variables equal zero |
| $\beta_1, \beta_2, ...$ | Slope Coefficients | Change in Y for one-unit change in X (holding others constant) |
| $\varepsilon$ | Error Term | Random variation not explained by the model |
| $\hat{Y}$ | Predicted Value | Model's prediction for Y |

::: {.callout-important}
We use sample data to estimate the unknown population parameters ($\beta_0, \beta_1, \beta_2, ...$) with our sample estimates ($b_0, b_1, b_2, ...$).
:::

## Assumptions of Linear Regression

Linear regression makes several key assumptions that must be met for valid results:

::: {.incremental}
1. **Linearity:** The relationship between X and Y is linear
2. **Independence:** Observations are independent of each other  
3. **Homoscedasticity:** Constant variance of residuals across all levels of X
4. **Normality:** Residuals are normally distributed
5. **No Multicollinearity:** Independent variables are not highly correlated (multiple regression)
:::

## What Happens When Assumptions Are Violated? {.smaller}

| Assumption | Consequence of Violation | Detection Method |
|------------|-------------------------|------------------|
| **Linearity** | Biased and inconsistent estimates | Scatterplots, residual plots |
| **Independence** | Incorrect standard errors and CIs | Durbin-Watson test, plots |
| **Homoscedasticity** | Inefficient estimates, incorrect SEs | Residual plots, Breusch-Pagan test |
| **Normality** | Invalid hypothesis tests and CIs | Q-Q plots, Shapiro-Wilk test |
| **Multicollinearity** | Unstable coefficient estimates | VIF, correlation matrix |

::: {.callout-warning}
Always check assumptions before interpreting results!
:::

## Least Squares Estimation

The method of **Ordinary Least Squares (OLS)** finds the best-fitting line by minimizing the sum of squared residuals.

### The Objective Function
$$\text{Minimize: } \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n e_i^2$$

### For Simple Linear Regression:
$$b_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$

$$b_0 = \bar{y} - b_1\bar{x}$$

## Properties of OLS Estimators

::::: {.columns}

:::: {.column width="50%"}
### BLUE Properties:
- **Best:** Minimum variance
- **Linear:** Linear combination of Y values  
- **Unbiased:** $E[b_1] = \beta_1$
- **Efficient:** Smallest variance among unbiased estimators
::::

:::: {.column width="50%"}
### Additional Properties:
- Sum of residuals equals zero: $\sum e_i = 0$
- Regression line passes through $(\bar{x}, \bar{y})$
- Residuals uncorrelated with X values
- Residuals uncorrelated with predicted values
::::

:::::

::: {.callout-note}
## Geometric Interpretation
OLS finds the line that minimizes the vertical distances (residuals) between actual Y values and the regression line.
:::

## Python Implementation: Simple Linear Regression

```{python}
# Generate sample data
np.random.seed(42)
x = np.random.normal(5, 2, 100)
y = 30 + 4*x + np.random.normal(0, 3, 100)

# Manual calculation of OLS coefficients
x_mean, y_mean = np.mean(x), np.mean(y)
numerator = np.sum((x - x_mean) * (y - y_mean))
denominator = np.sum((x - x_mean)**2)

b1 = numerator / denominator  # Slope
b0 = y_mean - b1 * x_mean     # Intercept

print(f"Manual calculation:")
print(f"Intercept (b0): {b0:.2f}")
print(f"Slope (b1): {b1:.2f}")

# Using scikit-learn
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x.reshape(-1, 1), y)

print(f"\nScikit-learn results:")
print(f"Intercept: {model.intercept_:.2f}")
print(f"Slope: {model.coef_[0]:.2f}")
```

## Interpreting Regression Coefficients

### Simple Linear Regression: $Y = b_0 + b_1X$

::::: {.columns}

:::: {.column width="50%"}
**Intercept** $(b_0)$:
- Expected value of Y when X = 0
- Y-axis intercept of the regression line
- May not be meaningful if X = 0 is impossible
::::

:::: {.column width="50%"}  
**Slope** $(b_1)$:
- Change in Y for a one-unit increase in X
- Rate of change or marginal effect
- Positive = positive relationship
- Negative = negative relationship
::::

:::::

### Multiple Linear Regression: $Y = b_0 + b_1X_1 + b_2X_2 + ...$

::: {.callout-important}
## Partial Regression Coefficients
$b_1$ = Change in Y for a one-unit increase in $X_1$, **holding all other variables constant**

This "holding constant" aspect is crucial for causal interpretation!
:::

## Example: House Price Regression

$$\text{Price} = 50,000 + 120(\text{SqFt}) + 15,000(\text{Bedrooms}) - 2,000(\text{Age})$$

### Interpretation:

- **Intercept:** A house with 0 sq ft, 0 bedrooms, and 0 years old would cost $50,000 *(not meaningful)*
- **SqFt:** Each additional square foot increases price by $120, holding bedrooms and age constant
- **Bedrooms:** Each additional bedroom increases price by $15,000, holding sq ft and age constant  
- **Age:** Each additional year decreases price by $2,000, holding sq ft and bedrooms constant

## Model Evaluation: R-squared

**R-squared** $(R^2)$ measures the proportion of variance in Y explained by the regression model.

$$R^2 = 1 - \frac{SSE}{SST} = \frac{SSR}{SST}$$

Where:
- $SST = \sum(y_i - \bar{y})^2$ (Total Sum of Squares)
- $SSR = \sum(\hat{y}_i - \bar{y})^2$ (Regression Sum of Squares)  
- $SSE = \sum(y_i - \hat{y}_i)^2$ (Error Sum of Squares)

```{python}
# Calculate R-squared manually
y_pred = model.predict(x.reshape(-1, 1))
sst = np.sum((y - np.mean(y))**2)
sse = np.sum((y - y_pred)**2)
r2_manual = 1 - (sse / sst)

# Using sklearn
r2_sklearn = model.score(x.reshape(-1, 1), y)

print(f"R-squared (manual): {r2_manual:.3f}")
print(f"R-squared (sklearn): {r2_sklearn:.3f}")
print(f"Model explains {r2_sklearn*100:.1f}% of variance in Y")
```

## R-squared Interpretation

| $R^2$ Value | Interpretation | Model Quality |
|-------------|----------------|---------------|
| 0.0 | Model explains 0% of variance | No predictive power |
| 0.3 | Model explains 30% of variance | Weak |
| 0.7 | Model explains 70% of variance | Good |
| 0.9 | Model explains 90% of variance | Excellent |
| 1.0 | Model explains 100% of variance | Perfect fit |

::: {.callout-warning}
## Limitations of $R^2$
- Higher is not always better (overfitting)
- Doesn't indicate causation
- Can be artificially inflated by adding variables
- Doesn't indicate if model assumptions are met
:::

## Adjusted R-squared

$$\text{Adjusted } R^2 = 1 - \frac{(1-R^2)(n-1)}{n-k-1}$$

```{python}
# Calculate adjusted R-squared
n = len(y)  # sample size
k = 1       # number of predictors
adj_r2 = 1 - ((1 - r2_sklearn) * (n - 1) / (n - k - 1))

print(f"R-squared: {r2_sklearn:.3f}")
print(f"Adjusted R-squared: {adj_r2:.3f}")
```

::: {.callout-tip}
**Adjusted $R^2$** penalizes for additional variables; better for comparing models with different numbers of predictors.
:::

## Hypothesis Testing with Statsmodels

```{python}
# Using statsmodels for detailed statistics
import statsmodels.api as sm

# Add constant for intercept
X = sm.add_constant(x)
model_sm = sm.OLS(y, X).fit()

# Print summary
print(model_sm.summary())
```

## Hypothesis Testing in Regression

### Testing Individual Coefficients (t-tests)
$$H_0: \beta_j = 0 \text{ (No effect)}$$
$$H_1: \beta_j \neq 0 \text{ (Significant effect)}$$
$$t = \frac{b_j}{SE(b_j)}$$

### Testing Overall Model Significance (F-test)  
$$H_0: \beta_1 = \beta_2 = ... = \beta_k = 0$$
$$H_1: \text{At least one } \beta_j \neq 0$$
$$F = \frac{SSR/k}{SSE/(n-k-1)}$$

## Hypothesis Test Interpretation

```{python}
# Extract key statistics
params = model_sm.params
pvalues = model_sm.pvalues
conf_int = model_sm.conf_int()

results_df = pd.DataFrame({
    'Coefficient': params,
    'P-value': pvalues,
    'Significant': pvalues < 0.05
})

print("Coefficient Analysis:")
print(results_df)
print(f"\nF-statistic: {model_sm.fvalue:.2f}")
print(f"F-test p-value: {model_sm.f_pvalue:.2e}")
```

## Confidence and Prediction Intervals

### Confidence Interval for Coefficients
$$b_j \pm t_{(\alpha/2, n-k-1)} \times SE(b_j)$$

```{python}
# Confidence intervals for coefficients
conf_intervals = model_sm.conf_int()
print("95% Confidence Intervals for Coefficients:")
print(conf_intervals)

# Prediction intervals for new observations
new_x = np.array([3, 5, 7]).reshape(-1, 1)
predictions = model.predict(new_x)

print(f"\nPredictions for X = [3, 5, 7]:")
for i, (x_val, pred) in enumerate(zip([3, 5, 7], predictions)):
    print(f"X = {x_val}: Predicted Y = {pred:.2f}")
```

### Confidence vs. Prediction Intervals

::::: {.columns}

:::: {.column width="50%"}
**Confidence Interval:**
- For the **mean** of Y (narrower)
- Range for average Y at given X values
::::

:::: {.column width="50%"}
**Prediction Interval:**  
- For **individual** Y values (wider)
- Accounts for both model uncertainty AND individual variation
::::

:::::

## Residual Analysis

**Residuals** $(e = y - \hat{y})$ are the key to checking model assumptions and identifying problems.

```{python}
#| fig-width: 12
#| fig-height: 8

# Calculate residuals
residuals = y - y_pred
fitted_values = y_pred

# Create residual plots
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
fig.suptitle('Residual Analysis Plots', fontsize=16)

# 1. Residuals vs Fitted
axes[0, 0].scatter(fitted_values, residuals, alpha=0.6)
axes[0, 0].axhline(y=0, color='red', linestyle='--')
axes[0, 0].set_xlabel('Fitted Values')
axes[0, 0].set_ylabel('Residuals')
axes[0, 0].set_title('Residuals vs Fitted Values')

# 2. Q-Q plot
stats.probplot(residuals, dist="norm", plot=axes[0, 1])
axes[0, 1].set_title('Normal Q-Q Plot')

# 3. Scale-Location plot
sqrt_residuals = np.sqrt(np.abs(residuals))
axes[1, 0].scatter(fitted_values, sqrt_residuals, alpha=0.6)
axes[1, 0].set_xlabel('Fitted Values')
axes[1, 0].set_ylabel('‚àö|Residuals|')
axes[1, 0].set_title('Scale-Location Plot')

# 4. Histogram of residuals
axes[1, 1].hist(residuals, bins=20, alpha=0.7, edgecolor='black')
axes[1, 1].set_xlabel('Residuals')
axes[1, 1].set_ylabel('Frequency')
axes[1, 1].set_title('Distribution of Residuals')

plt.tight_layout()
plt.show()
```

## Essential Residual Plots:

1. **Residuals vs. Fitted Values** - Checks linearity and homoscedasticity
2. **Normal Q-Q Plot** - Checks normality of residuals  
3. **Scale-Location Plot** - Checks homoscedasticity
4. **Histogram** - Shows distribution of residuals

::: {.callout-note}
**Good residual plots show no clear patterns** ‚Äì just random scatter around zero!
:::

## Common Residual Plot Problems {.smaller}

| Pattern | Problem | Solution |
|---------|---------|----------|
| Curved pattern | Non-linearity | Transform variables, add polynomial terms |
| Funnel shape | Heteroscedasticity | Transform Y, use weighted regression |
| Outliers | Unusual observations | Investigate, consider robust methods |
| Clusters/patterns | Missing variables | Add relevant predictors |

```{python}
#| fig-width: 10
#| fig-height: 4

# Demonstrate good vs bad residual patterns
np.random.seed(123)
x_demo = np.linspace(0, 10, 100)

# Good pattern (random)
y_good = 2 + 3*x_demo + np.random.normal(0, 1, 100)
model_good = LinearRegression().fit(x_demo.reshape(-1, 1), y_good)
residuals_good = y_good - model_good.predict(x_demo.reshape(-1, 1))

# Bad pattern (curved)
y_bad = 2 + 3*x_demo + 0.5*x_demo**2 + np.random.normal(0, 1, 100)
model_bad = LinearRegression().fit(x_demo.reshape(-1, 1), y_bad)
residuals_bad = y_bad - model_bad.predict(x_demo.reshape(-1, 1))

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

ax1.scatter(model_good.predict(x_demo.reshape(-1, 1)), residuals_good, alpha=0.6)
ax1.axhline(y=0, color='red', linestyle='--')
ax1.set_title('Good: Random Scatter')
ax1.set_xlabel('Fitted Values')
ax1.set_ylabel('Residuals')

ax2.scatter(model_bad.predict(x_demo.reshape(-1, 1)), residuals_bad, alpha=0.6)
ax2.axhline(y=0, color='red', linestyle='--')
ax2.set_title('Bad: Curved Pattern')
ax2.set_xlabel('Fitted Values')
ax2.set_ylabel('Residuals')

plt.tight_layout()
plt.show()
```

## Common Problems in Linear Regression

### 1. Multicollinearity
::: {.callout-warning}
**Problem:** High correlation between independent variables

**Detection:** Correlation matrix (|r| > 0.8), VIF > 10

**Solutions:** Remove correlated variables, ridge regression, PCA
:::

```{python}
# Check for multicollinearity with VIF
def calculate_vif(df):
    """Calculate Variance Inflation Factor for features"""
    vif = pd.DataFrame()
    vif["Feature"] = df.columns
    vif["VIF"] = [variance_inflation_factor(df.values, i) 
                  for i in range(df.shape[1])]
    return vif

# Example with correlated variables
np.random.seed(42)
x1 = np.random.normal(0, 1, 100)
x2 = x1 + np.random.normal(0, 0.1, 100)  # Highly correlated with x1
x3 = np.random.normal(0, 1, 100)  # Independent

df = pd.DataFrame({'X1': x1, 'X2': x2, 'X3': x3})
vif_results = calculate_vif(df)
print("Variance Inflation Factors:")
print(vif_results)
```

### 2. Heteroscedasticity  
::: {.callout-warning}
**Problem:** Non-constant variance of residuals

**Detection:** Residual plots, Breusch-Pagan test

**Solutions:** Transform Y, weighted least squares, robust SEs
:::

```{python}
# Test for heteroscedasticity
from statsmodels.stats.diagnostic import het_breuschpagan

# Breusch-Pagan test
bp_test = het_breuschpagan(residuals, X)
print(f"Breusch-Pagan test:")
print(f"LM statistic: {bp_test[0]:.4f}")
print(f"p-value: {bp_test[1]:.4f}")
print(f"Heteroscedasticity detected: {bp_test[1] < 0.05}")
```

## Outliers and Influential Points

```{python}
#| fig-width: 10
#| fig-height: 4

# Demonstrate influence of outliers
np.random.seed(123)
x_clean = np.random.uniform(0, 10, 20)
y_clean = 2 + 0.5*x_clean + np.random.normal(0, 1, 20)

# Add influential point
x_with_outlier = np.append(x_clean, 15)
y_with_outlier = np.append(y_clean, 3)

# Fit models
model_clean = LinearRegression().fit(x_clean.reshape(-1, 1), y_clean)
model_outlier = LinearRegression().fit(x_with_outlier.reshape(-1, 1), y_with_outlier)

# Plot comparison
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Without outlier
ax1.scatter(x_clean, y_clean, alpha=0.7)
x_line = np.linspace(0, 16, 100)
ax1.plot(x_line, model_clean.predict(x_line.reshape(-1, 1)), 'b-', lw=2, label='Without outlier')
ax1.set_title('Without Influential Point')
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.legend()

# With outlier
ax2.scatter(x_clean, y_clean, alpha=0.7, label='Normal points')
ax2.scatter(15, 3, color='red', s=100, label='Influential point')
ax2.plot(x_line, model_outlier.predict(x_line.reshape(-1, 1)), 'b-', lw=2, label='With outlier')
ax2.plot(x_line, model_clean.predict(x_line.reshape(-1, 1)), 'r--', lw=2, label='Without outlier')
ax2.set_title('With Influential Point')
ax2.set_xlabel('X')
ax2.set_ylabel('Y')
ax2.legend()

plt.tight_layout()
plt.show()
```

**Detection:**
- Standardized residuals > |3|
- Cook's distance > 4/n
- Leverage > 2k/n

## Variable Selection with Python

```{python}
# Demonstrate variable selection techniques
from sklearn.feature_selection import RFE, SelectKBest, f_regression
from sklearn.linear_model import LassoCV, RidgeCV

# Generate sample data with multiple features
np.random.seed(42)
n_samples, n_features = 100, 5
X_multi = np.random.randn(n_samples, n_features)
# Only first 2 features are truly related to y
y_multi = 3*X_multi[:, 0] + 2*X_multi[:, 1] + np.random.randn(n_samples)

# Method 1: Recursive Feature Elimination
rfe = RFE(LinearRegression(), n_features_to_select=3)
rfe.fit(X_multi, y_multi)
print("RFE Selected Features:", rfe.support_)

# Method 2: SelectKBest with F-test
selector = SelectKBest(f_regression, k=3)
selector.fit(X_multi, y_multi)
print("SelectKBest Features:", selector.get_support())

# Method 3: Lasso (L1 regularization)
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_multi, y_multi)
print("Lasso Coefficients:", lasso.coef_)
print("Non-zero coefficients:", np.sum(lasso.coef_ != 0))
```

## Model Selection Criteria

| Criterion | Formula | Goal |
|-----------|---------|------|
| **AIC** | $n \ln(SSE/n) + 2k$ | Minimize |
| **BIC** | $n \ln(SSE/n) + k \ln(n)$ | Minimize |
| **Adjusted $R^2$** | $1 - \frac{(1-R^2)(n-1)}{n-k-1}$ | Maximize |

```{python}
# Calculate model selection criteria
def calculate_aic_bic(y_true, y_pred, n_params):
    n = len(y_true)
    sse = np.sum((y_true - y_pred)**2)
    aic = n * np.log(sse/n) + 2*n_params
    bic = n * np.log(sse/n) + n_params*np.log(n)
    return aic, bic

aic, bic = calculate_aic_bic(y, y_pred, 2)  # 2 parameters: intercept + slope
print(f"AIC: {aic:.2f}")
print(f"BIC: {bic:.2f}")
```

::: {.callout-tip}
**Cross-Validation:** Split data into training and testing sets to evaluate true predictive performance and avoid overfitting.
:::

## Cross-Validation Example

```{python}
from sklearn.model_selection import cross_val_score, KFold

# Perform k-fold cross-validation
cv_scores = cross_val_score(LinearRegression(), x.reshape(-1, 1), y, 
                           cv=5, scoring='r2')

print("Cross-validation R¬≤ scores:", cv_scores)
print(f"Mean CV R¬≤: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# Compare with training R¬≤
train_r2 = model.score(x.reshape(-1, 1), y)
print(f"Training R¬≤: {train_r2:.3f}")
print(f"Difference (potential overfitting): {train_r2 - cv_scores.mean():.3f}")
```

## Worked Example: Simple Linear Regression

::: {.callout-note}
## Problem
A marketing manager wants to understand the relationship between advertising spend (X, in $1000s) and sales revenue (Y, in $1000s).

**Given:** n=10, $\bar{x}=5$, $\bar{y}=50$, $\sum(x-\bar{x})^2=50$, $\sum(x-\bar{x})(y-\bar{y})=200$, SSE=100
:::

```{python}
# Manual calculation following the example
n = 10
x_mean = 5
y_mean = 50
sum_x_dev_sq = 50
sum_xy_dev = 200
sse = 100

# Calculate coefficients
b1 = sum_xy_dev / sum_x_dev_sq
b0 = y_mean - b1 * x_mean

# Calculate R-squared
ssr = b1**2 * sum_x_dev_sq
sst = ssr + sse
r_squared = ssr / sst

print("Solution Steps:")
print(f"1. Slope (b1): {sum_xy_dev}/{sum_x_dev_sq} = {b1}")
print(f"2. Intercept (b0): {y_mean} - {b1}√ó{x_mean} = {b0}")
print(f"3. Regression Equation: ≈∂ = {b0} + {b1}X")
print(f"4. R¬≤: {ssr}/{sst} = {r_squared:.3f}")
```

## Interpretation of Example

### Coefficient Interpretation:
- **Slope (4):** Each $1000 increase in advertising increases sales by $4000
- **Intercept (30):** With $0 advertising, expected sales are $30,000  
- **$R^2$ (0.889):** 88.9% of sales variation is explained by advertising

```{python}
#| fig-width: 8
#| fig-height: 5

# Visualize the example
np.random.seed(42)
x_example = np.array([2, 3, 4, 4, 5, 5, 6, 6, 7, 8])
y_example = 30 + 4*x_example + np.random.normal(0, 1.5, 10)

plt.figure(figsize=(8, 5))
plt.scatter(x_example, y_example, s=80, alpha=0.7, color='blue', label='Data points')
plt.plot(x_example, 30 + 4*x_example, color='red', linewidth=2, label='Regression line')
plt.xlabel('Advertising Spend ($1000s)')
plt.ylabel('Sales Revenue ($1000s)')
plt.title('Advertising vs Sales Revenue')
plt.text(6, 45, 'Y = 30 + 4X\nR¬≤ = 0.889', fontsize=12, 
         bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

## Multiple Regression Example with Python

```{python}
# Create multiple regression example
np.random.seed(42)
n = 50

# Generate house data
sqft = np.random.normal(2000, 500, n)
bedrooms = np.random.poisson(3, n)
age = np.random.uniform(0, 50, n)

# Generate price with known relationship
price = -10000 + 150*sqft + 5000*bedrooms - 1200*age + np.random.normal(0, 20000, n)

# Create DataFrame
house_data = pd.DataFrame({
    'sqft': sqft,
    'bedrooms': bedrooms,
    'age': age,
    'price': price
})

# Fit multiple regression model
X_house = house_data[['sqft', 'bedrooms', 'age']]
y_house = house_data['price']

# Using statsmodels for detailed output
X_house_sm = sm.add_constant(X_house)
house_model = sm.OLS(y_house, X_house_sm).fit()

print("Multiple Regression Results:")
print(house_model.summary())
```

## Multiple Regression Interpretation

### Significant Variables:
- **SqFt:** Each additional sq ft increases price significantly
- **Age:** Each additional year decreases price significantly

### Model Assessment:
- **R¬≤:** Proportion of variance explained
- **F-statistic:** Overall model significance
- **Individual t-tests:** Significance of each predictor

```{python}
# Extract and display key results
results_summary = pd.DataFrame({
    'Variable': house_model.params.index,
    'Coefficient': house_model.params.values,
    'P-value': house_model.pvalues.values,
    'Significant': house_model.pvalues.values < 0.05
})

print("\nKey Results Summary:")
print(results_summary)
print(f"\nModel R¬≤: {house_model.rsquared:.3f}")
print(f"Adjusted R¬≤: {house_model.rsquared_adj:.3f}")
print(f"F-statistic p-value: {house_model.f_pvalue:.2e}")
```

## Advanced Topics

### 1. Polynomial Regression
```{python}
from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features
poly = PolynomialFeatures(degree=2)
x_poly = poly.fit_transform(x.reshape(-1, 1))

# Fit polynomial model
poly_model = LinearRegression().fit(x_poly, y)
poly_r2 = poly_model.score(x_poly, y)

print(f"Linear model R¬≤: {r2_sklearn:.3f}")
print(f"Polynomial model R¬≤: {poly_r2:.3f}")
```

### 2. Regularized Regression
```{python}
from sklearn.linear_model import Ridge, Lasso

# Ridge Regression (L2)
ridge = RidgeCV(alphas=[0.1, 1.0, 10.0])
ridge.fit(X_house, y_house)

# Lasso Regression (L1)
lasso = LassoCV(alphas=[0.1, 1.0, 10.0])
lasso.fit(X_house, y_house)

print("Ridge coefficients:", ridge.coef_)
print("Lasso coefficients:", lasso.coef_)
print("Lasso selected features:", np.sum(lasso.coef_ != 0))
```

## Best Practices {.smaller}

::: {.callout-tip}
## ‚úÖ Do These:
- Always plot your data first - scatterplots reveal relationships
- Check assumptions using residual plots and diagnostic tests  
- Start simple with few variables, then add complexity
- Use cross-validation to assess true predictive performance
- Report confidence intervals alongside point estimates
- Consider practical significance not just statistical significance
- Validate with new data when possible
:::

::: {.callout-warning}  
## ‚ùå Avoid These:
- Assuming causation from correlation
- Ignoring assumptions - leads to invalid inferences
- Overfitting - too many variables relative to sample size
- Extrapolating beyond data range
- Not checking for outliers
- Ignoring multicollinearity
- Using inadequate sample sizes
:::

## Python Libraries for Linear Regression

```{python}
# Summary of key Python libraries
libraries_info = {
    'Library': ['scikit-learn', 'statsmodels', 'numpy', 'pandas', 'matplotlib', 'seaborn'],
    'Primary Use': [
        'Machine learning, cross-validation',
        'Statistical inference, detailed output',
        'Numerical computations, arrays',
        'Data manipulation, DataFrames',
        'Basic plotting, customization',
        'Statistical plotting, easy visualization'
    ],
    'Key Functions': [
        'LinearRegression(), cross_val_score()',
        'OLS(), summary(), diagnostic tests',
        'mean(), std(), linear algebra',
        'DataFrame, groupby(), merge()',
        'plot(), scatter(), subplots()',
        'regplot(), residplot(), pairplot()'
    ]
}

libraries_df = pd.DataFrame(libraries_info)
print("Essential Python Libraries for Regression:")
print(libraries_df.to_string(index=False))
```

## Diagnostic Checklist

::: {.incremental}
- ‚òê Scatterplot of Y vs. each X variable
- ‚òê Residuals vs. fitted values plot  
- ‚òê Normal Q-Q plot of residuals
- ‚òê Check for outliers and leverage points
- ‚òê Correlation matrix for multicollinearity
- ‚òê Test statistical significance of coefficients
- ‚òê Calculate and interpret $R^2$ and adjusted $R^2$
- ‚òê Cross-validate if sample size permits
:::

```{python}
# Complete diagnostic function
def regression_diagnostics(model, X, y, feature_names=None):
    """Comprehensive regression diagnostics"""
    if feature_names is None:
        feature_names = [f'X{i}' for i in range(X.shape[1])]
    
    # Predictions and residuals
    y_pred = model.predict(X)
    residuals = y - y_pred
    
    # Basic metrics
    r2 = model.score(X, y)
    n, k = X.shape
    adj_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))
    
    print("=== REGRESSION DIAGNOSTICS ===")
    print(f"R¬≤: {r2:.3f}")
    print(f"Adjusted R¬≤: {adj_r2:.3f}")
    print(f"RMSE: {np.sqrt(np.mean(residuals**2)):.3f}")
    
    # Check for outliers
    standardized_residuals = residuals / np.std(residuals)
    outliers = np.abs(standardized_residuals) > 3
    print(f"Potential outliers (|std residual| > 3): {np.sum(outliers)}")
    
    return {
        'r2': r2,
        'adj_r2': adj_r2,
        'residuals': residuals,
        'outliers': outliers
    }

# Example usage
diagnostics = regression_diagnostics(model, x.reshape(-1, 1), y, ['Advertising'])
```

## Summary: Key Takeaways

::::: {.columns}

:::: {.column width="50%"}
### Core Concepts:
- Models linear relationships between variables
- Uses least squares for parameter estimation  
- Provides prediction and explanation
- Foundation for more complex models

### Essential Components:
- Model specification and assumptions
- Coefficient interpretation
- Statistical significance testing
- Model evaluation with $R^2$
::::

:::: {.column width="50%"}
### Critical Skills:
- Residual analysis for assumption checking
- Identifying and handling problems
- Variable selection techniques  
- Distinguishing correlation from causation

### Python Implementation:
- scikit-learn for ML-focused approach
- statsmodels for statistical inference
- matplotlib/seaborn for visualization
- pandas for data manipulation
::::

:::::

## The Linear Regression Process

::: {.callout-note}
## Remember the Process
**Explore ‚Üí Model ‚Üí Check ‚Üí Interpret ‚Üí Apply**

*Plot data ‚Üí Fit model ‚Üí Validate assumptions ‚Üí Understand coefficients ‚Üí Make predictions*
:::

> *"All models are wrong, but some are useful"* - George Box

Linear regression is both an art and a science - master the fundamentals, then adapt to your specific context and data!

## Questions?

::: {.callout-tip}
## Additional Resources
- **Python packages:** `scikit-learn`, `statsmodels`, `pandas`, `matplotlib`, `seaborn`
- **Documentation:** scikit-learn.org, statsmodels.org
- **Books:** "Introduction to Statistical Learning" (Python edition)
- **Online:** Coursera, edX, Kaggle Learn
:::

Thank you for your attention! 

Linear regression is a powerful tool - use it wisely and always check your assumptions! üìäüêç

